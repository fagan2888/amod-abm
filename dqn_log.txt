step 2; T: 1050; average reward: -4.22 - action: n; reward: -4.22
    1/2000: episode: 1, duration: 0.212s, episode steps: 1, steps per second: 5, episode reward: -9.220, mean reward: -9.220 [-9.220, -9.220], mean action: 8.000 [8.000, 8.000], mean observation: 0.219 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 10; T: 2850; average reward: 25.37 - action: n; reward: 54.95
    5/2000: episode: 2, duration: 0.321s, episode steps: 4, steps per second: 12, episode reward: 19.955, mean reward: 4.989 [-10.000, 49.955], mean action: 6.500 [3.000, 8.000], mean observation: 0.367 [0.000, 4.000], loss: --, mean_absolute_error: --, mean_q: --
step 11; T: 4350; average reward: 24.79 - action: e; reward: 23.62
    6/2000: episode: 3, duration: 0.124s, episode steps: 1, steps per second: 8, episode reward: 23.622, mean reward: 23.622 [23.622, 23.622], mean action: 2.000 [2.000, 2.000], mean observation: 0.403 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 16; T: 6900; average reward: 17.34 - action: e; reward: -5.00
    9/2000: episode: 4, duration: 0.256s, episode steps: 3, steps per second: 12, episode reward: -25.000, mean reward: -8.333 [-10.000, -5.000], mean action: 4.000 [2.000, 8.000], mean observation: 0.244 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 19; T: 7350; average reward: 20.06 - action: e; reward: 30.95
   11/2000: episode: 5, duration: 0.102s, episode steps: 2, steps per second: 20, episode reward: 20.950, mean reward: 10.475 [-10.000, 30.950], mean action: 2.500 [2.000, 3.000], mean observation: 0.160 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 20; T: 7950; average reward: 8.70 - action: e; reward: -48.11
   12/2000: episode: 6, duration: 0.059s, episode steps: 1, steps per second: 17, episode reward: -48.106, mean reward: -48.106 [-48.106, -48.106], mean action: 2.000 [2.000, 2.000], mean observation: 0.408 [0.000, 4.000], loss: --, mean_absolute_error: --, mean_q: --
step 25; T: 9600; average reward: 0.02 - action: e; reward: -52.05
   15/2000: episode: 7, duration: 0.197s, episode steps: 3, steps per second: 15, episode reward: -72.051, mean reward: -24.017 [-52.051, -10.000], mean action: 3.667 [2.000, 7.000], mean observation: 0.297 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 27; T: 10950; average reward: -0.61 - action: n; reward: -5.00
   16/2000: episode: 8, duration: 0.137s, episode steps: 1, steps per second: 7, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.251 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 28; T: 11700; average reward: -12.16 - action: n; reward: -104.61
   17/2000: episode: 9, duration: 0.066s, episode steps: 1, steps per second: 15, episode reward: -104.606, mean reward: -104.606 [-104.606, -104.606], mean action: 8.000 [8.000, 8.000], mean observation: 0.197 [0.000, 4.000], loss: --, mean_absolute_error: --, mean_q: --
step 30; T: 12150; average reward: -4.13 - action: e; reward: 68.16
   18/2000: episode: 10, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: 63.164, mean reward: 63.164 [63.164, 63.164], mean action: 2.000 [2.000, 2.000], mean observation: 0.227 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 32; T: 12750; average reward: -4.21 - action: e; reward: -5.00
   19/2000: episode: 11, duration: 0.091s, episode steps: 1, steps per second: 11, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.429 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 39; T: 14700; average reward: 2.13 - action: e; reward: 71.90
   23/2000: episode: 12, duration: 0.290s, episode steps: 4, steps per second: 14, episode reward: 41.901, mean reward: 10.475 [-10.000, 71.901], mean action: 4.750 [2.000, 8.000], mean observation: 0.348 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 44; T: 15900; average reward: 1.94 - action: e; reward: -0.43
   26/2000: episode: 13, duration: 0.251s, episode steps: 3, steps per second: 12, episode reward: -20.434, mean reward: -6.811 [-10.000, -0.434], mean action: 3.667 [2.000, 7.000], mean observation: 0.340 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 52; T: 18000; average reward: 0.41 - action: e; reward: -19.38
   30/2000: episode: 14, duration: 0.418s, episode steps: 4, steps per second: 10, episode reward: -54.382, mean reward: -13.595 [-24.382, -10.000], mean action: 3.500 [2.000, 7.000], mean observation: 0.317 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 54; T: 19050; average reward: -1.87 - action: n; reward: -33.87
   31/2000: episode: 15, duration: 0.142s, episode steps: 1, steps per second: 7, episode reward: -38.868, mean reward: -38.868 [-38.868, -38.868], mean action: 8.000 [8.000, 8.000], mean observation: 0.459 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 55; T: 20100; average reward: -3.36 - action: e; reward: -25.64
   32/2000: episode: 16, duration: 0.083s, episode steps: 1, steps per second: 12, episode reward: -25.635, mean reward: -25.635 [-25.635, -25.635], mean action: 2.000 [2.000, 2.000], mean observation: 0.429 [0.000, 4.000], loss: --, mean_absolute_error: --, mean_q: --
step 58; T: 23250; average reward: -7.27 - action: e; reward: -69.94
   34/2000: episode: 17, duration: 0.288s, episode steps: 2, steps per second: 7, episode reward: -79.942, mean reward: -39.971 [-69.942, -10.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.427 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 61; T: 24150; average reward: -8.70 - action: n; reward: -32.90
   36/2000: episode: 18, duration: 0.162s, episode steps: 2, steps per second: 12, episode reward: -42.900, mean reward: -21.450 [-32.900, -10.000], mean action: 7.500 [7.000, 8.000], mean observation: 0.367 [0.000, 4.000], loss: --, mean_absolute_error: --, mean_q: --
step 62; T: 24300; average reward: -10.80 - action: n; reward: -48.66
   37/2000: episode: 19, duration: 0.048s, episode steps: 1, steps per second: 21, episode reward: -48.656, mean reward: -48.656 [-48.656, -48.656], mean action: 8.000 [8.000, 8.000], mean observation: 0.299 [0.000, 4.000], loss: --, mean_absolute_error: --, mean_q: --
step 67; T: 25350; average reward: -8.98 - action: se; reward: 25.65
   40/2000: episode: 20, duration: 0.258s, episode steps: 3, steps per second: 12, episode reward: 5.649, mean reward: 1.883 [-10.000, 25.649], mean action: 4.333 [2.000, 8.000], mean observation: 0.458 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 69; T: 27150; average reward: -8.79 - action: e; reward: -5.00
   41/2000: episode: 21, duration: 0.168s, episode steps: 1, steps per second: 6, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.376 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --
step 70; T: 29250; average reward: -8.50 - action: e; reward: -2.33
   42/2000: episode: 22, duration: 0.166s, episode steps: 1, steps per second: 6, episode reward: -2.334, mean reward: -2.334 [-2.334, -2.334], mean action: 2.000 [2.000, 2.000], mean observation: 0.253 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --
step 71; T: 31200; average reward: -7.78 - action: n; reward: 7.90
   43/2000: episode: 23, duration: 0.156s, episode steps: 1, steps per second: 6, episode reward: 7.902, mean reward: 7.902 [7.902, 7.902], mean action: 8.000 [8.000, 8.000], mean observation: 0.304 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 72; T: 31650; average reward: -4.45 - action: n; reward: 72.12
   44/2000: episode: 24, duration: 0.071s, episode steps: 1, steps per second: 14, episode reward: 72.124, mean reward: 72.124 [72.124, 72.124], mean action: 8.000 [8.000, 8.000], mean observation: 0.280 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 76; T: 32550; average reward: -4.47 - action: e; reward: -5.00
   46/2000: episode: 25, duration: 0.235s, episode steps: 2, steps per second: 9, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.299 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 80; T: 35250; average reward: -4.03 - action: nw; reward: 7.17
   48/2000: episode: 26, duration: 0.349s, episode steps: 2, steps per second: 6, episode reward: -7.826, mean reward: -3.913 [-10.000, 2.174], mean action: 4.500 [2.000, 7.000], mean observation: 0.264 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 81; T: 36000; average reward: -5.73 - action: e; reward: -49.89
   49/2000: episode: 27, duration: 0.114s, episode steps: 1, steps per second: 9, episode reward: -49.887, mean reward: -49.887 [-49.887, -49.887], mean action: 2.000 [2.000, 2.000], mean observation: 0.227 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --
step 82; T: 37050; average reward: -5.70 - action: nw; reward: -5.00
   50/2000: episode: 28, duration: 0.113s, episode steps: 1, steps per second: 9, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --
step 85; T: 39000; average reward: -5.68 - action: w; reward: -5.00
   52/2000: episode: 29, duration: 0.287s, episode steps: 2, steps per second: 7, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.173 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 87; T: 40200; average reward: -5.65 - action: w; reward: -5.00
   53/2000: episode: 30, duration: 0.193s, episode steps: 1, steps per second: 5, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.309 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 88; T: 40950; average reward: -3.03 - action: nw; reward: 75.80
   54/2000: episode: 31, duration: 0.099s, episode steps: 1, steps per second: 10, episode reward: 75.796, mean reward: 75.796 [75.796, 75.796], mean action: 7.000 [7.000, 7.000], mean observation: 0.227 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 90; T: 41850; average reward: -3.09 - action: e; reward: -5.00
   55/2000: episode: 32, duration: 0.166s, episode steps: 1, steps per second: 6, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.365 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 93; T: 43950; average reward: -3.14 - action: nw; reward: -5.00
   57/2000: episode: 33, duration: 0.279s, episode steps: 2, steps per second: 7, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.283 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 95; T: 45450; average reward: 0.77 - action: n; reward: 129.86
   58/2000: episode: 34, duration: 0.209s, episode steps: 1, steps per second: 5, episode reward: 124.859, mean reward: 124.859 [124.859, 124.859], mean action: 8.000 [8.000, 8.000], mean observation: 0.320 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --
step 96; T: 46200; average reward: 4.16 - action: nw; reward: 119.48
   59/2000: episode: 35, duration: 0.105s, episode steps: 1, steps per second: 10, episode reward: 119.485, mean reward: 119.485 [119.485, 119.485], mean action: 7.000 [7.000, 7.000], mean observation: 0.357 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 98; T: 47400; average reward: 4.82 - action: nw; reward: 27.81
   60/2000: episode: 36, duration: 0.210s, episode steps: 1, steps per second: 5, episode reward: 22.813, mean reward: 22.813 [22.813, 22.813], mean action: 7.000 [7.000, 7.000], mean observation: 0.192 [0.000, 4.000], loss: --, mean_absolute_error: --, mean_q: --
step 107; T: 49650; average reward: 4.55 - action: e; reward: -5.00
   65/2000: episode: 37, duration: 0.808s, episode steps: 5, steps per second: 6, episode reward: -45.000, mean reward: -9.000 [-10.000, -5.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.215 [0.000, 4.000], loss: --, mean_absolute_error: --, mean_q: --
step 108; T: 50400; average reward: 4.04 - action: e; reward: -14.85
   66/2000: episode: 38, duration: 0.118s, episode steps: 1, steps per second: 9, episode reward: -14.847, mean reward: -14.847 [-14.847, -14.847], mean action: 2.000 [2.000, 2.000], mean observation: 0.375 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 109; T: 51900; average reward: 1.61 - action: e; reward: -90.80
   67/2000: episode: 39, duration: 0.187s, episode steps: 1, steps per second: 5, episode reward: -90.805, mean reward: -90.805 [-90.805, -90.805], mean action: 2.000 [2.000, 2.000], mean observation: 0.325 [0.000, 5.000], loss: --, mean_absolute_error: --, mean_q: --
step 112; T: 53550; average reward: 1.44 - action: e; reward: -5.00
   69/2000: episode: 40, duration: 0.294s, episode steps: 2, steps per second: 7, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.237 [0.000, 4.000], loss: --, mean_absolute_error: --, mean_q: --
step 114; T: 54000; average reward: 1.28 - action: n; reward: -5.39
   70/2000: episode: 41, duration: 0.152s, episode steps: 1, steps per second: 7, episode reward: -10.390, mean reward: -10.390 [-10.390, -10.390], mean action: 8.000 [8.000, 8.000], mean observation: 0.352 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 115; T: 55950; average reward: 1.21 - action: e; reward: -1.32
   71/2000: episode: 42, duration: 0.173s, episode steps: 1, steps per second: 6, episode reward: -1.325, mean reward: -1.325 [-1.325, -1.325], mean action: 2.000 [2.000, 2.000], mean observation: 0.240 [0.000, 4.000], loss: --, mean_absolute_error: --, mean_q: --
step 117; T: 57000; average reward: 1.01 - action: e; reward: -7.44
   72/2000: episode: 43, duration: 0.193s, episode steps: 1, steps per second: 5, episode reward: -12.436, mean reward: -12.436 [-12.436, -12.436], mean action: 2.000 [2.000, 2.000], mean observation: 0.269 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 119; T: 58800; average reward: 0.88 - action: e; reward: -5.00
   73/2000: episode: 44, duration: 0.289s, episode steps: 1, steps per second: 3, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.381 [0.000, 6.000], loss: --, mean_absolute_error: --, mean_q: --
step 121; T: 61500; average reward: 2.14 - action: nw; reward: 57.69
   74/2000: episode: 45, duration: 0.363s, episode steps: 1, steps per second: 3, episode reward: 52.688, mean reward: 52.688 [52.688, 52.688], mean action: 7.000 [7.000, 7.000], mean observation: 0.437 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 122; T: 63900; average reward: 4.56 - action: n; reward: 113.63
   75/2000: episode: 46, duration: 0.236s, episode steps: 1, steps per second: 4, episode reward: 113.633, mean reward: 113.633 [113.633, 113.633], mean action: 8.000 [8.000, 8.000], mean observation: 0.267 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --
step 123; T: 64650; average reward: 4.36 - action: s; reward: -5.00
   76/2000: episode: 47, duration: 0.138s, episode steps: 1, steps per second: 7, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.205 [0.000, 4.000], loss: --, mean_absolute_error: --, mean_q: --
step 124; T: 68250; average reward: 5.07 - action: noop; reward: 38.40
   77/2000: episode: 48, duration: 0.375s, episode steps: 1, steps per second: 3, episode reward: 38.403, mean reward: 38.403 [38.403, 38.403], mean action: 0.000 [0.000, 0.000], mean observation: 0.237 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --
step 128; T: 69150; average reward: 4.61 - action: e; reward: -17.63
   79/2000: episode: 49, duration: 0.387s, episode steps: 2, steps per second: 5, episode reward: -32.632, mean reward: -16.316 [-22.632, -10.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.245 [0.000, 4.000], loss: --, mean_absolute_error: --, mean_q: --
step 130; T: 69750; average reward: 5.00 - action: e; reward: 24.50
   80/2000: episode: 50, duration: 0.203s, episode steps: 1, steps per second: 5, episode reward: 19.501, mean reward: 19.501 [19.501, 19.501], mean action: 2.000 [2.000, 2.000], mean observation: 0.187 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --
step 134; T: 71700; average reward: 4.81 - action: e; reward: -5.00
   82/2000: episode: 51, duration: 0.419s, episode steps: 2, steps per second: 5, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 4.500 [2.000, 7.000], mean observation: 0.309 [0.000, 4.000], loss: --, mean_absolute_error: --, mean_q: --
step 135; T: 73200; average reward: 5.85 - action: n; reward: 59.00
   83/2000: episode: 52, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 58.998, mean reward: 58.998 [58.998, 58.998], mean action: 8.000 [8.000, 8.000], mean observation: 0.325 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 139; T: 74700; average reward: 5.64 - action: e; reward: -5.00
   85/2000: episode: 53, duration: 0.450s, episode steps: 2, steps per second: 4, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.275 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 142; T: 76350; average reward: 5.45 - action: n; reward: -5.00
   87/2000: episode: 54, duration: 0.378s, episode steps: 2, steps per second: 5, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.220 [0.000, 4.000], loss: --, mean_absolute_error: --, mean_q: --
step 144; T: 78600; average reward: 4.45 - action: e; reward: -49.48
   88/2000: episode: 55, duration: 0.326s, episode steps: 1, steps per second: 3, episode reward: -54.480, mean reward: -54.480 [-54.480, -54.480], mean action: 2.000 [2.000, 2.000], mean observation: 0.207 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 145; T: 79500; average reward: 3.74 - action: e; reward: -35.18
   89/2000: episode: 56, duration: 0.176s, episode steps: 1, steps per second: 6, episode reward: -35.184, mean reward: -35.184 [-35.184, -35.184], mean action: 2.000 [2.000, 2.000], mean observation: 0.227 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 146; T: 80100; average reward: 3.55 - action: n; reward: -7.12
   90/2000: episode: 57, duration: 0.126s, episode steps: 1, steps per second: 8, episode reward: -7.117, mean reward: -7.117 [-7.117, -7.117], mean action: 8.000 [8.000, 8.000], mean observation: 0.376 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 150; T: 82800; average reward: 3.82 - action: e; reward: 19.20
   92/2000: episode: 58, duration: 0.623s, episode steps: 2, steps per second: 3, episode reward: 4.201, mean reward: 2.100 [-10.000, 14.201], mean action: 2.000 [2.000, 2.000], mean observation: 0.405 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 151; T: 83850; average reward: 2.37 - action: s; reward: -81.92
   93/2000: episode: 59, duration: 0.145s, episode steps: 1, steps per second: 7, episode reward: -81.919, mean reward: -81.919 [-81.919, -81.919], mean action: 4.000 [4.000, 4.000], mean observation: 0.160 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 154; T: 84600; average reward: 2.85 - action: n; reward: 31.19
   95/2000: episode: 60, duration: 0.320s, episode steps: 2, steps per second: 6, episode reward: 21.191, mean reward: 10.595 [-10.000, 31.191], mean action: 5.000 [2.000, 8.000], mean observation: 0.291 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 157; T: 85650; average reward: 3.48 - action: n; reward: 41.52
   97/2000: episode: 61, duration: 0.370s, episode steps: 2, steps per second: 5, episode reward: 31.524, mean reward: 15.762 [-10.000, 41.524], mean action: 5.000 [2.000, 8.000], mean observation: 0.277 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 158; T: 89400; average reward: 4.40 - action: e; reward: 60.58
   98/2000: episode: 62, duration: 0.325s, episode steps: 1, steps per second: 3, episode reward: 60.579, mean reward: 60.579 [60.579, 60.579], mean action: 2.000 [2.000, 2.000], mean observation: 0.256 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --
step 163; T: 90750; average reward: 4.44 - action: ne; reward: 6.99
  101/2000: episode: 63, duration: 1.399s, episode steps: 3, steps per second: 2, episode reward: -13.012, mean reward: -4.337 [-10.000, 6.988], mean action: 3.667 [1.000, 8.000], mean observation: 0.392 [0.000, 3.000], loss: --, mean_absolute_error: --, mean_q: --
step 164; T: 91500; average reward: 3.76 - action: nw; reward: -39.54
  102/2000: episode: 64, duration: 0.170s, episode steps: 1, steps per second: 6, episode reward: -39.539, mean reward: -39.539 [-39.539, -39.539], mean action: 7.000 [7.000, 7.000], mean observation: 0.381 [0.000, 3.000], loss: 1078.876465, mean_absolute_error: 3.821577, mean_q: 1.117871
step 165; T: 92400; average reward: 4.92 - action: nw; reward: 79.57
  103/2000: episode: 65, duration: 0.172s, episode steps: 1, steps per second: 6, episode reward: 79.569, mean reward: 79.569 [79.569, 79.569], mean action: 7.000 [7.000, 7.000], mean observation: 0.160 [0.000, 3.000], loss: 605.977600, mean_absolute_error: 2.945975, mean_q: 0.915225
step 166; T: 94350; average reward: 5.17 - action: e; reward: 20.99
  104/2000: episode: 66, duration: 0.252s, episode steps: 1, steps per second: 4, episode reward: 20.988, mean reward: 20.988 [20.988, 20.988], mean action: 2.000 [2.000, 2.000], mean observation: 0.197 [0.000, 3.000], loss: 534.507996, mean_absolute_error: 2.794265, mean_q: 1.026702
step 169; T: 95850; average reward: 5.23 - action: s; reward: 9.33
  106/2000: episode: 67, duration: 0.446s, episode steps: 2, steps per second: 4, episode reward: -0.675, mean reward: -0.337 [-10.000, 9.325], mean action: 6.000 [4.000, 8.000], mean observation: 0.265 [0.000, 3.000], loss: 474.861084, mean_absolute_error: 2.669389, mean_q: 0.906546
step 172; T: 97350; average reward: 6.03 - action: n; reward: 59.56
  108/2000: episode: 68, duration: 0.830s, episode steps: 2, steps per second: 2, episode reward: 49.565, mean reward: 24.782 [-10.000, 59.565], mean action: 7.500 [7.000, 8.000], mean observation: 0.356 [0.000, 3.000], loss: 599.333130, mean_absolute_error: 2.768641, mean_q: 0.914003
step 174; T: 98700; average reward: 5.96 - action: nw; reward: 1.26
  109/2000: episode: 69, duration: 0.369s, episode steps: 1, steps per second: 3, episode reward: -3.744, mean reward: -3.744 [-3.744, -3.744], mean action: 7.000 [7.000, 7.000], mean observation: 0.419 [0.000, 3.000], loss: 1041.766968, mean_absolute_error: 4.123278, mean_q: 0.979991
step 180; T: 100800; average reward: 5.80 - action: n; reward: -5.00
  112/2000: episode: 70, duration: 0.851s, episode steps: 3, steps per second: 4, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.667 [7.000, 8.000], mean observation: 0.363 [0.000, 4.000], loss: 876.819031, mean_absolute_error: 3.538046, mean_q: 1.100779
step 185; T: 102450; average reward: 5.65 - action: n; reward: -5.00
  115/2000: episode: 71, duration: 0.770s, episode steps: 3, steps per second: 4, episode reward: -25.000, mean reward: -8.333 [-10.000, -5.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.217 [0.000, 4.000], loss: 571.051880, mean_absolute_error: 2.931778, mean_q: 1.075454
step 193; T: 104100; average reward: 5.50 - action: sw; reward: -5.00
  119/2000: episode: 72, duration: 1.244s, episode steps: 4, steps per second: 3, episode reward: -40.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.000 [5.000, 7.000], mean observation: 0.309 [0.000, 4.000], loss: 732.919006, mean_absolute_error: 3.437719, mean_q: 1.323480
step 195; T: 105900; average reward: 5.36 - action: nw; reward: -5.00
  120/2000: episode: 73, duration: 0.419s, episode steps: 1, steps per second: 2, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.227 [0.000, 3.000], loss: 664.608643, mean_absolute_error: 3.349544, mean_q: 1.463874
step 196; T: 106500; average reward: 5.76 - action: n; reward: 34.93
  121/2000: episode: 74, duration: 0.180s, episode steps: 1, steps per second: 6, episode reward: 34.928, mean reward: 34.928 [34.928, 34.928], mean action: 8.000 [8.000, 8.000], mean observation: 0.251 [0.000, 3.000], loss: 684.929443, mean_absolute_error: 3.141590, mean_q: 1.609399
step 200; T: 107400; average reward: 5.61 - action: n; reward: -5.00
  123/2000: episode: 75, duration: 0.536s, episode steps: 2, steps per second: 4, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.287 [0.000, 3.000], loss: 610.964355, mean_absolute_error: 3.247201, mean_q: 1.627035
step 201; T: 109050; average reward: 7.44 - action: n; reward: 144.70
  124/2000: episode: 76, duration: 0.264s, episode steps: 1, steps per second: 4, episode reward: 144.703, mean reward: 144.703 [144.703, 144.703], mean action: 8.000 [8.000, 8.000], mean observation: 0.341 [0.000, 3.000], loss: 447.035919, mean_absolute_error: 2.617362, mean_q: 1.609468
step 207; T: 111300; average reward: 7.28 - action: nw; reward: -5.00
  127/2000: episode: 77, duration: 0.925s, episode steps: 3, steps per second: 3, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.333 [7.000, 8.000], mean observation: 0.272 [0.000, 4.000], loss: 694.589661, mean_absolute_error: 3.304698, mean_q: 1.720988
step 210; T: 112500; average reward: 7.51 - action: n; reward: 24.74
  129/2000: episode: 78, duration: 0.513s, episode steps: 2, steps per second: 4, episode reward: 14.744, mean reward: 7.372 [-10.000, 24.744], mean action: 7.500 [7.000, 8.000], mean observation: 0.329 [0.000, 3.000], loss: 648.688232, mean_absolute_error: 3.134935, mean_q: 1.839586
step 212; T: 113550; average reward: 7.84 - action: nw; reward: 33.98
  130/2000: episode: 79, duration: 0.325s, episode steps: 1, steps per second: 3, episode reward: 28.978, mean reward: 28.978 [28.978, 28.978], mean action: 7.000 [7.000, 7.000], mean observation: 0.485 [0.000, 3.000], loss: 639.201843, mean_absolute_error: 3.006037, mean_q: 1.982147
step 214; T: 114150; average reward: 8.08 - action: nw; reward: 27.17
  131/2000: episode: 80, duration: 0.280s, episode steps: 1, steps per second: 4, episode reward: 22.171, mean reward: 22.171 [22.171, 22.171], mean action: 7.000 [7.000, 7.000], mean observation: 0.341 [0.000, 3.000], loss: 988.173706, mean_absolute_error: 3.934426, mean_q: 2.065258
step 215; T: 114900; average reward: 8.18 - action: n; reward: 15.90
  132/2000: episode: 81, duration: 0.186s, episode steps: 1, steps per second: 5, episode reward: 15.895, mean reward: 15.895 [15.895, 15.895], mean action: 8.000 [8.000, 8.000], mean observation: 0.285 [0.000, 3.000], loss: 527.899353, mean_absolute_error: 3.372542, mean_q: 2.328302
step 216; T: 116400; average reward: 8.57 - action: nw; reward: 39.86
  133/2000: episode: 82, duration: 0.219s, episode steps: 1, steps per second: 5, episode reward: 39.860, mean reward: 39.860 [39.860, 39.860], mean action: 7.000 [7.000, 7.000], mean observation: 0.256 [0.000, 4.000], loss: 419.642944, mean_absolute_error: 2.796484, mean_q: 1.999201
step 224; T: 118650; average reward: 8.40 - action: n; reward: -5.00
  137/2000: episode: 83, duration: 1.158s, episode steps: 4, steps per second: 3, episode reward: -40.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.250 [0.000, 4.000], loss: 556.178467, mean_absolute_error: 3.493185, mean_q: 2.426773
step 226; T: 120750; average reward: 8.24 - action: n; reward: -5.00
  138/2000: episode: 84, duration: 0.393s, episode steps: 1, steps per second: 3, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.232 [0.000, 3.000], loss: 704.204590, mean_absolute_error: 3.446593, mean_q: 2.603447
step 228; T: 122100; average reward: 8.14 - action: nw; reward: -0.69
  139/2000: episode: 85, duration: 0.347s, episode steps: 1, steps per second: 3, episode reward: -5.695, mean reward: -5.695 [-5.695, -5.695], mean action: 7.000 [7.000, 7.000], mean observation: 0.528 [0.000, 4.000], loss: 376.757263, mean_absolute_error: 2.907992, mean_q: 2.453232
step 229; T: 123600; average reward: 8.11 - action: nw; reward: 6.16
  140/2000: episode: 86, duration: 0.219s, episode steps: 1, steps per second: 5, episode reward: 6.156, mean reward: 6.156 [6.156, 6.156], mean action: 7.000 [7.000, 7.000], mean observation: 0.221 [0.000, 2.000], loss: 826.323975, mean_absolute_error: 3.679005, mean_q: 2.474696
step 230; T: 124650; average reward: 9.51 - action: n; reward: 129.71
  141/2000: episode: 87, duration: 0.223s, episode steps: 1, steps per second: 4, episode reward: 129.706, mean reward: 129.706 [129.706, 129.706], mean action: 8.000 [8.000, 8.000], mean observation: 0.179 [0.000, 4.000], loss: 679.202820, mean_absolute_error: 3.521890, mean_q: 2.875797
step 232; T: 125400; average reward: 9.34 - action: noop; reward: -5.23
  142/2000: episode: 88, duration: 0.344s, episode steps: 1, steps per second: 3, episode reward: -5.230, mean reward: -5.230 [-5.230, -5.230], mean action: 0.000 [0.000, 0.000], mean observation: 0.373 [0.000, 4.000], loss: 908.252869, mean_absolute_error: 3.956572, mean_q: 2.835558
step 233; T: 126450; average reward: 9.56 - action: nw; reward: 28.42
  143/2000: episode: 89, duration: 0.220s, episode steps: 1, steps per second: 5, episode reward: 28.424, mean reward: 28.424 [28.424, 28.424], mean action: 7.000 [7.000, 7.000], mean observation: 0.413 [0.000, 3.000], loss: 460.548920, mean_absolute_error: 3.173867, mean_q: 2.993804
step 234; T: 127050; average reward: 9.82 - action: nw; reward: 33.21
  144/2000: episode: 90, duration: 0.198s, episode steps: 1, steps per second: 5, episode reward: 33.206, mean reward: 33.206 [33.206, 33.206], mean action: 7.000 [7.000, 7.000], mean observation: 0.408 [0.000, 4.000], loss: 758.688354, mean_absolute_error: 4.083567, mean_q: 3.319321
step 237; T: 127950; average reward: 9.42 - action: w; reward: -26.73
  146/2000: episode: 91, duration: 0.490s, episode steps: 2, steps per second: 4, episode reward: -36.730, mean reward: -18.365 [-26.730, -10.000], mean action: 7.000 [6.000, 8.000], mean observation: 0.345 [0.000, 4.000], loss: 502.381653, mean_absolute_error: 3.484666, mean_q: 3.532777
step 239; T: 129750; average reward: 10.03 - action: n; reward: 65.18
  147/2000: episode: 92, duration: 0.551s, episode steps: 1, steps per second: 2, episode reward: 60.175, mean reward: 60.175 [60.175, 60.175], mean action: 8.000 [8.000, 8.000], mean observation: 0.336 [0.000, 3.000], loss: 422.458862, mean_absolute_error: 3.473425, mean_q: 3.346364
step 242; T: 130500; average reward: 9.71 - action: n; reward: -19.27
  149/2000: episode: 93, duration: 0.465s, episode steps: 2, steps per second: 4, episode reward: -29.270, mean reward: -14.635 [-19.270, -10.000], mean action: 7.500 [7.000, 8.000], mean observation: 0.451 [0.000, 3.000], loss: 481.858337, mean_absolute_error: 3.709700, mean_q: 3.751009
step 245; T: 133800; average reward: 10.73 - action: n; reward: 105.70
  151/2000: episode: 94, duration: 0.635s, episode steps: 2, steps per second: 3, episode reward: 95.705, mean reward: 47.852 [-10.000, 105.705], mean action: 7.500 [7.000, 8.000], mean observation: 0.443 [0.000, 3.000], loss: 245.607391, mean_absolute_error: 3.223095, mean_q: 3.636162
step 246; T: 135300; average reward: 10.85 - action: n; reward: 22.28
  152/2000: episode: 95, duration: 0.249s, episode steps: 1, steps per second: 4, episode reward: 22.279, mean reward: 22.279 [22.279, 22.279], mean action: 8.000 [8.000, 8.000], mean observation: 0.197 [0.000, 4.000], loss: 502.358948, mean_absolute_error: 4.167821, mean_q: 4.360085
step 253; T: 136800; average reward: 10.69 - action: n; reward: -5.00
  156/2000: episode: 96, duration: 1.246s, episode steps: 4, steps per second: 3, episode reward: -35.000, mean reward: -8.750 [-10.000, -5.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.258 [0.000, 4.000], loss: 717.369263, mean_absolute_error: 4.355831, mean_q: 4.258814
step 254; T: 137250; average reward: 10.94 - action: n; reward: 34.93
  157/2000: episode: 97, duration: 0.243s, episode steps: 1, steps per second: 4, episode reward: 34.934, mean reward: 34.934 [34.934, 34.934], mean action: 8.000 [8.000, 8.000], mean observation: 0.339 [0.000, 3.000], loss: 1134.959229, mean_absolute_error: 5.011393, mean_q: 4.294145
step 260; T: 138750; average reward: 10.78 - action: n; reward: -5.00
  160/2000: episode: 98, duration: 1.223s, episode steps: 3, steps per second: 2, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.667 [7.000, 8.000], mean observation: 0.217 [0.000, 3.000], loss: 577.595398, mean_absolute_error: 4.267204, mean_q: 4.902433
step 262; T: 139650; average reward: 11.39 - action: nw; reward: 71.54
  161/2000: episode: 99, duration: 0.401s, episode steps: 1, steps per second: 2, episode reward: 66.544, mean reward: 66.544 [66.544, 66.544], mean action: 7.000 [7.000, 7.000], mean observation: 0.216 [0.000, 3.000], loss: 548.570190, mean_absolute_error: 4.266621, mean_q: 4.373039
step 264; T: 140100; average reward: 9.97 - action: n; reward: -130.92
  162/2000: episode: 100, duration: 0.369s, episode steps: 1, steps per second: 3, episode reward: -135.923, mean reward: -135.923 [-135.923, -135.923], mean action: 8.000 [8.000, 8.000], mean observation: 0.280 [0.000, 4.000], loss: 839.877258, mean_absolute_error: 4.941667, mean_q: 5.292544
step 271; T: 142950; average reward: 9.82 - action: nw; reward: -5.00
  166/2000: episode: 101, duration: 1.708s, episode steps: 4, steps per second: 2, episode reward: -35.000, mean reward: -8.750 [-10.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.290 [0.000, 4.000], loss: 680.412048, mean_absolute_error: 4.692176, mean_q: 5.670368
step 273; T: 144450; average reward: 9.67 - action: n; reward: -5.00
  167/2000: episode: 102, duration: 0.475s, episode steps: 1, steps per second: 2, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.157 [0.000, 2.000], loss: 500.924927, mean_absolute_error: 5.013376, mean_q: 6.559372
step 276; T: 145200; average reward: 9.53 - action: nw; reward: -5.00
  169/2000: episode: 103, duration: 0.527s, episode steps: 2, steps per second: 4, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.244 [0.000, 3.000], loss: 749.322693, mean_absolute_error: 5.196964, mean_q: 5.824762
step 282; T: 148200; average reward: 9.39 - action: w; reward: -5.00
  172/2000: episode: 104, duration: 1.208s, episode steps: 3, steps per second: 2, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [6.000, 8.000], mean observation: 0.266 [0.000, 3.000], loss: 588.368286, mean_absolute_error: 4.842996, mean_q: 6.494743
step 284; T: 150300; average reward: 9.16 - action: w; reward: -15.05
  173/2000: episode: 105, duration: 0.417s, episode steps: 1, steps per second: 2, episode reward: -20.053, mean reward: -20.053 [-20.053, -20.053], mean action: 6.000 [6.000, 6.000], mean observation: 0.371 [0.000, 3.000], loss: 859.235474, mean_absolute_error: 5.537524, mean_q: 6.420528
step 287; T: 151350; average reward: 10.07 - action: n; reward: 106.25
  175/2000: episode: 106, duration: 0.547s, episode steps: 2, steps per second: 4, episode reward: 96.247, mean reward: 48.124 [-10.000, 106.247], mean action: 7.500 [7.000, 8.000], mean observation: 0.341 [0.000, 3.000], loss: 381.614258, mean_absolute_error: 4.706093, mean_q: 6.536511
step 290; T: 152400; average reward: 9.85 - action: nw; reward: -14.13
  177/2000: episode: 107, duration: 0.581s, episode steps: 2, steps per second: 3, episode reward: -24.133, mean reward: -12.066 [-14.133, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.433 [0.000, 3.000], loss: 757.094238, mean_absolute_error: 5.446985, mean_q: 7.262741
step 291; T: 153000; average reward: 9.46 - action: nw; reward: -31.84
  178/2000: episode: 108, duration: 0.201s, episode steps: 1, steps per second: 5, episode reward: -31.837, mean reward: -31.837 [-31.837, -31.837], mean action: 7.000 [7.000, 7.000], mean observation: 0.296 [0.000, 4.000], loss: 615.686401, mean_absolute_error: 5.104804, mean_q: 7.038082
step 292; T: 154050; average reward: 9.58 - action: n; reward: 22.50
  179/2000: episode: 109, duration: 0.262s, episode steps: 1, steps per second: 4, episode reward: 22.503, mean reward: 22.503 [22.503, 22.503], mean action: 8.000 [8.000, 8.000], mean observation: 0.213 [0.000, 2.000], loss: 807.028320, mean_absolute_error: 5.798482, mean_q: 7.674497
step 293; T: 156150; average reward: 9.17 - action: s; reward: -35.89
  180/2000: episode: 110, duration: 0.297s, episode steps: 1, steps per second: 3, episode reward: -35.887, mean reward: -35.887 [-35.887, -35.887], mean action: 4.000 [4.000, 4.000], mean observation: 0.221 [0.000, 4.000], loss: 769.436523, mean_absolute_error: 6.071101, mean_q: 8.386475
step 300; T: 158100; average reward: 9.83 - action: n; reward: 82.74
  184/2000: episode: 111, duration: 1.280s, episode steps: 4, steps per second: 3, episode reward: 52.742, mean reward: 13.185 [-10.000, 82.742], mean action: 7.500 [7.000, 8.000], mean observation: 0.353 [0.000, 4.000], loss: 515.769043, mean_absolute_error: 5.795840, mean_q: 8.561500
step 301; T: 158550; average reward: 9.76 - action: n; reward: 1.76
  185/2000: episode: 112, duration: 0.210s, episode steps: 1, steps per second: 5, episode reward: 1.757, mean reward: 1.757 [1.757, 1.757], mean action: 8.000 [8.000, 8.000], mean observation: 0.216 [0.000, 2.000], loss: 480.430359, mean_absolute_error: 5.416926, mean_q: 8.446596
step 302; T: 159300; average reward: 9.88 - action: n; reward: 23.82
  186/2000: episode: 113, duration: 0.310s, episode steps: 1, steps per second: 3, episode reward: 23.817, mean reward: 23.817 [23.817, 23.817], mean action: 8.000 [8.000, 8.000], mean observation: 0.232 [0.000, 2.000], loss: 806.980286, mean_absolute_error: 6.280362, mean_q: 8.623798
step 303; T: 159750; average reward: 10.06 - action: n; reward: 30.12
  187/2000: episode: 114, duration: 0.193s, episode steps: 1, steps per second: 5, episode reward: 30.118, mean reward: 30.118 [30.118, 30.118], mean action: 8.000 [8.000, 8.000], mean observation: 0.173 [0.000, 3.000], loss: 377.088501, mean_absolute_error: 5.895185, mean_q: 9.310328
step 307; T: 161850; average reward: 10.06 - action: nw; reward: 9.72
  189/2000: episode: 115, duration: 0.861s, episode steps: 2, steps per second: 2, episode reward: -5.284, mean reward: -2.642 [-10.000, 4.716], mean action: 7.500 [7.000, 8.000], mean observation: 0.268 [0.000, 4.000], loss: 875.939941, mean_absolute_error: 6.364440, mean_q: 9.155389
step 320; T: 163950; average reward: 9.93 - action: n; reward: -5.00
  196/2000: episode: 116, duration: 2.479s, episode steps: 7, steps per second: 3, episode reward: -65.000, mean reward: -9.286 [-10.000, -5.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.195 [0.000, 4.000], loss: 562.577698, mean_absolute_error: 6.318585, mean_q: 9.697455
step 322; T: 166350; average reward: 10.12 - action: nw; reward: 31.87
  197/2000: episode: 117, duration: 0.491s, episode steps: 1, steps per second: 2, episode reward: 26.868, mean reward: 26.868 [26.868, 26.868], mean action: 7.000 [7.000, 7.000], mean observation: 0.173 [0.000, 2.000], loss: 707.315430, mean_absolute_error: 6.144582, mean_q: 8.515127
step 323; T: 168300; average reward: 8.74 - action: n; reward: -152.38
  198/2000: episode: 118, duration: 0.291s, episode steps: 1, steps per second: 3, episode reward: -152.378, mean reward: -152.378 [-152.378, -152.378], mean action: 8.000 [8.000, 8.000], mean observation: 0.251 [0.000, 2.000], loss: 794.796021, mean_absolute_error: 6.596309, mean_q: 10.571938
step 326; T: 169650; average reward: 9.35 - action: n; reward: 81.83
  200/2000: episode: 119, duration: 0.666s, episode steps: 2, steps per second: 3, episode reward: 71.825, mean reward: 35.913 [-10.000, 81.825], mean action: 8.000 [8.000, 8.000], mean observation: 0.353 [0.000, 5.000], loss: 794.997681, mean_absolute_error: 6.914062, mean_q: 11.055014
step 328; T: 170400; average reward: 9.21 - action: sw; reward: -8.25
  201/2000: episode: 120, duration: 0.459s, episode steps: 1, steps per second: 2, episode reward: -13.248, mean reward: -13.248 [-13.248, -13.248], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 3.000], loss: 594.360657, mean_absolute_error: 6.660302, mean_q: 10.812844
step 329; T: 171150; average reward: 9.38 - action: n; reward: 30.62
  202/2000: episode: 121, duration: 0.273s, episode steps: 1, steps per second: 4, episode reward: 30.624, mean reward: 30.624 [30.624, 30.624], mean action: 8.000 [8.000, 8.000], mean observation: 0.232 [0.000, 2.000], loss: 322.166199, mean_absolute_error: 5.878722, mean_q: 11.614657
step 331; T: 173400; average reward: 9.77 - action: e; reward: 57.02
  203/2000: episode: 122, duration: 1.032s, episode steps: 1, steps per second: 1, episode reward: 52.020, mean reward: 52.020 [52.020, 52.020], mean action: 2.000 [2.000, 2.000], mean observation: 0.331 [0.000, 3.000], loss: 557.199768, mean_absolute_error: 7.176823, mean_q: 12.115000
step 333; T: 174600; average reward: 9.89 - action: n; reward: 23.67
  204/2000: episode: 123, duration: 0.516s, episode steps: 1, steps per second: 2, episode reward: 18.670, mean reward: 18.670 [18.670, 18.670], mean action: 8.000 [8.000, 8.000], mean observation: 0.277 [0.000, 2.000], loss: 661.157715, mean_absolute_error: 6.842553, mean_q: 10.756773
step 334; T: 175950; average reward: 10.13 - action: n; reward: 39.55
  205/2000: episode: 124, duration: 0.299s, episode steps: 1, steps per second: 3, episode reward: 39.552, mean reward: 39.552 [39.552, 39.552], mean action: 8.000 [8.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 886.485046, mean_absolute_error: 7.181389, mean_q: 10.718492
step 338; T: 177750; average reward: 9.49 - action: n; reward: -69.04
  207/2000: episode: 125, duration: 0.908s, episode steps: 2, steps per second: 2, episode reward: -84.042, mean reward: -42.021 [-74.042, -10.000], mean action: 5.500 [3.000, 8.000], mean observation: 0.336 [0.000, 3.000], loss: 944.262573, mean_absolute_error: 7.364507, mean_q: 11.441023
step 340; T: 178500; average reward: 8.61 - action: n; reward: -101.48
  208/2000: episode: 126, duration: 0.480s, episode steps: 1, steps per second: 2, episode reward: -106.483, mean reward: -106.483 [-106.483, -106.483], mean action: 8.000 [8.000, 8.000], mean observation: 0.256 [0.000, 3.000], loss: 749.306641, mean_absolute_error: 6.759436, mean_q: 9.903625
step 342; T: 180000; average reward: 8.50 - action: n; reward: -5.00
  209/2000: episode: 127, duration: 0.576s, episode steps: 1, steps per second: 2, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.197 [0.000, 2.000], loss: 247.840271, mean_absolute_error: 6.370711, mean_q: 11.601757
step 344; T: 180900; average reward: 8.42 - action: n; reward: -2.12
  210/2000: episode: 128, duration: 0.570s, episode steps: 1, steps per second: 2, episode reward: -7.125, mean reward: -7.125 [-7.125, -7.125], mean action: 8.000 [8.000, 8.000], mean observation: 0.408 [0.000, 3.000], loss: 774.514404, mean_absolute_error: 6.919340, mean_q: 10.768139
step 348; T: 182250; average reward: 8.32 - action: n; reward: -5.00
  212/2000: episode: 129, duration: 0.924s, episode steps: 2, steps per second: 2, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.253 [0.000, 3.000], loss: 583.041565, mean_absolute_error: 7.177746, mean_q: 11.821685
step 353; T: 183900; average reward: 8.64 - action: e; reward: 49.62
  215/2000: episode: 130, duration: 1.099s, episode steps: 3, steps per second: 3, episode reward: 29.622, mean reward: 9.874 [-10.000, 49.622], mean action: 6.000 [2.000, 8.000], mean observation: 0.353 [0.000, 3.000], loss: 662.331543, mean_absolute_error: 7.225023, mean_q: 11.218030
step 369; T: 186750; average reward: 8.53 - action: n; reward: -5.00
  223/2000: episode: 131, duration: 3.442s, episode steps: 8, steps per second: 2, episode reward: -80.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.293 [0.000, 3.000], loss: 623.377197, mean_absolute_error: 7.302458, mean_q: 11.455344
step 370; T: 189300; average reward: 8.60 - action: n; reward: 17.93
  224/2000: episode: 132, duration: 0.371s, episode steps: 1, steps per second: 3, episode reward: 17.934, mean reward: 17.934 [17.934, 17.934], mean action: 8.000 [8.000, 8.000], mean observation: 0.397 [0.000, 3.000], loss: 644.118774, mean_absolute_error: 7.669356, mean_q: 12.031315
step 372; T: 189750; average reward: 8.50 - action: n; reward: -5.00
  225/2000: episode: 133, duration: 0.534s, episode steps: 1, steps per second: 2, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.464 [0.000, 5.000], loss: 863.751770, mean_absolute_error: 7.391894, mean_q: 10.249586
step 377; T: 190500; average reward: 8.40 - action: n; reward: -5.00
  228/2000: episode: 134, duration: 1.104s, episode steps: 3, steps per second: 3, episode reward: -25.000, mean reward: -8.333 [-10.000, -5.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.412 [0.000, 5.000], loss: 543.795532, mean_absolute_error: 6.869556, mean_q: 9.800824
step 380; T: 191100; average reward: 8.51 - action: nw; reward: 23.25
  230/2000: episode: 135, duration: 0.835s, episode steps: 2, steps per second: 2, episode reward: 13.247, mean reward: 6.624 [-10.000, 23.247], mean action: 4.500 [2.000, 7.000], mean observation: 0.313 [0.000, 5.000], loss: 358.798950, mean_absolute_error: 6.837325, mean_q: 9.895226
step 382; T: 192300; average reward: 8.41 - action: n; reward: -5.00
  231/2000: episode: 136, duration: 0.457s, episode steps: 1, steps per second: 2, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.253 [0.000, 3.000], loss: 1211.886108, mean_absolute_error: 7.253184, mean_q: 8.167439
step 388; T: 195750; average reward: 8.31 - action: n; reward: -5.00
  234/2000: episode: 137, duration: 1.664s, episode steps: 3, steps per second: 2, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.667 [7.000, 8.000], mean observation: 0.353 [0.000, 3.000], loss: 385.077728, mean_absolute_error: 6.527219, mean_q: 8.453868
step 389; T: 196800; average reward: 8.75 - action: n; reward: 69.03
  235/2000: episode: 138, duration: 0.277s, episode steps: 1, steps per second: 4, episode reward: 69.028, mean reward: 69.028 [69.028, 69.028], mean action: 8.000 [8.000, 8.000], mean observation: 0.205 [0.000, 2.000], loss: 389.935333, mean_absolute_error: 6.913274, mean_q: 9.494188
step 391; T: 198900; average reward: 8.87 - action: n; reward: 25.29
  236/2000: episode: 139, duration: 0.678s, episode steps: 1, steps per second: 1, episode reward: 20.293, mean reward: 20.293 [20.293, 20.293], mean action: 8.000 [8.000, 8.000], mean observation: 0.379 [0.000, 3.000], loss: 976.442444, mean_absolute_error: 8.263041, mean_q: 9.112144
step 394; T: 199650; average reward: 8.77 - action: n; reward: -5.00
  238/2000: episode: 140, duration: 0.705s, episode steps: 2, steps per second: 3, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.264 [0.000, 3.000], loss: 761.166748, mean_absolute_error: 7.179819, mean_q: 8.756789
step 395; T: 200100; average reward: 9.42 - action: n; reward: 100.76
  239/2000: episode: 141, duration: 0.267s, episode steps: 1, steps per second: 4, episode reward: 100.759, mean reward: 100.759 [100.759, 100.759], mean action: 8.000 [8.000, 8.000], mean observation: 0.283 [0.000, 3.000], loss: 534.151123, mean_absolute_error: 7.775064, mean_q: 10.157683
step 397; T: 201450; average reward: 9.32 - action: n; reward: -5.00
  240/2000: episode: 142, duration: 0.548s, episode steps: 1, steps per second: 2, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.160 [0.000, 4.000], loss: 528.184937, mean_absolute_error: 6.525097, mean_q: 7.524187
step 403; T: 202650; average reward: 9.33 - action: n; reward: 10.18
  243/2000: episode: 143, duration: 1.391s, episode steps: 3, steps per second: 2, episode reward: -4.817, mean reward: -1.606 [-10.000, 5.183], mean action: 4.000 [0.000, 8.000], mean observation: 0.232 [0.000, 4.000], loss: 526.927490, mean_absolute_error: 6.819010, mean_q: 8.869006
step 404; T: 203550; average reward: 9.80 - action: nw; reward: 77.83
  244/2000: episode: 144, duration: 0.278s, episode steps: 1, steps per second: 4, episode reward: 77.834, mean reward: 77.834 [77.834, 77.834], mean action: 7.000 [7.000, 7.000], mean observation: 0.147 [0.000, 3.000], loss: 355.749115, mean_absolute_error: 6.343799, mean_q: 8.182756
step 405; T: 204750; average reward: 9.65 - action: n; reward: -12.93
  245/2000: episode: 145, duration: 0.389s, episode steps: 1, steps per second: 3, episode reward: -12.929, mean reward: -12.929 [-12.929, -12.929], mean action: 8.000 [8.000, 8.000], mean observation: 0.411 [0.000, 4.000], loss: 370.203003, mean_absolute_error: 6.501843, mean_q: 8.180403
step 406; T: 206550; average reward: 8.27 - action: n; reward: -191.66
  246/2000: episode: 146, duration: 0.390s, episode steps: 1, steps per second: 3, episode reward: -191.664, mean reward: -191.664 [-191.664, -191.664], mean action: 8.000 [8.000, 8.000], mean observation: 0.277 [0.000, 2.000], loss: 566.053589, mean_absolute_error: 8.030468, mean_q: 10.822093
step 409; T: 209100; average reward: 7.25 - action: n; reward: -141.26
  248/2000: episode: 147, duration: 0.829s, episode steps: 2, steps per second: 2, episode reward: -151.264, mean reward: -75.632 [-141.264, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.401 [0.000, 3.000], loss: 933.310791, mean_absolute_error: 7.327678, mean_q: 8.567900
step 412; T: 211050; average reward: 7.63 - action: nw; reward: 63.25
  250/2000: episode: 148, duration: 0.813s, episode steps: 2, steps per second: 2, episode reward: 53.250, mean reward: 26.625 [-10.000, 63.250], mean action: 7.500 [7.000, 8.000], mean observation: 0.269 [0.000, 3.000], loss: 741.096680, mean_absolute_error: 7.199742, mean_q: 7.701793
step 414; T: 211650; average reward: 8.31 - action: n; reward: 108.35
  251/2000: episode: 149, duration: 0.493s, episode steps: 1, steps per second: 2, episode reward: 103.352, mean reward: 103.352 [103.352, 103.352], mean action: 8.000 [8.000, 8.000], mean observation: 0.456 [0.000, 3.000], loss: 1048.633423, mean_absolute_error: 7.698818, mean_q: 7.017167
step 415; T: 213000; average reward: 8.67 - action: n; reward: 62.91
  252/2000: episode: 150, duration: 0.338s, episode steps: 1, steps per second: 3, episode reward: 62.911, mean reward: 62.911 [62.911, 62.911], mean action: 8.000 [8.000, 8.000], mean observation: 0.251 [0.000, 3.000], loss: 646.469299, mean_absolute_error: 6.668143, mean_q: 8.622863
step 417; T: 214050; average reward: 8.61 - action: noop; reward: 0.00
  253/2000: episode: 151, duration: 0.524s, episode steps: 1, steps per second: 2, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.309 [0.000, 2.000], loss: 586.594543, mean_absolute_error: 7.392663, mean_q: 8.292148
step 418; T: 214650; average reward: 8.55 - action: n; reward: -0.27
  254/2000: episode: 152, duration: 0.274s, episode steps: 1, steps per second: 4, episode reward: -0.274, mean reward: -0.274 [-0.274, -0.274], mean action: 8.000 [8.000, 8.000], mean observation: 0.387 [0.000, 3.000], loss: 865.082458, mean_absolute_error: 7.346441, mean_q: 7.414573
step 421; T: 215700; average reward: 8.50 - action: noop; reward: 0.00
  256/2000: episode: 153, duration: 0.771s, episode steps: 2, steps per second: 3, episode reward: -10.000, mean reward: -5.000 [-10.000, 0.000], mean action: 3.500 [0.000, 7.000], mean observation: 0.344 [0.000, 3.000], loss: 407.226776, mean_absolute_error: 6.555768, mean_q: 8.117777
step 422; T: 216600; average reward: 8.83 - action: n; reward: 59.44
  257/2000: episode: 154, duration: 0.284s, episode steps: 1, steps per second: 4, episode reward: 59.442, mean reward: 59.442 [59.442, 59.442], mean action: 8.000 [8.000, 8.000], mean observation: 0.293 [0.000, 3.000], loss: 499.821289, mean_absolute_error: 7.013062, mean_q: 7.445404
step 425; T: 217200; average reward: 9.00 - action: n; reward: 35.49
  259/2000: episode: 155, duration: 0.851s, episode steps: 2, steps per second: 2, episode reward: 25.488, mean reward: 12.744 [-10.000, 35.488], mean action: 6.500 [5.000, 8.000], mean observation: 0.237 [0.000, 3.000], loss: 473.186890, mean_absolute_error: 7.024788, mean_q: 7.368967
step 427; T: 218100; average reward: 8.98 - action: n; reward: 5.10
  260/2000: episode: 156, duration: 0.547s, episode steps: 1, steps per second: 2, episode reward: 0.098, mean reward: 0.098 [0.098, 0.098], mean action: 8.000 [8.000, 8.000], mean observation: 0.368 [0.000, 3.000], loss: 406.974060, mean_absolute_error: 6.299162, mean_q: 7.128325
step 431; T: 219450; average reward: 9.28 - action: nw; reward: 56.45
  262/2000: episode: 157, duration: 1.455s, episode steps: 2, steps per second: 1, episode reward: 41.451, mean reward: 20.725 [-10.000, 51.451], mean action: 7.000 [7.000, 7.000], mean observation: 0.415 [0.000, 3.000], loss: 639.672852, mean_absolute_error: 7.024697, mean_q: 7.817210
step 437; T: 222300; average reward: 8.94 - action: nw; reward: -44.62
  265/2000: episode: 158, duration: 1.550s, episode steps: 3, steps per second: 2, episode reward: -69.623, mean reward: -23.208 [-49.623, -10.000], mean action: 7.333 [7.000, 8.000], mean observation: 0.420 [0.000, 4.000], loss: 1035.431763, mean_absolute_error: 8.071192, mean_q: 7.995139
step 440; T: 223800; average reward: 8.99 - action: nw; reward: 16.84
  267/2000: episode: 159, duration: 0.803s, episode steps: 2, steps per second: 2, episode reward: 6.838, mean reward: 3.419 [-10.000, 16.838], mean action: 7.000 [7.000, 7.000], mean observation: 0.377 [0.000, 3.000], loss: 566.933899, mean_absolute_error: 7.373619, mean_q: 8.667529
step 442; T: 225300; average reward: 9.47 - action: nw; reward: 86.60
  268/2000: episode: 160, duration: 0.600s, episode steps: 1, steps per second: 2, episode reward: 81.600, mean reward: 81.600 [81.600, 81.600], mean action: 7.000 [7.000, 7.000], mean observation: 0.355 [0.000, 3.000], loss: 174.712204, mean_absolute_error: 6.695133, mean_q: 9.145039
step 445; T: 226650; average reward: 9.65 - action: nw; reward: 38.47
  270/2000: episode: 161, duration: 1.001s, episode steps: 2, steps per second: 2, episode reward: 28.469, mean reward: 14.234 [-10.000, 38.469], mean action: 7.000 [7.000, 7.000], mean observation: 0.484 [0.000, 3.000], loss: 647.038269, mean_absolute_error: 7.421635, mean_q: 8.397845
step 446; T: 227400; average reward: 9.30 - action: noop; reward: -46.95
  271/2000: episode: 162, duration: 0.310s, episode steps: 1, steps per second: 3, episode reward: -46.948, mean reward: -46.948 [-46.948, -46.948], mean action: 0.000 [0.000, 0.000], mean observation: 0.221 [0.000, 3.000], loss: 377.648560, mean_absolute_error: 7.305083, mean_q: 7.991030
step 447; T: 229500; average reward: 9.03 - action: noop; reward: -35.49
  272/2000: episode: 163, duration: 0.414s, episode steps: 1, steps per second: 2, episode reward: -35.485, mean reward: -35.485 [-35.485, -35.485], mean action: 0.000 [0.000, 0.000], mean observation: 0.291 [0.000, 3.000], loss: 902.865540, mean_absolute_error: 8.610036, mean_q: 8.327520
step 448; T: 230700; average reward: 8.43 - action: n; reward: -89.42
  273/2000: episode: 164, duration: 0.311s, episode steps: 1, steps per second: 3, episode reward: -89.421, mean reward: -89.421 [-89.421, -89.421], mean action: 8.000 [8.000, 8.000], mean observation: 0.216 [0.000, 2.000], loss: 465.961975, mean_absolute_error: 8.058188, mean_q: 9.486729
step 450; T: 231900; average reward: 8.35 - action: nw; reward: -5.00
  274/2000: episode: 165, duration: 0.548s, episode steps: 1, steps per second: 2, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.213 [0.000, 3.000], loss: 997.439209, mean_absolute_error: 8.553720, mean_q: 9.510064
step 455; T: 234900; average reward: 8.27 - action: nw; reward: -5.00
  277/2000: episode: 166, duration: 1.351s, episode steps: 3, steps per second: 2, episode reward: -15.000, mean reward: -5.000 [-10.000, 0.000], mean action: 4.667 [0.000, 7.000], mean observation: 0.255 [0.000, 3.000], loss: 463.987579, mean_absolute_error: 7.444542, mean_q: 8.722960
step 457; T: 235950; average reward: 8.19 - action: n; reward: -5.00
  278/2000: episode: 167, duration: 0.563s, episode steps: 1, steps per second: 2, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.171 [0.000, 3.000], loss: 227.770294, mean_absolute_error: 8.220863, mean_q: 11.509757
step 469; T: 238500; average reward: 8.14 - action: noop; reward: 0.00
  284/2000: episode: 168, duration: 4.029s, episode steps: 6, steps per second: 1, episode reward: -30.000, mean reward: -5.000 [-10.000, 0.000], mean action: 3.333 [0.000, 8.000], mean observation: 0.267 [0.000, 3.000], loss: 647.822571, mean_absolute_error: 7.963902, mean_q: 9.375215
step 470; T: 239700; average reward: 9.12 - action: nw; reward: 174.95
  285/2000: episode: 169, duration: 0.374s, episode steps: 1, steps per second: 3, episode reward: 174.946, mean reward: 174.946 [174.946, 174.946], mean action: 7.000 [7.000, 7.000], mean observation: 0.352 [0.000, 2.000], loss: 667.238770, mean_absolute_error: 8.029809, mean_q: 9.511229
step 477; T: 241350; average reward: 9.07 - action: noop; reward: 0.00
  289/2000: episode: 170, duration: 1.906s, episode steps: 4, steps per second: 2, episode reward: -20.000, mean reward: -5.000 [-10.000, 0.000], mean action: 3.500 [0.000, 7.000], mean observation: 0.311 [0.000, 3.000], loss: 616.787048, mean_absolute_error: 7.754152, mean_q: 9.060096
step 478; T: 242400; average reward: 9.38 - action: nw; reward: 62.11
  290/2000: episode: 171, duration: 0.349s, episode steps: 1, steps per second: 3, episode reward: 62.106, mean reward: 62.106 [62.106, 62.106], mean action: 7.000 [7.000, 7.000], mean observation: 0.267 [0.000, 4.000], loss: 350.142120, mean_absolute_error: 6.946042, mean_q: 7.933720
step 485; T: 244950; average reward: 9.30 - action: ne; reward: -5.00
  294/2000: episode: 172, duration: 2.049s, episode steps: 4, steps per second: 2, episode reward: -35.000, mean reward: -8.750 [-10.000, -5.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.241 [0.000, 4.000], loss: 704.199341, mean_absolute_error: 7.697630, mean_q: 8.352493
step 491; T: 247050; average reward: 9.21 - action: ne; reward: -5.00
  297/2000: episode: 173, duration: 1.737s, episode steps: 3, steps per second: 2, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.186 [0.000, 3.000], loss: 729.083008, mean_absolute_error: 7.931705, mean_q: 8.295997
step 495; T: 247800; average reward: 9.47 - action: ne; reward: 53.89
  299/2000: episode: 174, duration: 1.555s, episode steps: 2, steps per second: 1, episode reward: 38.887, mean reward: 19.444 [-10.000, 48.887], mean action: 4.000 [1.000, 7.000], mean observation: 0.277 [0.000, 2.000], loss: 509.905273, mean_absolute_error: 7.268545, mean_q: 8.217889
step 498; T: 248850; average reward: 9.39 - action: n; reward: -5.00
  301/2000: episode: 175, duration: 1.445s, episode steps: 2, steps per second: 1, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.419 [0.000, 4.000], loss: 871.401672, mean_absolute_error: 7.596079, mean_q: 7.956242
step 499; T: 251250; average reward: 9.35 - action: noop; reward: 3.36
  302/2000: episode: 176, duration: 0.850s, episode steps: 1, steps per second: 1, episode reward: 3.358, mean reward: 3.358 [3.358, 3.358], mean action: 0.000 [0.000, 0.000], mean observation: 0.400 [0.000, 3.000], loss: 445.375763, mean_absolute_error: 7.044019, mean_q: 8.228127
step 501; T: 252150; average reward: 9.51 - action: nw; reward: 36.39
  303/2000: episode: 177, duration: 0.575s, episode steps: 1, steps per second: 2, episode reward: 31.387, mean reward: 31.387 [31.387, 31.387], mean action: 7.000 [7.000, 7.000], mean observation: 0.307 [0.000, 3.000], loss: 658.080322, mean_absolute_error: 8.019824, mean_q: 9.723314
step 504; T: 253800; average reward: 9.31 - action: se; reward: -26.36
  305/2000: episode: 178, duration: 1.283s, episode steps: 2, steps per second: 2, episode reward: -36.361, mean reward: -18.180 [-26.361, -10.000], mean action: 5.500 [3.000, 8.000], mean observation: 0.201 [0.000, 4.000], loss: 509.805786, mean_absolute_error: 7.698091, mean_q: 9.483530
step 523; T: 257100; average reward: 9.23 - action: nw; reward: -5.00
  315/2000: episode: 179, duration: 5.637s, episode steps: 10, steps per second: 2, episode reward: -95.000, mean reward: -9.500 [-10.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.235 [0.000, 3.000], loss: 694.936890, mean_absolute_error: 7.940936, mean_q: 9.689316
step 525; T: 258300; average reward: 9.21 - action: n; reward: 6.37
  316/2000: episode: 180, duration: 0.633s, episode steps: 1, steps per second: 2, episode reward: 1.366, mean reward: 1.366 [1.366, 1.366], mean action: 8.000 [8.000, 8.000], mean observation: 0.224 [0.000, 4.000], loss: 645.115845, mean_absolute_error: 7.086646, mean_q: 8.444014
step 528; T: 260700; average reward: 9.19 - action: nw; reward: 6.08
  318/2000: episode: 181, duration: 0.960s, episode steps: 2, steps per second: 2, episode reward: -3.923, mean reward: -1.961 [-10.000, 6.077], mean action: 7.000 [7.000, 7.000], mean observation: 0.319 [0.000, 3.000], loss: 439.375183, mean_absolute_error: 7.382503, mean_q: 8.986996
step 531; T: 262800; average reward: 9.82 - action: e; reward: 123.71
  320/2000: episode: 182, duration: 0.930s, episode steps: 2, steps per second: 2, episode reward: 113.715, mean reward: 56.857 [-10.000, 123.715], mean action: 5.000 [2.000, 8.000], mean observation: 0.245 [0.000, 4.000], loss: 518.268127, mean_absolute_error: 8.044372, mean_q: 10.072128
step 533; T: 266700; average reward: 9.74 - action: nw; reward: -5.00
  321/2000: episode: 183, duration: 0.871s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.221 [0.000, 3.000], loss: 853.433472, mean_absolute_error: 8.028378, mean_q: 8.566960
step 537; T: 268050; average reward: 9.66 - action: sw; reward: -5.00
  323/2000: episode: 184, duration: 1.139s, episode steps: 2, steps per second: 2, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.000 [5.000, 7.000], mean observation: 0.235 [0.000, 3.000], loss: 983.358704, mean_absolute_error: 8.714800, mean_q: 10.537848
step 539; T: 269400; average reward: 9.81 - action: e; reward: 36.92
  324/2000: episode: 185, duration: 0.611s, episode steps: 1, steps per second: 2, episode reward: 31.923, mean reward: 31.923 [31.923, 31.923], mean action: 2.000 [2.000, 2.000], mean observation: 0.259 [0.000, 3.000], loss: 1298.120361, mean_absolute_error: 8.394218, mean_q: 9.271587
step 541; T: 270300; average reward: 9.86 - action: nw; reward: 19.09
  325/2000: episode: 186, duration: 0.612s, episode steps: 1, steps per second: 2, episode reward: 14.093, mean reward: 14.093 [14.093, 14.093], mean action: 7.000 [7.000, 7.000], mean observation: 0.427 [0.000, 3.000], loss: 305.166138, mean_absolute_error: 7.443162, mean_q: 9.794212
step 549; T: 272850; average reward: 9.78 - action: n; reward: -5.00
  329/2000: episode: 187, duration: 2.521s, episode steps: 4, steps per second: 2, episode reward: -40.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.750 [7.000, 8.000], mean observation: 0.237 [0.000, 3.000], loss: 838.757996, mean_absolute_error: 8.050108, mean_q: 8.655260
step 554; T: 274500; average reward: 9.70 - action: se; reward: -5.00
  332/2000: episode: 188, duration: 1.734s, episode steps: 3, steps per second: 2, episode reward: -25.000, mean reward: -8.333 [-10.000, -5.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.220 [0.000, 2.000], loss: 567.645569, mean_absolute_error: 8.021658, mean_q: 9.982090
step 556; T: 274950; average reward: 9.78 - action: n; reward: 24.69
  333/2000: episode: 189, duration: 0.617s, episode steps: 1, steps per second: 2, episode reward: 19.693, mean reward: 19.693 [19.693, 19.693], mean action: 8.000 [8.000, 8.000], mean observation: 0.328 [0.000, 3.000], loss: 609.712463, mean_absolute_error: 7.984677, mean_q: 10.724183
step 561; T: 277950; average reward: 9.48 - action: n; reward: -47.59
  336/2000: episode: 190, duration: 1.730s, episode steps: 3, steps per second: 2, episode reward: -67.585, mean reward: -22.528 [-47.585, -10.000], mean action: 7.667 [7.000, 8.000], mean observation: 0.320 [0.000, 4.000], loss: 577.499329, mean_absolute_error: 8.538389, mean_q: 11.127358
step 563; T: 279150; average reward: 9.72 - action: nw; reward: 56.57
  337/2000: episode: 191, duration: 0.846s, episode steps: 1, steps per second: 1, episode reward: 51.567, mean reward: 51.567 [51.567, 51.567], mean action: 7.000 [7.000, 7.000], mean observation: 0.213 [0.000, 2.000], loss: 597.246216, mean_absolute_error: 8.918095, mean_q: 11.183294
step 564; T: 279750; average reward: 10.13 - action: n; reward: 88.62
  338/2000: episode: 192, duration: 0.401s, episode steps: 1, steps per second: 2, episode reward: 88.617, mean reward: 88.617 [88.617, 88.617], mean action: 8.000 [8.000, 8.000], mean observation: 0.323 [0.000, 3.000], loss: 225.173096, mean_absolute_error: 8.137477, mean_q: 10.605699
step 565; T: 280800; average reward: 10.70 - action: nw; reward: 118.93
  339/2000: episode: 193, duration: 0.568s, episode steps: 1, steps per second: 2, episode reward: 118.933, mean reward: 118.933 [118.933, 118.933], mean action: 7.000 [7.000, 7.000], mean observation: 0.437 [0.000, 3.000], loss: 534.111023, mean_absolute_error: 8.415049, mean_q: 9.174644
step 567; T: 282150; average reward: 11.06 - action: nw; reward: 81.14
  340/2000: episode: 194, duration: 0.687s, episode steps: 1, steps per second: 1, episode reward: 76.139, mean reward: 76.139 [76.139, 76.139], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 4.000], loss: 327.821472, mean_absolute_error: 8.644297, mean_q: 12.866223
step 569; T: 283650; average reward: 10.98 - action: se; reward: -5.00
  341/2000: episode: 195, duration: 0.745s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.253 [0.000, 2.000], loss: 901.013916, mean_absolute_error: 9.047967, mean_q: 9.759072
step 570; T: 284700; average reward: 10.78 - action: n; reward: -27.53
  342/2000: episode: 196, duration: 0.378s, episode steps: 1, steps per second: 3, episode reward: -27.531, mean reward: -27.531 [-27.531, -27.531], mean action: 8.000 [8.000, 8.000], mean observation: 0.357 [0.000, 3.000], loss: 605.868774, mean_absolute_error: 8.770569, mean_q: 11.441368
step 572; T: 285450; average reward: 10.90 - action: nw; reward: 33.23
  343/2000: episode: 197, duration: 0.660s, episode steps: 1, steps per second: 2, episode reward: 28.229, mean reward: 28.229 [28.229, 28.229], mean action: 7.000 [7.000, 7.000], mean observation: 0.339 [0.000, 3.000], loss: 456.713806, mean_absolute_error: 7.697530, mean_q: 8.998209
step 577; T: 287250; average reward: 11.04 - action: nw; reward: 38.47
  346/2000: episode: 198, duration: 1.663s, episode steps: 3, steps per second: 2, episode reward: 18.470, mean reward: 6.157 [-10.000, 38.470], mean action: 5.667 [3.000, 7.000], mean observation: 0.256 [0.000, 4.000], loss: 491.353516, mean_absolute_error: 8.429709, mean_q: 10.461195
step 579; T: 288000; average reward: 11.04 - action: e; reward: 12.52
  347/2000: episode: 199, duration: 0.743s, episode steps: 1, steps per second: 1, episode reward: 7.525, mean reward: 7.525 [7.525, 7.525], mean action: 2.000 [2.000, 2.000], mean observation: 0.427 [0.000, 4.000], loss: 936.953857, mean_absolute_error: 8.757856, mean_q: 10.154037
step 585; T: 289650; average reward: 10.96 - action: n; reward: -5.00
  350/2000: episode: 200, duration: 1.890s, episode steps: 3, steps per second: 2, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.667 [7.000, 8.000], mean observation: 0.316 [0.000, 3.000], loss: 639.934998, mean_absolute_error: 8.313549, mean_q: 10.823364
step 586; T: 290850; average reward: 11.03 - action: n; reward: 23.89
  351/2000: episode: 201, duration: 0.475s, episode steps: 1, steps per second: 2, episode reward: 23.889, mean reward: 23.889 [23.889, 23.889], mean action: 8.000 [8.000, 8.000], mean observation: 0.277 [0.000, 3.000], loss: 833.941284, mean_absolute_error: 9.501432, mean_q: 9.448286
step 592; T: 293100; average reward: 10.95 - action: nw; reward: -5.00
  354/2000: episode: 202, duration: 1.990s, episode steps: 3, steps per second: 2, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.333 [2.000, 7.000], mean observation: 0.239 [0.000, 3.000], loss: 547.789001, mean_absolute_error: 8.518040, mean_q: 10.246902
step 599; T: 295350; average reward: 10.87 - action: n; reward: -5.00
  358/2000: episode: 203, duration: 2.292s, episode steps: 4, steps per second: 2, episode reward: -35.000, mean reward: -8.750 [-10.000, -5.000], mean action: 4.500 [1.000, 8.000], mean observation: 0.229 [0.000, 3.000], loss: 432.827484, mean_absolute_error: 8.825937, mean_q: 11.623856
step 600; T: 297300; average reward: 10.60 - action: n; reward: -43.83
  359/2000: episode: 204, duration: 0.406s, episode steps: 1, steps per second: 2, episode reward: -43.830, mean reward: -43.830 [-43.830, -43.830], mean action: 8.000 [8.000, 8.000], mean observation: 0.440 [0.000, 3.000], loss: 435.726685, mean_absolute_error: 8.888865, mean_q: 11.758047
step 601; T: 297900; average reward: 10.09 - action: se; reward: -94.14
  360/2000: episode: 205, duration: 0.337s, episode steps: 1, steps per second: 3, episode reward: -94.144, mean reward: -94.144 [-94.144, -94.144], mean action: 3.000 [3.000, 3.000], mean observation: 0.472 [0.000, 3.000], loss: 156.354370, mean_absolute_error: 7.389985, mean_q: 8.582268
step 603; T: 301650; average reward: 10.21 - action: n; reward: 33.80
  361/2000: episode: 206, duration: 0.808s, episode steps: 1, steps per second: 1, episode reward: 28.799, mean reward: 28.799 [28.799, 28.799], mean action: 8.000 [8.000, 8.000], mean observation: 0.459 [0.000, 3.000], loss: 1011.373413, mean_absolute_error: 9.474774, mean_q: 8.683622
step 604; T: 302400; average reward: 10.83 - action: n; reward: 138.66
  362/2000: episode: 207, duration: 0.340s, episode steps: 1, steps per second: 3, episode reward: 138.659, mean reward: 138.659 [138.659, 138.659], mean action: 8.000 [8.000, 8.000], mean observation: 0.323 [0.000, 3.000], loss: 374.647644, mean_absolute_error: 8.509266, mean_q: 9.802372
step 609; T: 303450; average reward: 10.71 - action: n; reward: -12.68
  365/2000: episode: 208, duration: 1.773s, episode steps: 3, steps per second: 2, episode reward: -32.682, mean reward: -10.894 [-12.682, -10.000], mean action: 5.667 [2.000, 8.000], mean observation: 0.332 [0.000, 3.000], loss: 511.795746, mean_absolute_error: 8.934016, mean_q: 10.606393
step 610; T: 304350; average reward: 10.81 - action: n; reward: 31.19
  366/2000: episode: 209, duration: 0.370s, episode steps: 1, steps per second: 3, episode reward: 31.194, mean reward: 31.194 [31.194, 31.194], mean action: 8.000 [8.000, 8.000], mean observation: 0.325 [0.000, 5.000], loss: 682.684448, mean_absolute_error: 9.264427, mean_q: 9.610085
step 611; T: 304650; average reward: 10.86 - action: nw; reward: 20.79
  367/2000: episode: 210, duration: 0.340s, episode steps: 1, steps per second: 3, episode reward: 20.791, mean reward: 20.791 [20.791, 20.791], mean action: 7.000 [7.000, 7.000], mean observation: 0.272 [0.000, 2.000], loss: 410.563782, mean_absolute_error: 9.060251, mean_q: 11.339432
step 613; T: 306900; average reward: 10.91 - action: nw; reward: 20.69
  368/2000: episode: 211, duration: 0.761s, episode steps: 1, steps per second: 1, episode reward: 15.688, mean reward: 15.688 [15.688, 15.688], mean action: 7.000 [7.000, 7.000], mean observation: 0.160 [0.000, 4.000], loss: 351.064789, mean_absolute_error: 9.237171, mean_q: 12.912258
step 616; T: 307950; average reward: 10.83 - action: nw; reward: -5.00
  370/2000: episode: 212, duration: 0.986s, episode steps: 2, steps per second: 2, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.264 [0.000, 4.000], loss: 534.397705, mean_absolute_error: 9.120418, mean_q: 8.949343
step 617; T: 308700; average reward: 10.86 - action: nw; reward: 17.73
  371/2000: episode: 213, duration: 0.375s, episode steps: 1, steps per second: 3, episode reward: 17.725, mean reward: 17.725 [17.725, 17.725], mean action: 7.000 [7.000, 7.000], mean observation: 0.275 [0.000, 2.000], loss: 748.389832, mean_absolute_error: 9.514796, mean_q: 11.113983
step 618; T: 308850; average reward: 10.73 - action: n; reward: -18.11
  372/2000: episode: 214, duration: 0.322s, episode steps: 1, steps per second: 3, episode reward: -18.111, mean reward: -18.111 [-18.111, -18.111], mean action: 8.000 [8.000, 8.000], mean observation: 0.205 [0.000, 2.000], loss: 639.298767, mean_absolute_error: 9.238482, mean_q: 9.861260
step 619; T: 310200; average reward: 10.86 - action: se; reward: 39.53
  373/2000: episode: 215, duration: 0.380s, episode steps: 1, steps per second: 3, episode reward: 39.534, mean reward: 39.534 [39.534, 39.534], mean action: 3.000 [3.000, 3.000], mean observation: 0.352 [0.000, 3.000], loss: 542.706848, mean_absolute_error: 8.641304, mean_q: 8.606984
step 620; T: 310800; average reward: 10.84 - action: nw; reward: 6.93
  374/2000: episode: 216, duration: 0.340s, episode steps: 1, steps per second: 3, episode reward: 6.926, mean reward: 6.926 [6.926, 6.926], mean action: 7.000 [7.000, 7.000], mean observation: 0.341 [0.000, 5.000], loss: 165.842407, mean_absolute_error: 8.317083, mean_q: 10.584438
step 621; T: 311850; average reward: 11.71 - action: nw; reward: 199.85
  375/2000: episode: 217, duration: 0.485s, episode steps: 1, steps per second: 2, episode reward: 199.851, mean reward: 199.851 [199.851, 199.851], mean action: 7.000 [7.000, 7.000], mean observation: 0.251 [0.000, 3.000], loss: 1005.768921, mean_absolute_error: 10.756804, mean_q: 13.657746
step 626; T: 314250; average reward: 11.93 - action: w; reward: 59.52
  378/2000: episode: 218, duration: 1.796s, episode steps: 3, steps per second: 2, episode reward: 39.523, mean reward: 13.174 [-10.000, 59.523], mean action: 5.667 [3.000, 8.000], mean observation: 0.354 [0.000, 3.000], loss: 1114.278076, mean_absolute_error: 10.108274, mean_q: 9.890644
step 627; T: 316500; average reward: 11.74 - action: nw; reward: -30.02
  379/2000: episode: 219, duration: 0.447s, episode steps: 1, steps per second: 2, episode reward: -30.022, mean reward: -30.022 [-30.022, -30.022], mean action: 7.000 [7.000, 7.000], mean observation: 0.283 [0.000, 2.000], loss: 119.388779, mean_absolute_error: 8.552656, mean_q: 10.027401
step 630; T: 318300; average reward: 12.21 - action: n; reward: 114.60
  381/2000: episode: 220, duration: 1.045s, episode steps: 2, steps per second: 2, episode reward: 104.605, mean reward: 52.302 [-10.000, 114.605], mean action: 7.500 [7.000, 8.000], mean observation: 0.267 [0.000, 3.000], loss: 698.324768, mean_absolute_error: 8.970576, mean_q: 11.378681
step 636; T: 319350; average reward: 12.13 - action: nw; reward: -5.00
  384/2000: episode: 221, duration: 2.097s, episode steps: 3, steps per second: 1, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.236 [0.000, 3.000], loss: 671.822327, mean_absolute_error: 9.314900, mean_q: 11.778026
step 638; T: 319950; average reward: 12.26 - action: nw; reward: 40.99
  385/2000: episode: 222, duration: 0.772s, episode steps: 1, steps per second: 1, episode reward: 35.992, mean reward: 35.992 [35.992, 35.992], mean action: 7.000 [7.000, 7.000], mean observation: 0.392 [0.000, 3.000], loss: 1190.897949, mean_absolute_error: 10.187592, mean_q: 9.886330
step 641; T: 322050; average reward: 12.35 - action: nw; reward: 32.04
  387/2000: episode: 223, duration: 1.133s, episode steps: 2, steps per second: 2, episode reward: 22.040, mean reward: 11.020 [-10.000, 32.040], mean action: 7.500 [7.000, 8.000], mean observation: 0.271 [0.000, 3.000], loss: 343.439575, mean_absolute_error: 8.808935, mean_q: 10.828375
step 643; T: 322500; average reward: 12.13 - action: n; reward: -35.98
  388/2000: episode: 224, duration: 0.665s, episode steps: 1, steps per second: 2, episode reward: -40.975, mean reward: -40.975 [-40.975, -40.975], mean action: 8.000 [8.000, 8.000], mean observation: 0.184 [0.000, 2.000], loss: 257.252563, mean_absolute_error: 9.095896, mean_q: 12.870840
step 652; T: 324300; average reward: 12.06 - action: sw; reward: -5.00
  393/2000: episode: 225, duration: 3.109s, episode steps: 5, steps per second: 2, episode reward: -45.000, mean reward: -9.000 [-10.000, -5.000], mean action: 5.600 [2.000, 7.000], mean observation: 0.271 [0.000, 3.000], loss: 656.963745, mean_absolute_error: 9.709028, mean_q: 11.841147
step 653; T: 326400; average reward: 12.28 - action: nw; reward: 62.55
  394/2000: episode: 226, duration: 0.451s, episode steps: 1, steps per second: 2, episode reward: 62.554, mean reward: 62.554 [62.554, 62.554], mean action: 7.000 [7.000, 7.000], mean observation: 0.283 [0.000, 2.000], loss: 318.991364, mean_absolute_error: 8.848988, mean_q: 11.862688
step 656; T: 327150; average reward: 12.41 - action: n; reward: 40.90
  396/2000: episode: 227, duration: 1.092s, episode steps: 2, steps per second: 2, episode reward: 30.900, mean reward: 15.450 [-10.000, 40.900], mean action: 7.500 [7.000, 8.000], mean observation: 0.360 [0.000, 4.000], loss: 553.319397, mean_absolute_error: 9.826401, mean_q: 13.071905
step 657; T: 329100; average reward: 12.63 - action: nw; reward: 63.49
  397/2000: episode: 228, duration: 0.469s, episode steps: 1, steps per second: 2, episode reward: 63.493, mean reward: 63.493 [63.493, 63.493], mean action: 7.000 [7.000, 7.000], mean observation: 0.317 [0.000, 5.000], loss: 977.291199, mean_absolute_error: 10.007162, mean_q: 11.875347
step 658; T: 329400; average reward: 12.36 - action: nw; reward: -50.18
  398/2000: episode: 229, duration: 0.365s, episode steps: 1, steps per second: 3, episode reward: -50.180, mean reward: -50.180 [-50.180, -50.180], mean action: 7.000 [7.000, 7.000], mean observation: 0.413 [0.000, 3.000], loss: 1275.570923, mean_absolute_error: 10.811449, mean_q: 13.945711
step 659; T: 331200; average reward: 12.43 - action: se; reward: 29.20
  399/2000: episode: 230, duration: 0.441s, episode steps: 1, steps per second: 2, episode reward: 29.204, mean reward: 29.204 [29.204, 29.204], mean action: 3.000 [3.000, 3.000], mean observation: 0.240 [0.000, 3.000], loss: 869.475952, mean_absolute_error: 9.996578, mean_q: 10.123153
step 661; T: 331800; average reward: 12.73 - action: n; reward: 80.72
  400/2000: episode: 231, duration: 0.810s, episode steps: 1, steps per second: 1, episode reward: 75.721, mean reward: 75.721 [75.721, 75.721], mean action: 8.000 [8.000, 8.000], mean observation: 0.469 [0.000, 3.000], loss: 433.904236, mean_absolute_error: 9.707468, mean_q: 11.202152
step 662; T: 332100; average reward: 12.56 - action: nw; reward: -25.96
  401/2000: episode: 232, duration: 0.358s, episode steps: 1, steps per second: 3, episode reward: -25.960, mean reward: -25.960 [-25.960, -25.960], mean action: 7.000 [7.000, 7.000], mean observation: 0.416 [0.000, 3.000], loss: 268.815796, mean_absolute_error: 8.157088, mean_q: 11.024113
step 663; T: 332850; average reward: 12.49 - action: n; reward: -4.26
  402/2000: episode: 233, duration: 0.397s, episode steps: 1, steps per second: 3, episode reward: -4.259, mean reward: -4.259 [-4.259, -4.259], mean action: 8.000 [8.000, 8.000], mean observation: 0.440 [0.000, 3.000], loss: 345.769135, mean_absolute_error: 9.607030, mean_q: 13.963466
step 665; T: 334050; average reward: 12.60 - action: nw; reward: 38.73
  403/2000: episode: 234, duration: 0.773s, episode steps: 1, steps per second: 1, episode reward: 33.732, mean reward: 33.732 [33.732, 33.732], mean action: 7.000 [7.000, 7.000], mean observation: 0.157 [0.000, 4.000], loss: 439.367401, mean_absolute_error: 9.575843, mean_q: 14.235428
step 667; T: 336450; average reward: 12.61 - action: nw; reward: 16.08
  404/2000: episode: 235, duration: 0.847s, episode steps: 1, steps per second: 1, episode reward: 11.084, mean reward: 11.084 [11.084, 11.084], mean action: 7.000 [7.000, 7.000], mean observation: 0.381 [0.000, 2.000], loss: 134.427322, mean_absolute_error: 9.637655, mean_q: 14.244184
step 670; T: 341400; average reward: 12.13 - action: s; reward: -101.37
  406/2000: episode: 236, duration: 1.395s, episode steps: 2, steps per second: 1, episode reward: -111.373, mean reward: -55.686 [-101.373, -10.000], mean action: 5.500 [4.000, 7.000], mean observation: 0.281 [0.000, 3.000], loss: 726.026794, mean_absolute_error: 10.044327, mean_q: 13.351509
step 671; T: 342450; average reward: 12.06 - action: n; reward: -5.00
  407/2000: episode: 237, duration: 0.512s, episode steps: 1, steps per second: 2, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.251 [0.000, 3.000], loss: 293.675842, mean_absolute_error: 8.947687, mean_q: 11.503860
step 672; T: 342900; average reward: 12.10 - action: n; reward: 21.20
  408/2000: episode: 238, duration: 0.384s, episode steps: 1, steps per second: 3, episode reward: 21.196, mean reward: 21.196 [21.196, 21.196], mean action: 8.000 [8.000, 8.000], mean observation: 0.400 [0.000, 3.000], loss: 608.507141, mean_absolute_error: 9.540187, mean_q: 13.186298
step 673; T: 343500; average reward: 11.96 - action: nw; reward: -20.41
  409/2000: episode: 239, duration: 0.488s, episode steps: 1, steps per second: 2, episode reward: -20.412, mean reward: -20.412 [-20.412, -20.412], mean action: 7.000 [7.000, 7.000], mean observation: 0.440 [0.000, 3.000], loss: 148.300842, mean_absolute_error: 9.343488, mean_q: 13.298564
step 676; T: 345000; average reward: 12.26 - action: nw; reward: 82.93
  411/2000: episode: 240, duration: 1.279s, episode steps: 2, steps per second: 2, episode reward: 72.930, mean reward: 36.465 [-10.000, 82.930], mean action: 7.500 [7.000, 8.000], mean observation: 0.400 [0.000, 3.000], loss: 901.722778, mean_absolute_error: 10.168530, mean_q: 13.398075
step 677; T: 345600; average reward: 12.19 - action: nw; reward: -5.00
  412/2000: episode: 241, duration: 0.449s, episode steps: 1, steps per second: 2, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.291 [0.000, 3.000], loss: 785.058594, mean_absolute_error: 10.487604, mean_q: 11.888281
step 681; T: 347550; average reward: 12.11 - action: nw; reward: -5.00
  414/2000: episode: 242, duration: 1.703s, episode steps: 2, steps per second: 1, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.376 [0.000, 3.000], loss: 574.585815, mean_absolute_error: 9.951048, mean_q: 13.920209
step 687; T: 349050; average reward: 12.19 - action: e; reward: 30.41
  417/2000: episode: 243, duration: 2.510s, episode steps: 3, steps per second: 1, episode reward: 15.413, mean reward: 5.138 [-10.000, 25.413], mean action: 3.333 [0.000, 8.000], mean observation: 0.292 [0.000, 3.000], loss: 664.895813, mean_absolute_error: 10.631772, mean_q: 13.114188
step 688; T: 349800; average reward: 12.44 - action: s; reward: 73.95
  418/2000: episode: 244, duration: 0.401s, episode steps: 1, steps per second: 2, episode reward: 73.951, mean reward: 73.951 [73.951, 73.951], mean action: 4.000 [4.000, 4.000], mean observation: 0.379 [0.000, 3.000], loss: 391.925964, mean_absolute_error: 9.565379, mean_q: 10.934939
step 689; T: 350850; average reward: 12.56 - action: nw; reward: 39.90
  419/2000: episode: 245, duration: 0.437s, episode steps: 1, steps per second: 2, episode reward: 39.901, mean reward: 39.901 [39.901, 39.901], mean action: 7.000 [7.000, 7.000], mean observation: 0.213 [0.000, 3.000], loss: 931.027466, mean_absolute_error: 10.948512, mean_q: 13.134376
step 690; T: 352350; average reward: 12.70 - action: e; reward: 47.50
  420/2000: episode: 246, duration: 0.467s, episode steps: 1, steps per second: 2, episode reward: 47.498, mean reward: 47.498 [47.498, 47.498], mean action: 2.000 [2.000, 2.000], mean observation: 0.509 [0.000, 3.000], loss: 723.205627, mean_absolute_error: 11.107115, mean_q: 14.785622
step 692; T: 352950; average reward: 12.29 - action: n; reward: -88.53
  421/2000: episode: 247, duration: 0.781s, episode steps: 1, steps per second: 1, episode reward: -93.534, mean reward: -93.534 [-93.534, -93.534], mean action: 8.000 [8.000, 8.000], mean observation: 0.341 [0.000, 2.000], loss: 1052.822876, mean_absolute_error: 11.071501, mean_q: 12.529549
step 693; T: 354000; average reward: 11.97 - action: nw; reward: -65.38
  422/2000: episode: 248, duration: 0.463s, episode steps: 1, steps per second: 2, episode reward: -65.379, mean reward: -65.379 [-65.379, -65.379], mean action: 7.000 [7.000, 7.000], mean observation: 0.379 [0.000, 3.000], loss: 777.371399, mean_absolute_error: 9.462360, mean_q: 14.751607
step 695; T: 354900; average reward: 12.15 - action: n; reward: 54.76
  423/2000: episode: 249, duration: 0.857s, episode steps: 1, steps per second: 1, episode reward: 49.765, mean reward: 49.765 [49.765, 49.765], mean action: 8.000 [8.000, 8.000], mean observation: 0.293 [0.000, 3.000], loss: 1140.438232, mean_absolute_error: 11.061541, mean_q: 13.954398
step 698; T: 356850; average reward: 12.10 - action: nw; reward: -0.17
  425/2000: episode: 250, duration: 1.547s, episode steps: 2, steps per second: 1, episode reward: -10.167, mean reward: -5.084 [-10.000, -0.167], mean action: 7.000 [7.000, 7.000], mean observation: 0.396 [0.000, 5.000], loss: 609.676514, mean_absolute_error: 10.179388, mean_q: 12.072645
step 700; T: 357900; average reward: 12.34 - action: nw; reward: 73.69
  426/2000: episode: 251, duration: 0.885s, episode steps: 1, steps per second: 1, episode reward: 68.693, mean reward: 68.693 [68.693, 68.693], mean action: 7.000 [7.000, 7.000], mean observation: 0.171 [0.000, 3.000], loss: 242.434860, mean_absolute_error: 9.473826, mean_q: 12.259426
step 701; T: 358650; average reward: 12.35 - action: n; reward: 13.49
  427/2000: episode: 252, duration: 0.507s, episode steps: 1, steps per second: 2, episode reward: 13.493, mean reward: 13.493 [13.493, 13.493], mean action: 8.000 [8.000, 8.000], mean observation: 0.381 [0.000, 3.000], loss: 400.819885, mean_absolute_error: 9.910650, mean_q: 11.061171
step 702; T: 359400; average reward: 11.75 - action: nw; reward: -139.39
  428/2000: episode: 253, duration: 0.424s, episode steps: 1, steps per second: 2, episode reward: -139.391, mean reward: -139.391 [-139.391, -139.391], mean action: 7.000 [7.000, 7.000], mean observation: 0.280 [0.000, 4.000], loss: 891.954529, mean_absolute_error: 10.563586, mean_q: 13.272939
step 703; T: 360450; average reward: 12.18 - action: nw; reward: 121.41
  429/2000: episode: 254, duration: 0.486s, episode steps: 1, steps per second: 2, episode reward: 121.408, mean reward: 121.408 [121.408, 121.408], mean action: 7.000 [7.000, 7.000], mean observation: 0.301 [0.000, 2.000], loss: 505.442078, mean_absolute_error: 9.270684, mean_q: 11.972988
step 707; T: 361950; average reward: 12.16 - action: n; reward: 6.11
  431/2000: episode: 255, duration: 1.690s, episode steps: 2, steps per second: 1, episode reward: -8.887, mean reward: -4.444 [-10.000, 1.113], mean action: 7.500 [7.000, 8.000], mean observation: 0.439 [0.000, 3.000], loss: 548.817627, mean_absolute_error: 9.423344, mean_q: 12.091520
step 710; T: 363750; average reward: 12.11 - action: n; reward: 0.13
  433/2000: episode: 256, duration: 1.369s, episode steps: 2, steps per second: 1, episode reward: -9.872, mean reward: -4.936 [-10.000, 0.128], mean action: 7.500 [7.000, 8.000], mean observation: 0.316 [0.000, 3.000], loss: 649.490295, mean_absolute_error: 9.363076, mean_q: 8.975980
step 712; T: 364500; average reward: 12.15 - action: nw; reward: 23.68
  434/2000: episode: 257, duration: 0.873s, episode steps: 1, steps per second: 1, episode reward: 18.681, mean reward: 18.681 [18.681, 18.681], mean action: 7.000 [7.000, 7.000], mean observation: 0.395 [0.000, 4.000], loss: 1073.015137, mean_absolute_error: 9.909399, mean_q: 8.424292
step 716; T: 365700; average reward: 12.56 - action: e; reward: 116.54
  436/2000: episode: 258, duration: 1.554s, episode steps: 2, steps per second: 1, episode reward: 101.540, mean reward: 50.770 [-10.000, 111.540], mean action: 4.500 [2.000, 7.000], mean observation: 0.237 [0.000, 3.000], loss: 779.671631, mean_absolute_error: 9.684196, mean_q: 10.319887
step 719; T: 367950; average reward: 12.49 - action: n; reward: -5.00
  438/2000: episode: 259, duration: 1.202s, episode steps: 2, steps per second: 2, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 7.500 [7.000, 8.000], mean observation: 0.159 [0.000, 4.000], loss: 482.200562, mean_absolute_error: 9.790084, mean_q: 12.354177
step 720; T: 368850; average reward: 12.42 - action: nw; reward: -5.00
  439/2000: episode: 260, duration: 0.543s, episode steps: 1, steps per second: 2, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.461 [0.000, 3.000], loss: 919.021729, mean_absolute_error: 9.741067, mean_q: 9.011322
step 722; T: 370350; average reward: 12.49 - action: n; reward: 31.04
  440/2000: episode: 261, duration: 0.830s, episode steps: 1, steps per second: 1, episode reward: 26.040, mean reward: 26.040 [26.040, 26.040], mean action: 8.000 [8.000, 8.000], mean observation: 0.267 [0.000, 2.000], loss: 600.377014, mean_absolute_error: 9.209558, mean_q: 10.982794
step 723; T: 371100; average reward: 12.47 - action: nw; reward: 5.20
  441/2000: episode: 262, duration: 0.434s, episode steps: 1, steps per second: 2, episode reward: 5.199, mean reward: 5.199 [5.199, 5.199], mean action: 7.000 [7.000, 7.000], mean observation: 0.208 [0.000, 4.000], loss: 532.036194, mean_absolute_error: 9.420430, mean_q: 9.754144
step 737; T: 374250; average reward: 12.40 - action: nw; reward: -5.00
  448/2000: episode: 263, duration: 5.502s, episode steps: 7, steps per second: 1, episode reward: -70.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.571 [2.000, 7.000], mean observation: 0.309 [0.000, 4.000], loss: 663.131165, mean_absolute_error: 9.613393, mean_q: 10.994588
step 740; T: 375450; average reward: 12.64 - action: n; reward: 75.78
  450/2000: episode: 264, duration: 1.208s, episode steps: 2, steps per second: 2, episode reward: 65.777, mean reward: 32.888 [-10.000, 75.777], mean action: 5.000 [2.000, 8.000], mean observation: 0.347 [0.000, 3.000], loss: 579.035278, mean_absolute_error: 9.676273, mean_q: 12.069582
step 741; T: 376650; average reward: 12.12 - action: n; reward: -125.60
  451/2000: episode: 265, duration: 0.422s, episode steps: 1, steps per second: 2, episode reward: -125.601, mean reward: -125.601 [-125.601, -125.601], mean action: 8.000 [8.000, 8.000], mean observation: 0.205 [0.000, 2.000], loss: 101.256119, mean_absolute_error: 8.047338, mean_q: 10.107367
step 749; T: 378750; average reward: 12.06 - action: n; reward: -2.44
  455/2000: episode: 266, duration: 3.160s, episode steps: 4, steps per second: 1, episode reward: -37.436, mean reward: -9.359 [-10.000, -7.436], mean action: 6.000 [2.000, 8.000], mean observation: 0.250 [0.000, 3.000], loss: 560.631592, mean_absolute_error: 9.489199, mean_q: 11.150056
step 751; T: 380700; average reward: 12.16 - action: nw; reward: 37.14
  456/2000: episode: 267, duration: 0.905s, episode steps: 1, steps per second: 1, episode reward: 32.138, mean reward: 32.138 [32.138, 32.138], mean action: 7.000 [7.000, 7.000], mean observation: 0.171 [0.000, 2.000], loss: 213.838806, mean_absolute_error: 8.663273, mean_q: 8.257057
step 753; T: 381300; average reward: 12.09 - action: nw; reward: -5.00
  457/2000: episode: 268, duration: 0.867s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.260 [0.000, 3.000], loss: 1170.478882, mean_absolute_error: 10.439999, mean_q: 13.073791
step 754; T: 383250; average reward: 12.01 - action: n; reward: -11.58
  458/2000: episode: 269, duration: 0.455s, episode steps: 1, steps per second: 2, episode reward: -11.584, mean reward: -11.584 [-11.584, -11.584], mean action: 8.000 [8.000, 8.000], mean observation: 0.219 [0.000, 2.000], loss: 109.294899, mean_absolute_error: 8.575791, mean_q: 13.471823
step 756; T: 384150; average reward: 11.94 - action: nw; reward: -5.00
  459/2000: episode: 270, duration: 0.788s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.421 [0.000, 3.000], loss: 183.418839, mean_absolute_error: 9.084609, mean_q: 13.605053
step 757; T: 385200; average reward: 12.14 - action: e; reward: 65.33
  460/2000: episode: 271, duration: 0.428s, episode steps: 1, steps per second: 2, episode reward: 65.332, mean reward: 65.332 [65.332, 65.332], mean action: 2.000 [2.000, 2.000], mean observation: 0.227 [0.000, 3.000], loss: 300.900208, mean_absolute_error: 9.329693, mean_q: 11.860607
step 759; T: 385800; average reward: 12.10 - action: n; reward: 2.69
  461/2000: episode: 272, duration: 0.752s, episode steps: 1, steps per second: 1, episode reward: -2.307, mean reward: -2.307 [-2.307, -2.307], mean action: 8.000 [8.000, 8.000], mean observation: 0.379 [0.000, 3.000], loss: 847.935425, mean_absolute_error: 9.752327, mean_q: 12.561092
step 764; T: 390150; average reward: 12.21 - action: nw; reward: 40.00
  464/2000: episode: 273, duration: 2.193s, episode steps: 3, steps per second: 1, episode reward: 20.000, mean reward: 6.667 [-10.000, 40.000], mean action: 7.333 [7.000, 8.000], mean observation: 0.371 [0.000, 3.000], loss: 407.154694, mean_absolute_error: 8.987624, mean_q: 11.163199
step 769; T: 391650; average reward: 12.14 - action: nw; reward: -5.00
  467/2000: episode: 274, duration: 2.119s, episode steps: 3, steps per second: 1, episode reward: -25.000, mean reward: -8.333 [-10.000, -5.000], mean action: 5.333 [2.000, 7.000], mean observation: 0.239 [0.000, 3.000], loss: 672.979065, mean_absolute_error: 10.064033, mean_q: 12.988090
step 771; T: 392550; average reward: 12.08 - action: e; reward: -5.32
  468/2000: episode: 275, duration: 0.774s, episode steps: 1, steps per second: 1, episode reward: -10.319, mean reward: -10.319 [-10.319, -10.319], mean action: 2.000 [2.000, 2.000], mean observation: 0.493 [0.000, 4.000], loss: 609.943176, mean_absolute_error: 9.197001, mean_q: 9.643614
step 772; T: 393300; average reward: 11.96 - action: noop; reward: -21.23
  469/2000: episode: 276, duration: 0.446s, episode steps: 1, steps per second: 2, episode reward: -21.233, mean reward: -21.233 [-21.233, -21.233], mean action: 0.000 [0.000, 0.000], mean observation: 0.213 [0.000, 2.000], loss: 607.533875, mean_absolute_error: 9.828355, mean_q: 12.676574
step 776; T: 394800; average reward: 11.90 - action: nw; reward: -5.00
  471/2000: episode: 277, duration: 1.749s, episode steps: 2, steps per second: 1, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.176 [0.000, 3.000], loss: 614.438843, mean_absolute_error: 10.165943, mean_q: 14.126600
step 778; T: 395400; average reward: 12.03 - action: e; reward: 47.08
  472/2000: episode: 278, duration: 0.821s, episode steps: 1, steps per second: 1, episode reward: 42.085, mean reward: 42.085 [42.085, 42.085], mean action: 2.000 [2.000, 2.000], mean observation: 0.392 [0.000, 3.000], loss: 765.303589, mean_absolute_error: 10.909729, mean_q: 14.267738
step 780; T: 400500; average reward: 11.97 - action: nw; reward: -2.05
  473/2000: episode: 279, duration: 1.030s, episode steps: 1, steps per second: 1, episode reward: -7.051, mean reward: -7.051 [-7.051, -7.051], mean action: 7.000 [7.000, 7.000], mean observation: 0.147 [0.000, 2.000], loss: 295.383484, mean_absolute_error: 9.161711, mean_q: 11.796482
step 782; T: 402150; average reward: 11.91 - action: sw; reward: -5.00
  474/2000: episode: 280, duration: 0.863s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.219 [0.000, 3.000], loss: 354.438904, mean_absolute_error: 9.274446, mean_q: 12.190584
step 786; T: 403050; average reward: 11.85 - action: e; reward: -5.54
  476/2000: episode: 281, duration: 1.809s, episode steps: 2, steps per second: 1, episode reward: -20.541, mean reward: -10.270 [-10.541, -10.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.293 [0.000, 3.000], loss: 426.972290, mean_absolute_error: 9.323193, mean_q: 12.359003
step 792; T: 406050; average reward: 11.79 - action: n; reward: -5.00
  479/2000: episode: 282, duration: 2.777s, episode steps: 3, steps per second: 1, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.667 [7.000, 8.000], mean observation: 0.368 [0.000, 4.000], loss: 460.769653, mean_absolute_error: 9.357968, mean_q: 12.426639
step 795; T: 408150; average reward: 12.25 - action: e; reward: 142.04
  481/2000: episode: 283, duration: 1.558s, episode steps: 2, steps per second: 1, episode reward: 132.037, mean reward: 66.018 [-10.000, 142.037], mean action: 4.500 [2.000, 7.000], mean observation: 0.249 [0.000, 3.000], loss: 563.904236, mean_absolute_error: 9.323479, mean_q: 14.230515
step 796; T: 408450; average reward: 12.12 - action: nw; reward: -24.34
  482/2000: episode: 284, duration: 0.449s, episode steps: 1, steps per second: 2, episode reward: -24.341, mean reward: -24.341 [-24.341, -24.341], mean action: 7.000 [7.000, 7.000], mean observation: 0.376 [0.000, 3.000], loss: 670.216675, mean_absolute_error: 9.948505, mean_q: 15.733672
step 797; T: 409350; average reward: 12.25 - action: nw; reward: 49.06
  483/2000: episode: 285, duration: 0.523s, episode steps: 1, steps per second: 2, episode reward: 49.062, mean reward: 49.062 [49.062, 49.062], mean action: 7.000 [7.000, 7.000], mean observation: 0.240 [0.000, 4.000], loss: 868.987549, mean_absolute_error: 9.991723, mean_q: 12.489705
step 798; T: 409500; average reward: 12.05 - action: e; reward: -47.09
  484/2000: episode: 286, duration: 0.446s, episode steps: 1, steps per second: 2, episode reward: -47.092, mean reward: -47.092 [-47.092, -47.092], mean action: 2.000 [2.000, 2.000], mean observation: 0.251 [0.000, 4.000], loss: 293.227234, mean_absolute_error: 9.717137, mean_q: 13.643013
step 800; T: 410250; average reward: 12.25 - action: e; reward: 70.53
  485/2000: episode: 287, duration: 0.889s, episode steps: 1, steps per second: 1, episode reward: 65.527, mean reward: 65.527 [65.527, 65.527], mean action: 2.000 [2.000, 2.000], mean observation: 0.411 [0.000, 3.000], loss: 403.663208, mean_absolute_error: 8.915779, mean_q: 13.214937
step 801; T: 410550; average reward: 12.32 - action: n; reward: 32.00
  486/2000: episode: 288, duration: 0.430s, episode steps: 1, steps per second: 2, episode reward: 31.998, mean reward: 31.998 [31.998, 31.998], mean action: 8.000 [8.000, 8.000], mean observation: 0.365 [0.000, 3.000], loss: 241.441803, mean_absolute_error: 8.647873, mean_q: 16.158211
step 803; T: 411600; average reward: 12.22 - action: nw; reward: -15.12
  487/2000: episode: 289, duration: 0.950s, episode steps: 1, steps per second: 1, episode reward: -20.123, mean reward: -20.123 [-20.123, -20.123], mean action: 7.000 [7.000, 7.000], mean observation: 0.261 [0.000, 3.000], loss: 649.194092, mean_absolute_error: 9.677595, mean_q: 10.950207
step 805; T: 412200; average reward: 12.19 - action: n; reward: 1.98
  488/2000: episode: 290, duration: 1.005s, episode steps: 1, steps per second: 1, episode reward: -3.019, mean reward: -3.019 [-3.019, -3.019], mean action: 8.000 [8.000, 8.000], mean observation: 0.368 [0.000, 5.000], loss: 524.200989, mean_absolute_error: 8.835137, mean_q: 15.584993
step 806; T: 414150; average reward: 12.43 - action: nw; reward: 83.74
  489/2000: episode: 291, duration: 0.555s, episode steps: 1, steps per second: 2, episode reward: 83.741, mean reward: 83.741 [83.741, 83.741], mean action: 7.000 [7.000, 7.000], mean observation: 0.213 [0.000, 3.000], loss: 788.089783, mean_absolute_error: 8.913544, mean_q: 12.201500
step 809; T: 415200; average reward: 12.37 - action: e; reward: -5.00
  491/2000: episode: 292, duration: 1.339s, episode steps: 2, steps per second: 1, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.317 [0.000, 3.000], loss: 470.526855, mean_absolute_error: 9.892015, mean_q: 18.314644
step 815; T: 416850; average reward: 12.14 - action: nw; reward: -57.42
  494/2000: episode: 293, duration: 2.843s, episode steps: 3, steps per second: 1, episode reward: -82.425, mean reward: -27.475 [-62.425, -10.000], mean action: 5.333 [1.000, 8.000], mean observation: 0.286 [0.000, 4.000], loss: 855.884094, mean_absolute_error: 10.108165, mean_q: 15.561566
step 817; T: 417600; average reward: 12.02 - action: nw; reward: -22.62
  495/2000: episode: 294, duration: 0.923s, episode steps: 1, steps per second: 1, episode reward: -27.622, mean reward: -27.622 [-27.622, -27.622], mean action: 7.000 [7.000, 7.000], mean observation: 0.387 [0.000, 3.000], loss: 246.479401, mean_absolute_error: 8.957896, mean_q: 15.985134
step 820; T: 418950; average reward: 12.12 - action: nw; reward: 41.05
  497/2000: episode: 295, duration: 1.424s, episode steps: 2, steps per second: 1, episode reward: 31.048, mean reward: 15.524 [-10.000, 41.048], mean action: 7.500 [7.000, 8.000], mean observation: 0.337 [0.000, 3.000], loss: 582.514343, mean_absolute_error: 8.977574, mean_q: 11.707047
step 829; T: 420600; average reward: 12.06 - action: nw; reward: -5.00
  502/2000: episode: 296, duration: 3.730s, episode steps: 5, steps per second: 1, episode reward: -45.000, mean reward: -9.000 [-10.000, -5.000], mean action: 7.200 [7.000, 8.000], mean observation: 0.390 [0.000, 3.000], loss: 691.925964, mean_absolute_error: 9.571079, mean_q: 14.581271
step 830; T: 422850; average reward: 12.09 - action: e; reward: 22.51
  503/2000: episode: 297, duration: 0.505s, episode steps: 1, steps per second: 2, episode reward: 22.507, mean reward: 22.507 [22.507, 22.507], mean action: 2.000 [2.000, 2.000], mean observation: 0.163 [0.000, 4.000], loss: 1124.319702, mean_absolute_error: 10.826975, mean_q: 13.764615
step 835; T: 424950; average reward: 12.04 - action: sw; reward: -5.00
  506/2000: episode: 298, duration: 2.115s, episode steps: 3, steps per second: 1, episode reward: -25.000, mean reward: -8.333 [-10.000, -5.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.245 [0.000, 4.000], loss: 257.532074, mean_absolute_error: 8.443810, mean_q: 12.901574
step 836; T: 426450; average reward: 12.17 - action: nw; reward: 52.32
  507/2000: episode: 299, duration: 0.453s, episode steps: 1, steps per second: 2, episode reward: 52.319, mean reward: 52.319 [52.319, 52.319], mean action: 7.000 [7.000, 7.000], mean observation: 0.419 [0.000, 3.000], loss: 642.064819, mean_absolute_error: 9.789419, mean_q: 11.262600
step 839; T: 427200; average reward: 12.21 - action: n; reward: 22.76
  509/2000: episode: 300, duration: 1.205s, episode steps: 2, steps per second: 2, episode reward: 12.760, mean reward: 6.380 [-10.000, 22.760], mean action: 6.500 [5.000, 8.000], mean observation: 0.399 [0.000, 3.000], loss: 715.941040, mean_absolute_error: 9.487733, mean_q: 14.113066
step 840; T: 429750; average reward: 12.48 - action: nw; reward: 93.64
  510/2000: episode: 301, duration: 0.529s, episode steps: 1, steps per second: 2, episode reward: 93.636, mean reward: 93.636 [93.636, 93.636], mean action: 7.000 [7.000, 7.000], mean observation: 0.232 [0.000, 3.000], loss: 172.822495, mean_absolute_error: 8.874457, mean_q: 11.614872
step 846; T: 431400; average reward: 12.58 - action: e; reward: 45.27
  513/2000: episode: 302, duration: 2.503s, episode steps: 3, steps per second: 1, episode reward: 20.273, mean reward: 6.758 [-10.000, 40.273], mean action: 5.000 [2.000, 8.000], mean observation: 0.262 [0.000, 3.000], loss: 707.922363, mean_absolute_error: 9.283208, mean_q: 14.131030
step 847; T: 432600; average reward: 12.73 - action: n; reward: 57.30
  514/2000: episode: 303, duration: 0.452s, episode steps: 1, steps per second: 2, episode reward: 57.296, mean reward: 57.296 [57.296, 57.296], mean action: 8.000 [8.000, 8.000], mean observation: 0.227 [0.000, 3.000], loss: 897.399536, mean_absolute_error: 9.021919, mean_q: 11.648609
step 848; T: 433650; average reward: 12.75 - action: e; reward: 19.28
  515/2000: episode: 304, duration: 0.472s, episode steps: 1, steps per second: 2, episode reward: 19.275, mean reward: 19.275 [19.275, 19.275], mean action: 2.000 [2.000, 2.000], mean observation: 0.344 [0.000, 3.000], loss: 1007.218018, mean_absolute_error: 9.748606, mean_q: 13.719118
step 854; T: 435900; average reward: 12.90 - action: n; reward: 57.67
  518/2000: episode: 305, duration: 2.675s, episode steps: 3, steps per second: 1, episode reward: 32.665, mean reward: 10.888 [-10.000, 52.665], mean action: 6.667 [5.000, 8.000], mean observation: 0.358 [0.000, 4.000], loss: 438.511871, mean_absolute_error: 9.116883, mean_q: 14.661944
step 857; T: 437100; average reward: 12.85 - action: nw; reward: -3.24
  520/2000: episode: 306, duration: 1.315s, episode steps: 2, steps per second: 2, episode reward: -13.235, mean reward: -6.618 [-10.000, -3.235], mean action: 4.500 [2.000, 7.000], mean observation: 0.229 [0.000, 2.000], loss: 290.333984, mean_absolute_error: 8.853615, mean_q: 14.770496
step 859; T: 437550; average reward: 12.89 - action: nw; reward: 26.83
  521/2000: episode: 307, duration: 0.884s, episode steps: 1, steps per second: 1, episode reward: 21.835, mean reward: 21.835 [21.835, 21.835], mean action: 7.000 [7.000, 7.000], mean observation: 0.323 [0.000, 3.000], loss: 636.169922, mean_absolute_error: 9.190964, mean_q: 14.188223
step 860; T: 438750; average reward: 12.90 - action: nw; reward: 16.07
  522/2000: episode: 308, duration: 0.550s, episode steps: 1, steps per second: 2, episode reward: 16.075, mean reward: 16.075 [16.075, 16.075], mean action: 7.000 [7.000, 7.000], mean observation: 0.280 [0.000, 3.000], loss: 331.055359, mean_absolute_error: 8.910728, mean_q: 16.020411
step 864; T: 440250; average reward: 13.04 - action: nw; reward: 55.12
  524/2000: episode: 309, duration: 1.857s, episode steps: 2, steps per second: 1, episode reward: 40.117, mean reward: 20.059 [-10.000, 50.117], mean action: 4.500 [2.000, 7.000], mean observation: 0.219 [0.000, 4.000], loss: 331.348541, mean_absolute_error: 8.924147, mean_q: 13.743371
step 867; T: 441000; average reward: 12.94 - action: e; reward: -18.41
  526/2000: episode: 310, duration: 1.344s, episode steps: 2, steps per second: 1, episode reward: -28.410, mean reward: -14.205 [-18.410, -10.000], mean action: 4.500 [2.000, 7.000], mean observation: 0.252 [0.000, 4.000], loss: 566.660767, mean_absolute_error: 9.614645, mean_q: 17.584171
step 868; T: 441300; average reward: 12.95 - action: e; reward: 16.43
  527/2000: episode: 311, duration: 0.533s, episode steps: 1, steps per second: 2, episode reward: 16.434, mean reward: 16.434 [16.434, 16.434], mean action: 2.000 [2.000, 2.000], mean observation: 0.264 [0.000, 4.000], loss: 997.335938, mean_absolute_error: 10.179129, mean_q: 13.686269
step 872; T: 444300; average reward: 12.83 - action: e; reward: -24.48
  529/2000: episode: 312, duration: 1.846s, episode steps: 2, steps per second: 1, episode reward: -39.485, mean reward: -19.742 [-29.485, -10.000], mean action: 4.500 [2.000, 7.000], mean observation: 0.233 [0.000, 4.000], loss: 639.788147, mean_absolute_error: 9.906111, mean_q: 14.974002
step 878; T: 445500; average reward: 12.77 - action: ne; reward: -7.10
  532/2000: episode: 313, duration: 2.557s, episode steps: 3, steps per second: 1, episode reward: -32.100, mean reward: -10.700 [-12.100, -10.000], mean action: 3.333 [1.000, 7.000], mean observation: 0.311 [0.000, 3.000], loss: 603.251221, mean_absolute_error: 9.502048, mean_q: 16.159657
step 879; T: 445800; average reward: 12.76 - action: nw; reward: 9.47
  533/2000: episode: 314, duration: 0.414s, episode steps: 1, steps per second: 2, episode reward: 9.468, mean reward: 9.468 [9.468, 9.468], mean action: 7.000 [7.000, 7.000], mean observation: 0.440 [0.000, 3.000], loss: 1158.768433, mean_absolute_error: 10.077982, mean_q: 14.458783
step 885; T: 448200; average reward: 12.70 - action: nw; reward: -5.00
  536/2000: episode: 315, duration: 2.685s, episode steps: 3, steps per second: 1, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.333 [7.000, 8.000], mean observation: 0.265 [0.000, 3.000], loss: 348.250275, mean_absolute_error: 9.017779, mean_q: 14.468788
step 886; T: 449250; average reward: 12.64 - action: e; reward: -5.00
  537/2000: episode: 316, duration: 0.463s, episode steps: 1, steps per second: 2, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.229 [0.000, 3.000], loss: 244.285568, mean_absolute_error: 8.861996, mean_q: 15.647739
step 890; T: 450900; average reward: 12.68 - action: n; reward: 23.77
  539/2000: episode: 317, duration: 2.053s, episode steps: 2, steps per second: 1, episode reward: 8.769, mean reward: 4.385 [-10.000, 18.769], mean action: 5.000 [2.000, 8.000], mean observation: 0.301 [0.000, 3.000], loss: 458.830872, mean_absolute_error: 9.141258, mean_q: 16.058712
step 893; T: 454200; average reward: 12.64 - action: nw; reward: -1.36
  541/2000: episode: 318, duration: 1.666s, episode steps: 2, steps per second: 1, episode reward: -11.361, mean reward: -5.681 [-10.000, -1.361], mean action: 4.500 [2.000, 7.000], mean observation: 0.227 [0.000, 4.000], loss: 706.965332, mean_absolute_error: 9.165235, mean_q: 12.891631
step 895; T: 455700; average reward: 12.92 - action: n; reward: 104.70
  542/2000: episode: 319, duration: 1.024s, episode steps: 1, steps per second: 1, episode reward: 99.698, mean reward: 99.698 [99.698, 99.698], mean action: 8.000 [8.000, 8.000], mean observation: 0.307 [0.000, 5.000], loss: 924.565674, mean_absolute_error: 9.465471, mean_q: 16.179461
step 896; T: 457200; average reward: 12.65 - action: nw; reward: -75.68
  543/2000: episode: 320, duration: 0.590s, episode steps: 1, steps per second: 2, episode reward: -75.680, mean reward: -75.680 [-75.680, -75.680], mean action: 7.000 [7.000, 7.000], mean observation: 0.392 [0.000, 4.000], loss: 220.370743, mean_absolute_error: 8.482450, mean_q: 15.745068
step 898; T: 458850; average reward: 12.55 - action: nw; reward: -19.67
  544/2000: episode: 321, duration: 1.138s, episode steps: 1, steps per second: 1, episode reward: -24.675, mean reward: -24.675 [-24.675, -24.675], mean action: 7.000 [7.000, 7.000], mean observation: 0.403 [0.000, 4.000], loss: 284.179535, mean_absolute_error: 8.799963, mean_q: 11.844303
step 899; T: 459600; average reward: 12.63 - action: e; reward: 40.14
  545/2000: episode: 322, duration: 0.641s, episode steps: 1, steps per second: 2, episode reward: 40.143, mean reward: 40.143 [40.143, 40.143], mean action: 2.000 [2.000, 2.000], mean observation: 0.424 [0.000, 5.000], loss: 1343.869873, mean_absolute_error: 11.256811, mean_q: 15.582742
step 900; T: 459900; average reward: 13.31 - action: ne; reward: 231.46
  546/2000: episode: 323, duration: 0.497s, episode steps: 1, steps per second: 2, episode reward: 231.459, mean reward: 231.459 [231.459, 231.459], mean action: 1.000 [1.000, 1.000], mean observation: 0.419 [0.000, 5.000], loss: 564.184204, mean_absolute_error: 9.174250, mean_q: 19.181965
step 901; T: 461250; average reward: 13.12 - action: ne; reward: -46.89
  547/2000: episode: 324, duration: 0.586s, episode steps: 1, steps per second: 2, episode reward: -46.890, mean reward: -46.890 [-46.890, -46.890], mean action: 1.000 [1.000, 1.000], mean observation: 0.200 [0.000, 3.000], loss: 706.496216, mean_absolute_error: 10.136603, mean_q: 16.699110
step 918; T: 464700; average reward: 13.07 - action: e; reward: -5.00
  556/2000: episode: 325, duration: 8.671s, episode steps: 9, steps per second: 1, episode reward: -85.000, mean reward: -9.444 [-10.000, -5.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.211 [0.000, 3.000], loss: 720.206909, mean_absolute_error: 9.771988, mean_q: 14.546785
step 920; T: 465450; average reward: 12.89 - action: nw; reward: -46.05
  557/2000: episode: 326, duration: 1.086s, episode steps: 1, steps per second: 1, episode reward: -51.051, mean reward: -51.051 [-51.051, -51.051], mean action: 7.000 [7.000, 7.000], mean observation: 0.387 [0.000, 3.000], loss: 579.799927, mean_absolute_error: 9.218086, mean_q: 14.002293
step 921; T: 466950; average reward: 13.18 - action: nw; reward: 108.93
  558/2000: episode: 327, duration: 0.748s, episode steps: 1, steps per second: 1, episode reward: 108.928, mean reward: 108.928 [108.928, 108.928], mean action: 7.000 [7.000, 7.000], mean observation: 0.211 [0.000, 2.000], loss: 1085.075439, mean_absolute_error: 10.492588, mean_q: 18.534859
step 922; T: 468300; average reward: 12.72 - action: e; reward: -139.29
  559/2000: episode: 328, duration: 0.621s, episode steps: 1, steps per second: 2, episode reward: -139.289, mean reward: -139.289 [-139.289, -139.289], mean action: 2.000 [2.000, 2.000], mean observation: 0.144 [0.000, 2.000], loss: 472.876648, mean_absolute_error: 9.347151, mean_q: 16.188358
step 923; T: 471000; average reward: 12.66 - action: sw; reward: -5.00
  560/2000: episode: 329, duration: 0.694s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.331 [0.000, 4.000], loss: 267.170898, mean_absolute_error: 8.684787, mean_q: 13.250515
step 924; T: 472350; average reward: 12.57 - action: nw; reward: -17.14
  561/2000: episode: 330, duration: 0.569s, episode steps: 1, steps per second: 2, episode reward: -17.141, mean reward: -17.141 [-17.141, -17.141], mean action: 7.000 [7.000, 7.000], mean observation: 0.160 [0.000, 3.000], loss: 1040.298096, mean_absolute_error: 9.596819, mean_q: 13.949663
step 931; T: 475200; average reward: 12.52 - action: e; reward: -5.00
  565/2000: episode: 331, duration: 3.941s, episode steps: 4, steps per second: 1, episode reward: -35.000, mean reward: -8.750 [-10.000, -5.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.191 [0.000, 3.000], loss: 504.721130, mean_absolute_error: 9.511978, mean_q: 15.392131
step 936; T: 479100; average reward: 12.36 - action: e; reward: -39.19
  568/2000: episode: 332, duration: 2.757s, episode steps: 3, steps per second: 1, episode reward: -59.186, mean reward: -19.729 [-39.186, -10.000], mean action: 6.000 [2.000, 8.000], mean observation: 0.289 [0.000, 4.000], loss: 684.114563, mean_absolute_error: 9.251792, mean_q: 16.383051
step 937; T: 480000; average reward: 12.39 - action: e; reward: 20.62
  569/2000: episode: 333, duration: 0.540s, episode steps: 1, steps per second: 2, episode reward: 20.623, mean reward: 20.623 [20.623, 20.623], mean action: 2.000 [2.000, 2.000], mean observation: 0.200 [0.000, 4.000], loss: 283.423401, mean_absolute_error: 8.692374, mean_q: 13.707050
step 940; T: 481200; average reward: 12.34 - action: sw; reward: -5.00
  571/2000: episode: 334, duration: 1.531s, episode steps: 2, steps per second: 1, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.219 [0.000, 4.000], loss: 669.190918, mean_absolute_error: 9.367514, mean_q: 14.010444
step 941; T: 483300; average reward: 12.50 - action: e; reward: 68.84
  572/2000: episode: 335, duration: 0.695s, episode steps: 1, steps per second: 1, episode reward: 68.839, mean reward: 68.839 [68.839, 68.839], mean action: 2.000 [2.000, 2.000], mean observation: 0.216 [0.000, 3.000], loss: 406.215240, mean_absolute_error: 9.566752, mean_q: 11.921438
step 944; T: 485550; average reward: 12.93 - action: nw; reward: 156.17
  574/2000: episode: 336, duration: 1.900s, episode steps: 2, steps per second: 1, episode reward: 146.168, mean reward: 73.084 [-10.000, 156.168], mean action: 4.500 [2.000, 7.000], mean observation: 0.351 [0.000, 3.000], loss: 602.958435, mean_absolute_error: 10.070560, mean_q: 14.289177
step 946; T: 486450; average reward: 12.76 - action: nw; reward: -43.74
  575/2000: episode: 337, duration: 1.241s, episode steps: 1, steps per second: 1, episode reward: -48.742, mean reward: -48.742 [-48.742, -48.742], mean action: 7.000 [7.000, 7.000], mean observation: 0.237 [0.000, 3.000], loss: 183.515198, mean_absolute_error: 7.929522, mean_q: 14.617145
step 948; T: 487350; average reward: 12.71 - action: e; reward: -5.00
  576/2000: episode: 338, duration: 1.107s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.403 [0.000, 3.000], loss: 567.938721, mean_absolute_error: 9.969347, mean_q: 19.129513
step 949; T: 489900; average reward: 12.67 - action: e; reward: -1.45
  577/2000: episode: 339, duration: 0.717s, episode steps: 1, steps per second: 1, episode reward: -1.452, mean reward: -1.452 [-1.452, -1.452], mean action: 2.000 [2.000, 2.000], mean observation: 0.243 [0.000, 4.000], loss: 327.833862, mean_absolute_error: 9.263955, mean_q: 13.187229
step 954; T: 491250; average reward: 12.62 - action: n; reward: -5.00
  580/2000: episode: 340, duration: 2.857s, episode steps: 3, steps per second: 1, episode reward: -25.000, mean reward: -8.333 [-10.000, -5.000], mean action: 7.667 [7.000, 8.000], mean observation: 0.308 [0.000, 4.000], loss: 475.677368, mean_absolute_error: 9.714464, mean_q: 18.803083
step 955; T: 492300; average reward: 12.65 - action: nw; reward: 25.33
  581/2000: episode: 341, duration: 0.616s, episode steps: 1, steps per second: 2, episode reward: 25.331, mean reward: 25.331 [25.331, 25.331], mean action: 7.000 [7.000, 7.000], mean observation: 0.296 [0.000, 3.000], loss: 692.703613, mean_absolute_error: 8.842096, mean_q: 15.251859
step 957; T: 493350; average reward: 12.87 - action: nw; reward: 87.03
  582/2000: episode: 342, duration: 1.090s, episode steps: 1, steps per second: 1, episode reward: 82.031, mean reward: 82.031 [82.031, 82.031], mean action: 7.000 [7.000, 7.000], mean observation: 0.173 [0.000, 3.000], loss: 1008.395142, mean_absolute_error: 10.162313, mean_q: 17.910789
step 958; T: 495000; average reward: 13.17 - action: n; reward: 115.55
  583/2000: episode: 343, duration: 0.634s, episode steps: 1, steps per second: 2, episode reward: 115.548, mean reward: 115.548 [115.548, 115.548], mean action: 8.000 [8.000, 8.000], mean observation: 0.440 [0.000, 3.000], loss: 790.886047, mean_absolute_error: 10.366894, mean_q: 18.363729
step 959; T: 495750; average reward: 13.65 - action: nw; reward: 179.63
  584/2000: episode: 344, duration: 0.674s, episode steps: 1, steps per second: 1, episode reward: 179.629, mean reward: 179.629 [179.629, 179.629], mean action: 7.000 [7.000, 7.000], mean observation: 0.213 [0.000, 2.000], loss: 651.050598, mean_absolute_error: 10.344928, mean_q: 17.859936
step 960; T: 496500; average reward: 13.64 - action: nw; reward: 8.57
  585/2000: episode: 345, duration: 0.592s, episode steps: 1, steps per second: 2, episode reward: 8.570, mean reward: 8.570 [8.570, 8.570], mean action: 7.000 [7.000, 7.000], mean observation: 0.365 [0.000, 3.000], loss: 741.399231, mean_absolute_error: 10.265802, mean_q: 15.798697
step 961; T: 496950; average reward: 13.65 - action: nw; reward: 15.78
  586/2000: episode: 346, duration: 0.598s, episode steps: 1, steps per second: 2, episode reward: 15.779, mean reward: 15.779 [15.779, 15.779], mean action: 7.000 [7.000, 7.000], mean observation: 0.248 [0.000, 2.000], loss: 306.616913, mean_absolute_error: 8.855333, mean_q: 18.530052
step 963; T: 497400; average reward: 13.57 - action: nw; reward: -14.13
  587/2000: episode: 347, duration: 1.148s, episode steps: 1, steps per second: 1, episode reward: -19.128, mean reward: -19.128 [-19.128, -19.128], mean action: 7.000 [7.000, 7.000], mean observation: 0.235 [0.000, 2.000], loss: 805.573425, mean_absolute_error: 9.525421, mean_q: 17.600182
step 969; T: 498900; average reward: 13.51 - action: nw; reward: -5.00
  590/2000: episode: 348, duration: 3.003s, episode steps: 3, steps per second: 1, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.333 [2.000, 7.000], mean observation: 0.237 [0.000, 2.000], loss: 413.373688, mean_absolute_error: 9.068542, mean_q: 15.365354
step 972; T: 501000; average reward: 13.52 - action: nw; reward: 16.58
  592/2000: episode: 349, duration: 1.555s, episode steps: 2, steps per second: 1, episode reward: 6.581, mean reward: 3.290 [-10.000, 16.581], mean action: 7.500 [7.000, 8.000], mean observation: 0.343 [0.000, 4.000], loss: 357.287811, mean_absolute_error: 9.592855, mean_q: 19.294615
step 973; T: 502050; average reward: 13.61 - action: e; reward: 43.30
  593/2000: episode: 350, duration: 0.531s, episode steps: 1, steps per second: 2, episode reward: 43.300, mean reward: 43.300 [43.300, 43.300], mean action: 2.000 [2.000, 2.000], mean observation: 0.205 [0.000, 3.000], loss: 635.240234, mean_absolute_error: 10.408452, mean_q: 21.403471
step 975; T: 502650; average reward: 13.43 - action: sw; reward: -49.75
  594/2000: episode: 351, duration: 1.068s, episode steps: 1, steps per second: 1, episode reward: -54.752, mean reward: -54.752 [-54.752, -54.752], mean action: 5.000 [5.000, 5.000], mean observation: 0.301 [0.000, 3.000], loss: 1022.388855, mean_absolute_error: 10.455170, mean_q: 15.431431
step 978; T: 504000; average reward: 13.63 - action: e; reward: 83.98
  596/2000: episode: 352, duration: 1.485s, episode steps: 2, steps per second: 1, episode reward: 73.982, mean reward: 36.991 [-10.000, 83.982], mean action: 5.000 [2.000, 8.000], mean observation: 0.232 [0.000, 4.000], loss: 717.995178, mean_absolute_error: 10.005831, mean_q: 17.177677
step 979; T: 505050; average reward: 13.51 - action: e; reward: -28.81
  597/2000: episode: 353, duration: 0.533s, episode steps: 1, steps per second: 2, episode reward: -28.814, mean reward: -28.814 [-28.814, -28.814], mean action: 2.000 [2.000, 2.000], mean observation: 0.139 [0.000, 2.000], loss: 389.345795, mean_absolute_error: 9.640167, mean_q: 17.395159
step 982; T: 506550; average reward: 13.45 - action: sw; reward: -5.00
  599/2000: episode: 354, duration: 1.587s, episode steps: 2, steps per second: 1, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.169 [0.000, 2.000], loss: 710.175537, mean_absolute_error: 10.228507, mean_q: 17.865627
step 984; T: 507150; average reward: 13.48 - action: e; reward: 20.84
  600/2000: episode: 355, duration: 1.192s, episode steps: 1, steps per second: 1, episode reward: 15.843, mean reward: 15.843 [15.843, 15.843], mean action: 2.000 [2.000, 2.000], mean observation: 0.275 [0.000, 2.000], loss: 368.652649, mean_absolute_error: 8.023650, mean_q: 11.899736
step 992; T: 508800; average reward: 13.64 - action: ne; reward: 72.08
  604/2000: episode: 356, duration: 3.782s, episode steps: 4, steps per second: 1, episode reward: 37.080, mean reward: 9.270 [-10.000, 67.080], mean action: 3.750 [1.000, 7.000], mean observation: 0.282 [0.000, 5.000], loss: 865.058716, mean_absolute_error: 9.987183, mean_q: 16.558781
step 993; T: 509250; average reward: 13.82 - action: e; reward: 77.83
  605/2000: episode: 357, duration: 0.479s, episode steps: 1, steps per second: 2, episode reward: 77.828, mean reward: 77.828 [77.828, 77.828], mean action: 2.000 [2.000, 2.000], mean observation: 0.352 [0.000, 3.000], loss: 605.158936, mean_absolute_error: 10.333584, mean_q: 15.408146
step 999; T: 510450; average reward: 13.77 - action: sw; reward: -5.00
  608/2000: episode: 358, duration: 2.864s, episode steps: 3, steps per second: 1, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 4.667 [1.000, 8.000], mean observation: 0.216 [0.000, 4.000], loss: 480.129059, mean_absolute_error: 9.294068, mean_q: 14.977927
step 1007; T: 512400; average reward: 13.71 - action: nw; reward: -5.00
  612/2000: episode: 359, duration: 3.825s, episode steps: 4, steps per second: 1, episode reward: -40.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.263 [0.000, 4.000], loss: 597.426086, mean_absolute_error: 9.662046, mean_q: 15.348979
step 1009; T: 514500; average reward: 13.66 - action: e; reward: -7.44
  613/2000: episode: 360, duration: 1.005s, episode steps: 1, steps per second: 1, episode reward: -12.436, mean reward: -12.436 [-12.436, -12.436], mean action: 2.000 [2.000, 2.000], mean observation: 0.277 [0.000, 2.000], loss: 403.999268, mean_absolute_error: 8.478827, mean_q: 14.956501
step 1010; T: 518850; average reward: 13.53 - action: n; reward: -30.80
  614/2000: episode: 361, duration: 0.679s, episode steps: 1, steps per second: 1, episode reward: -30.797, mean reward: -30.797 [-30.797, -30.797], mean action: 8.000 [8.000, 8.000], mean observation: 0.283 [0.000, 3.000], loss: 728.477234, mean_absolute_error: 9.643650, mean_q: 15.428410
step 1013; T: 521100; average reward: 12.90 - action: e; reward: -214.64
  616/2000: episode: 362, duration: 1.526s, episode steps: 2, steps per second: 1, episode reward: -224.642, mean reward: -112.321 [-214.642, -10.000], mean action: 3.500 [2.000, 5.000], mean observation: 0.225 [0.000, 3.000], loss: 565.664429, mean_absolute_error: 9.386028, mean_q: 15.258643
step 1015; T: 522150; average reward: 12.85 - action: nw; reward: -5.00
  617/2000: episode: 363, duration: 1.097s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.243 [0.000, 3.000], loss: 611.370972, mean_absolute_error: 9.463354, mean_q: 13.634595
step 1017; T: 523500; average reward: 12.72 - action: e; reward: -35.01
  618/2000: episode: 364, duration: 0.966s, episode steps: 1, steps per second: 1, episode reward: -40.006, mean reward: -40.006 [-40.006, -40.006], mean action: 2.000 [2.000, 2.000], mean observation: 0.173 [0.000, 2.000], loss: 1448.635498, mean_absolute_error: 11.928743, mean_q: 12.310029
step 1021; T: 526200; average reward: 12.67 - action: nw; reward: -5.00
  620/2000: episode: 365, duration: 1.962s, episode steps: 2, steps per second: 1, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.231 [0.000, 2.000], loss: 600.545776, mean_absolute_error: 9.574493, mean_q: 12.540409
step 1026; T: 528750; average reward: 12.83 - action: nw; reward: 68.93
  623/2000: episode: 366, duration: 2.521s, episode steps: 3, steps per second: 1, episode reward: 48.928, mean reward: 16.309 [-10.000, 68.928], mean action: 5.667 [2.000, 8.000], mean observation: 0.358 [0.000, 4.000], loss: 588.650330, mean_absolute_error: 9.714438, mean_q: 12.113403
step 1030; T: 529950; average reward: 12.72 - action: nw; reward: -24.55
  625/2000: episode: 367, duration: 2.047s, episode steps: 2, steps per second: 1, episode reward: -39.552, mean reward: -19.776 [-29.552, -10.000], mean action: 5.000 [3.000, 7.000], mean observation: 0.361 [0.000, 3.000], loss: 597.114685, mean_absolute_error: 9.734597, mean_q: 13.657779
step 1035; T: 531450; average reward: 12.78 - action: nw; reward: 34.72
  628/2000: episode: 368, duration: 2.403s, episode steps: 3, steps per second: 1, episode reward: 14.724, mean reward: 4.908 [-10.000, 34.724], mean action: 7.333 [7.000, 8.000], mean observation: 0.396 [0.000, 5.000], loss: 777.534180, mean_absolute_error: 10.175936, mean_q: 15.150700
step 1036; T: 532650; average reward: 13.25 - action: nw; reward: 186.04
  629/2000: episode: 369, duration: 0.526s, episode steps: 1, steps per second: 2, episode reward: 186.039, mean reward: 186.039 [186.039, 186.039], mean action: 7.000 [7.000, 7.000], mean observation: 0.211 [0.000, 2.000], loss: 471.829285, mean_absolute_error: 9.502697, mean_q: 13.797999
step 1037; T: 532950; average reward: 13.28 - action: e; reward: 23.71
  630/2000: episode: 370, duration: 0.519s, episode steps: 1, steps per second: 2, episode reward: 23.711, mean reward: 23.711 [23.711, 23.711], mean action: 2.000 [2.000, 2.000], mean observation: 0.253 [0.000, 4.000], loss: 1682.316895, mean_absolute_error: 11.486703, mean_q: 12.901642
step 1041; T: 535950; average reward: 13.23 - action: nw; reward: -5.00
  632/2000: episode: 371, duration: 2.510s, episode steps: 2, steps per second: 1, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.276 [0.000, 4.000], loss: 537.102966, mean_absolute_error: 9.739600, mean_q: 13.633997
step 1047; T: 537450; average reward: 13.20 - action: noop; reward: 0.00
  635/2000: episode: 372, duration: 3.592s, episode steps: 3, steps per second: 1, episode reward: -20.000, mean reward: -6.667 [-10.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.344 [0.000, 3.000], loss: 633.311035, mean_absolute_error: 9.852880, mean_q: 15.608655
step 1049; T: 538350; average reward: 13.12 - action: nw; reward: -16.43
  636/2000: episode: 373, duration: 1.135s, episode steps: 1, steps per second: 1, episode reward: -21.426, mean reward: -21.426 [-21.426, -21.426], mean action: 7.000 [7.000, 7.000], mean observation: 0.216 [0.000, 4.000], loss: 825.633057, mean_absolute_error: 9.706736, mean_q: 15.697267
step 1054; T: 539850; average reward: 12.90 - action: ne; reward: -68.19
  639/2000: episode: 374, duration: 2.730s, episode steps: 3, steps per second: 1, episode reward: -88.189, mean reward: -29.396 [-68.189, -10.000], mean action: 3.333 [1.000, 7.000], mean observation: 0.241 [0.000, 3.000], loss: 662.576599, mean_absolute_error: 9.107080, mean_q: 12.694389
step 1056; T: 541350; average reward: 12.99 - action: e; reward: 47.95
  640/2000: episode: 375, duration: 1.112s, episode steps: 1, steps per second: 1, episode reward: 42.954, mean reward: 42.954 [42.954, 42.954], mean action: 2.000 [2.000, 2.000], mean observation: 0.248 [0.000, 2.000], loss: 361.144073, mean_absolute_error: 9.069084, mean_q: 14.651734
step 1058; T: 541950; average reward: 12.92 - action: e; reward: -13.62
  641/2000: episode: 376, duration: 1.106s, episode steps: 1, steps per second: 1, episode reward: -18.618, mean reward: -18.618 [-18.618, -18.618], mean action: 2.000 [2.000, 2.000], mean observation: 0.331 [0.000, 3.000], loss: 851.422241, mean_absolute_error: 11.151551, mean_q: 17.942837
step 1059; T: 543000; average reward: 12.95 - action: n; reward: 23.32
  642/2000: episode: 377, duration: 0.596s, episode steps: 1, steps per second: 2, episode reward: 23.317, mean reward: 23.317 [23.317, 23.317], mean action: 8.000 [8.000, 8.000], mean observation: 0.323 [0.000, 5.000], loss: 793.350647, mean_absolute_error: 10.053751, mean_q: 20.227215
step 1061; T: 544950; average reward: 12.90 - action: nw; reward: -5.06
  643/2000: episode: 378, duration: 1.257s, episode steps: 1, steps per second: 1, episode reward: -10.055, mean reward: -10.055 [-10.055, -10.055], mean action: 7.000 [7.000, 7.000], mean observation: 0.259 [0.000, 2.000], loss: 1610.380371, mean_absolute_error: 9.646132, mean_q: 11.936672
step 1062; T: 545700; average reward: 12.44 - action: nw; reward: -162.24
  644/2000: episode: 379, duration: 0.594s, episode steps: 1, steps per second: 2, episode reward: -162.236, mean reward: -162.236 [-162.236, -162.236], mean action: 7.000 [7.000, 7.000], mean observation: 0.373 [0.000, 3.000], loss: 845.295410, mean_absolute_error: 10.173536, mean_q: 16.929668
step 1063; T: 546750; average reward: 12.35 - action: e; reward: -22.36
  645/2000: episode: 380, duration: 0.571s, episode steps: 1, steps per second: 2, episode reward: -22.358, mean reward: -22.358 [-22.358, -22.358], mean action: 2.000 [2.000, 2.000], mean observation: 0.133 [0.000, 2.000], loss: 1127.771240, mean_absolute_error: 10.428200, mean_q: 11.361288
step 1064; T: 547500; average reward: 12.30 - action: sw; reward: -5.00
  646/2000: episode: 381, duration: 0.572s, episode steps: 1, steps per second: 2, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.421 [0.000, 4.000], loss: 1340.092407, mean_absolute_error: 11.296286, mean_q: 15.288803
step 1066; T: 549900; average reward: 12.26 - action: ne; reward: -5.00
  647/2000: episode: 382, duration: 1.161s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.411 [0.000, 3.000], loss: 2397.375977, mean_absolute_error: 11.579266, mean_q: 13.742027
step 1067; T: 550800; average reward: 12.11 - action: ne; reward: -46.04
  648/2000: episode: 383, duration: 0.547s, episode steps: 1, steps per second: 2, episode reward: -46.035, mean reward: -46.035 [-46.035, -46.035], mean action: 1.000 [1.000, 1.000], mean observation: 0.184 [0.000, 3.000], loss: 685.297119, mean_absolute_error: 10.144861, mean_q: 14.185322
step 1070; T: 553950; average reward: 12.29 - action: e; reward: 81.84
  650/2000: episode: 384, duration: 1.819s, episode steps: 2, steps per second: 1, episode reward: 71.837, mean reward: 35.918 [-10.000, 81.837], mean action: 5.000 [2.000, 8.000], mean observation: 0.293 [0.000, 3.000], loss: 509.235321, mean_absolute_error: 10.061681, mean_q: 12.630964
step 1071; T: 554700; average reward: 12.36 - action: n; reward: 39.96
  651/2000: episode: 385, duration: 0.555s, episode steps: 1, steps per second: 2, episode reward: 39.958, mean reward: 39.958 [39.958, 39.958], mean action: 8.000 [8.000, 8.000], mean observation: 0.283 [0.000, 2.000], loss: 683.751648, mean_absolute_error: 11.306168, mean_q: 15.560378
step 1072; T: 555600; average reward: 12.17 - action: ne; reward: -60.35
  652/2000: episode: 386, duration: 0.550s, episode steps: 1, steps per second: 2, episode reward: -60.353, mean reward: -60.353 [-60.353, -60.353], mean action: 1.000 [1.000, 1.000], mean observation: 0.221 [0.000, 3.000], loss: 513.823914, mean_absolute_error: 9.434227, mean_q: 12.934175
step 1078; T: 557850; average reward: 12.13 - action: ne; reward: -5.00
  655/2000: episode: 387, duration: 3.405s, episode steps: 3, steps per second: 1, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.241 [0.000, 3.000], loss: 743.406677, mean_absolute_error: 10.633113, mean_q: 11.798302
step 1079; T: 560250; average reward: 12.04 - action: nw; reward: -20.57
  656/2000: episode: 388, duration: 0.652s, episode steps: 1, steps per second: 2, episode reward: -20.574, mean reward: -20.574 [-20.574, -20.574], mean action: 7.000 [7.000, 7.000], mean observation: 0.427 [0.000, 4.000], loss: 816.849609, mean_absolute_error: 10.364592, mean_q: 12.091436
step 1081; T: 560850; average reward: 12.00 - action: ne; reward: -6.44
  657/2000: episode: 389, duration: 1.078s, episode steps: 1, steps per second: 1, episode reward: -11.445, mean reward: -11.445 [-11.445, -11.445], mean action: 1.000 [1.000, 1.000], mean observation: 0.373 [0.000, 3.000], loss: 1368.742676, mean_absolute_error: 12.327868, mean_q: 13.825852
step 1082; T: 562200; average reward: 12.05 - action: nw; reward: 34.78
  658/2000: episode: 390, duration: 0.701s, episode steps: 1, steps per second: 1, episode reward: 34.779, mean reward: 34.779 [34.779, 34.779], mean action: 7.000 [7.000, 7.000], mean observation: 0.352 [0.000, 3.000], loss: 533.928772, mean_absolute_error: 10.459446, mean_q: 11.282545
step 1086; T: 563100; average reward: 11.97 - action: nw; reward: -22.03
  660/2000: episode: 391, duration: 2.142s, episode steps: 2, steps per second: 1, episode reward: -37.025, mean reward: -18.513 [-27.025, -10.000], mean action: 7.500 [7.000, 8.000], mean observation: 0.357 [0.000, 3.000], loss: 781.018555, mean_absolute_error: 9.920483, mean_q: 14.175864
step 1089; T: 564000; average reward: 11.94 - action: noop; reward: 0.00
  662/2000: episode: 392, duration: 1.732s, episode steps: 2, steps per second: 1, episode reward: -10.000, mean reward: -5.000 [-10.000, 0.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.387 [0.000, 3.000], loss: 875.240784, mean_absolute_error: 10.076928, mean_q: 12.570900
step 1091; T: 566400; average reward: 11.78 - action: ne; reward: -49.73
  663/2000: episode: 393, duration: 1.192s, episode steps: 1, steps per second: 1, episode reward: -54.725, mean reward: -54.725 [-54.725, -54.725], mean action: 1.000 [1.000, 1.000], mean observation: 0.171 [0.000, 4.000], loss: 445.129211, mean_absolute_error: 11.099066, mean_q: 15.845469
step 1093; T: 568800; average reward: 11.74 - action: ne; reward: -5.00
  664/2000: episode: 394, duration: 1.168s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.224 [0.000, 2.000], loss: 1085.009277, mean_absolute_error: 10.873586, mean_q: 14.417794
step 1099; T: 570900; average reward: 11.69 - action: ne; reward: -5.00
  667/2000: episode: 395, duration: 3.429s, episode steps: 3, steps per second: 1, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 3.000 [1.000, 7.000], mean observation: 0.178 [0.000, 3.000], loss: 753.641174, mean_absolute_error: 10.779237, mean_q: 13.325442
step 1101; T: 571500; average reward: 11.78 - action: n; reward: 45.57
  668/2000: episode: 396, duration: 1.070s, episode steps: 1, steps per second: 1, episode reward: 40.573, mean reward: 40.573 [40.573, 40.573], mean action: 8.000 [8.000, 8.000], mean observation: 0.296 [0.000, 2.000], loss: 716.137878, mean_absolute_error: 10.792285, mean_q: 15.183764
step 1102; T: 572400; average reward: 12.15 - action: n; reward: 159.25
  669/2000: episode: 397, duration: 0.607s, episode steps: 1, steps per second: 2, episode reward: 159.250, mean reward: 159.250 [159.250, 159.250], mean action: 8.000 [8.000, 8.000], mean observation: 0.432 [0.000, 3.000], loss: 479.188690, mean_absolute_error: 10.029519, mean_q: 14.978298
step 1103; T: 573000; average reward: 12.33 - action: nw; reward: 82.67
  670/2000: episode: 398, duration: 0.599s, episode steps: 1, steps per second: 2, episode reward: 82.674, mean reward: 82.674 [82.674, 82.674], mean action: 7.000 [7.000, 7.000], mean observation: 0.477 [0.000, 3.000], loss: 666.305298, mean_absolute_error: 10.164125, mean_q: 13.554542
step 1108; T: 575400; average reward: 12.48 - action: n; reward: 71.56
  673/2000: episode: 399, duration: 2.921s, episode steps: 3, steps per second: 1, episode reward: 51.564, mean reward: 17.188 [-10.000, 71.564], mean action: 7.000 [6.000, 8.000], mean observation: 0.337 [0.000, 3.000], loss: 568.146667, mean_absolute_error: 9.817363, mean_q: 12.415860
step 1111; T: 577950; average reward: 12.77 - action: nw; reward: 129.90
  675/2000: episode: 400, duration: 1.842s, episode steps: 2, steps per second: 1, episode reward: 119.896, mean reward: 59.948 [-10.000, 129.896], mean action: 4.000 [1.000, 7.000], mean observation: 0.367 [0.000, 3.000], loss: 1069.444092, mean_absolute_error: 10.194462, mean_q: 12.495235
step 1117; T: 580500; average reward: 12.73 - action: ne; reward: -5.00
  678/2000: episode: 401, duration: 3.345s, episode steps: 3, steps per second: 1, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 3.667 [1.000, 8.000], mean observation: 0.212 [0.000, 4.000], loss: 719.405579, mean_absolute_error: 10.438953, mean_q: 13.882205
step 1121; T: 581550; average reward: 12.57 - action: nw; reward: -49.98
  680/2000: episode: 402, duration: 2.380s, episode steps: 2, steps per second: 1, episode reward: -64.978, mean reward: -32.489 [-54.978, -10.000], mean action: 4.500 [2.000, 7.000], mean observation: 0.332 [0.000, 3.000], loss: 337.568878, mean_absolute_error: 9.610189, mean_q: 11.745002
step 1122; T: 582000; average reward: 12.42 - action: e; reward: -47.85
  681/2000: episode: 403, duration: 0.560s, episode steps: 1, steps per second: 2, episode reward: -47.847, mean reward: -47.847 [-47.847, -47.847], mean action: 2.000 [2.000, 2.000], mean observation: 0.277 [0.000, 2.000], loss: 630.240295, mean_absolute_error: 9.820258, mean_q: 13.625668
step 1124; T: 584100; average reward: 12.38 - action: n; reward: -5.00
  682/2000: episode: 404, duration: 1.174s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.373 [0.000, 3.000], loss: 371.095947, mean_absolute_error: 9.745017, mean_q: 10.893291
step 1127; T: 585000; average reward: 12.24 - action: n; reward: -44.71
  684/2000: episode: 405, duration: 1.786s, episode steps: 2, steps per second: 1, episode reward: -54.711, mean reward: -27.355 [-44.711, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.322 [0.000, 4.000], loss: 611.674805, mean_absolute_error: 9.561899, mean_q: 11.524954
step 1128; T: 585900; average reward: 12.25 - action: nw; reward: 18.02
  685/2000: episode: 406, duration: 0.568s, episode steps: 1, steps per second: 2, episode reward: 18.021, mean reward: 18.021 [18.021, 18.021], mean action: 7.000 [7.000, 7.000], mean observation: 0.227 [0.000, 3.000], loss: 827.153259, mean_absolute_error: 9.401321, mean_q: 10.256108
step 1135; T: 588450; average reward: 12.41 - action: ne; reward: 78.79
  689/2000: episode: 407, duration: 4.173s, episode steps: 4, steps per second: 1, episode reward: 48.792, mean reward: 12.198 [-10.000, 78.792], mean action: 3.000 [1.000, 6.000], mean observation: 0.314 [0.000, 3.000], loss: 832.675354, mean_absolute_error: 10.067790, mean_q: 12.129589
step 1137; T: 590250; average reward: 12.37 - action: nw; reward: -5.00
  690/2000: episode: 408, duration: 1.205s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.267 [0.000, 4.000], loss: 345.911652, mean_absolute_error: 9.982471, mean_q: 15.217108
step 1142; T: 594450; average reward: 12.39 - action: nw; reward: 18.96
  693/2000: episode: 409, duration: 3.096s, episode steps: 3, steps per second: 1, episode reward: -1.036, mean reward: -0.345 [-10.000, 18.964], mean action: 5.667 [2.000, 8.000], mean observation: 0.376 [0.000, 3.000], loss: 839.379700, mean_absolute_error: 10.367544, mean_q: 12.769371
step 1144; T: 595650; average reward: 12.45 - action: ne; reward: 36.50
  694/2000: episode: 410, duration: 1.169s, episode steps: 1, steps per second: 1, episode reward: 31.497, mean reward: 31.497 [31.497, 31.497], mean action: 1.000 [1.000, 1.000], mean observation: 0.272 [0.000, 2.000], loss: 687.647217, mean_absolute_error: 9.785797, mean_q: 15.065729
step 1147; T: 596550; average reward: 12.40 - action: se; reward: -5.00
  696/2000: episode: 411, duration: 1.861s, episode steps: 2, steps per second: 1, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.268 [0.000, 2.000], loss: 392.853027, mean_absolute_error: 8.889395, mean_q: 11.858852
step 1154; T: 598200; average reward: 12.36 - action: nw; reward: -5.00
  700/2000: episode: 412, duration: 4.790s, episode steps: 4, steps per second: 1, episode reward: -35.000, mean reward: -8.750 [-10.000, -5.000], mean action: 7.250 [7.000, 8.000], mean observation: 0.381 [0.000, 4.000], loss: 667.828491, mean_absolute_error: 9.723124, mean_q: 10.489241
step 1157; T: 599550; average reward: 12.21 - action: n; reward: -49.64
  702/2000: episode: 413, duration: 1.852s, episode steps: 2, steps per second: 1, episode reward: -59.638, mean reward: -29.819 [-49.638, -10.000], mean action: 7.500 [7.000, 8.000], mean observation: 0.433 [0.000, 5.000], loss: 1191.243896, mean_absolute_error: 10.393553, mean_q: 12.562319
step 1159; T: 600450; average reward: 12.34 - action: nw; reward: 65.30
  703/2000: episode: 414, duration: 1.289s, episode steps: 1, steps per second: 1, episode reward: 60.295, mean reward: 60.295 [60.295, 60.295], mean action: 7.000 [7.000, 7.000], mean observation: 0.280 [0.000, 2.000], loss: 468.905701, mean_absolute_error: 10.581180, mean_q: 16.673637
step 1160; T: 601200; average reward: 12.36 - action: n; reward: 19.67
  704/2000: episode: 415, duration: 0.622s, episode steps: 1, steps per second: 2, episode reward: 19.669, mean reward: 19.669 [19.669, 19.669], mean action: 8.000 [8.000, 8.000], mean observation: 0.451 [0.000, 3.000], loss: 771.834106, mean_absolute_error: 10.674662, mean_q: 11.635712
step 1161; T: 601800; average reward: 12.19 - action: s; reward: -59.19
  705/2000: episode: 416, duration: 0.655s, episode steps: 1, steps per second: 2, episode reward: -59.189, mean reward: -59.189 [-59.189, -59.189], mean action: 4.000 [4.000, 4.000], mean observation: 0.379 [0.000, 4.000], loss: 669.507568, mean_absolute_error: 10.149206, mean_q: 14.303230
step 1162; T: 602850; average reward: 12.13 - action: noop; reward: -9.45
  706/2000: episode: 417, duration: 0.650s, episode steps: 1, steps per second: 2, episode reward: -9.454, mean reward: -9.454 [-9.454, -9.454], mean action: 0.000 [0.000, 0.000], mean observation: 0.411 [0.000, 3.000], loss: 395.680267, mean_absolute_error: 9.140394, mean_q: 10.567610
step 1163; T: 603300; average reward: 12.24 - action: n; reward: 57.64
  707/2000: episode: 418, duration: 0.610s, episode steps: 1, steps per second: 2, episode reward: 57.637, mean reward: 57.637 [57.637, 57.637], mean action: 8.000 [8.000, 8.000], mean observation: 0.451 [0.000, 3.000], loss: 978.057922, mean_absolute_error: 10.509104, mean_q: 12.547131
step 1164; T: 604200; average reward: 12.06 - action: n; reward: -62.74
  708/2000: episode: 419, duration: 0.637s, episode steps: 1, steps per second: 2, episode reward: -62.736, mean reward: -62.736 [-62.736, -62.736], mean action: 8.000 [8.000, 8.000], mean observation: 0.269 [0.000, 3.000], loss: 324.721802, mean_absolute_error: 9.170918, mean_q: 15.544675
step 1168; T: 605700; average reward: 12.02 - action: nw; reward: -5.00
  710/2000: episode: 420, duration: 2.448s, episode steps: 2, steps per second: 1, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 4.000 [1.000, 7.000], mean observation: 0.236 [0.000, 3.000], loss: 610.354919, mean_absolute_error: 9.761062, mean_q: 14.126518
step 1171; T: 607800; average reward: 11.77 - action: n; reward: -94.99
  712/2000: episode: 421, duration: 1.834s, episode steps: 2, steps per second: 1, episode reward: -104.987, mean reward: -52.493 [-94.987, -10.000], mean action: 4.500 [1.000, 8.000], mean observation: 0.304 [0.000, 3.000], loss: 304.959290, mean_absolute_error: 9.040916, mean_q: 13.216856
step 1173; T: 609900; average reward: 11.83 - action: nw; reward: 39.20
  713/2000: episode: 422, duration: 1.350s, episode steps: 1, steps per second: 1, episode reward: 34.200, mean reward: 34.200 [34.200, 34.200], mean action: 7.000 [7.000, 7.000], mean observation: 0.232 [0.000, 3.000], loss: 909.329102, mean_absolute_error: 10.498915, mean_q: 13.408882
step 1176; T: 611100; average reward: 11.90 - action: nw; reward: 39.02
  715/2000: episode: 423, duration: 1.758s, episode steps: 2, steps per second: 1, episode reward: 29.023, mean reward: 14.512 [-10.000, 39.023], mean action: 4.500 [2.000, 7.000], mean observation: 0.199 [0.000, 3.000], loss: 1155.480225, mean_absolute_error: 10.329911, mean_q: 13.631530
step 1177; T: 612150; average reward: 11.86 - action: ne; reward: -5.00
  716/2000: episode: 424, duration: 0.625s, episode steps: 1, steps per second: 2, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.227 [0.000, 2.000], loss: 501.807037, mean_absolute_error: 9.482621, mean_q: 11.485837
step 1180; T: 614100; average reward: 11.79 - action: e; reward: -16.39
  718/2000: episode: 425, duration: 1.946s, episode steps: 2, steps per second: 1, episode reward: -26.394, mean reward: -13.197 [-16.394, -10.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.232 [0.000, 3.000], loss: 482.372253, mean_absolute_error: 9.748678, mean_q: 13.124533
step 1184; T: 615000; average reward: 11.81 - action: nw; reward: 18.44
  720/2000: episode: 426, duration: 2.367s, episode steps: 2, steps per second: 1, episode reward: 3.436, mean reward: 1.718 [-10.000, 13.436], mean action: 4.500 [2.000, 7.000], mean observation: 0.200 [0.000, 2.000], loss: 567.496216, mean_absolute_error: 9.240290, mean_q: 11.540990
step 1187; T: 617400; average reward: 11.73 - action: se; reward: -22.54
  722/2000: episode: 427, duration: 1.987s, episode steps: 2, steps per second: 1, episode reward: -32.537, mean reward: -16.269 [-22.537, -10.000], mean action: 4.500 [3.000, 6.000], mean observation: 0.336 [0.000, 4.000], loss: 437.989136, mean_absolute_error: 9.236926, mean_q: 10.872047
step 1188; T: 619800; average reward: 11.76 - action: nw; reward: 23.89
  723/2000: episode: 428, duration: 0.746s, episode steps: 1, steps per second: 1, episode reward: 23.889, mean reward: 23.889 [23.889, 23.889], mean action: 7.000 [7.000, 7.000], mean observation: 0.272 [0.000, 3.000], loss: 797.917175, mean_absolute_error: 9.658042, mean_q: 12.041070
step 1189; T: 621450; average reward: 11.79 - action: n; reward: 27.68
  724/2000: episode: 429, duration: 0.656s, episode steps: 1, steps per second: 2, episode reward: 27.679, mean reward: 27.679 [27.679, 27.679], mean action: 8.000 [8.000, 8.000], mean observation: 0.408 [0.000, 4.000], loss: 531.747986, mean_absolute_error: 11.112304, mean_q: 14.600032
step 1190; T: 621900; average reward: 11.85 - action: nw; reward: 38.16
  725/2000: episode: 430, duration: 0.590s, episode steps: 1, steps per second: 2, episode reward: 38.161, mean reward: 38.161 [38.161, 38.161], mean action: 7.000 [7.000, 7.000], mean observation: 0.320 [0.000, 2.000], loss: 219.039429, mean_absolute_error: 9.562103, mean_q: 11.260907
step 1191; T: 623700; average reward: 12.02 - action: n; reward: 83.17
  726/2000: episode: 431, duration: 0.787s, episode steps: 1, steps per second: 1, episode reward: 83.175, mean reward: 83.175 [83.175, 83.175], mean action: 8.000 [8.000, 8.000], mean observation: 0.397 [0.000, 3.000], loss: 495.496887, mean_absolute_error: 9.535330, mean_q: 12.318644
step 1192; T: 627300; average reward: 12.06 - action: nw; reward: 30.13
  727/2000: episode: 432, duration: 0.796s, episode steps: 1, steps per second: 1, episode reward: 30.126, mean reward: 30.126 [30.126, 30.126], mean action: 7.000 [7.000, 7.000], mean observation: 0.237 [0.000, 3.000], loss: 244.121506, mean_absolute_error: 9.111309, mean_q: 10.602474
step 1193; T: 628350; average reward: 12.12 - action: w; reward: 36.32
  728/2000: episode: 433, duration: 0.644s, episode steps: 1, steps per second: 2, episode reward: 36.318, mean reward: 36.318 [36.318, 36.318], mean action: 6.000 [6.000, 6.000], mean observation: 0.416 [0.000, 3.000], loss: 410.022491, mean_absolute_error: 10.029634, mean_q: 15.902569
step 1194; T: 630300; average reward: 11.91 - action: n; reward: -77.59
  729/2000: episode: 434, duration: 0.674s, episode steps: 1, steps per second: 1, episode reward: -77.586, mean reward: -77.586 [-77.586, -77.586], mean action: 8.000 [8.000, 8.000], mean observation: 0.200 [0.000, 2.000], loss: 513.095215, mean_absolute_error: 9.907002, mean_q: 8.954069
step 1195; T: 631050; average reward: 11.87 - action: nw; reward: -5.00
  730/2000: episode: 435, duration: 0.676s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.443 [0.000, 3.000], loss: 874.055542, mean_absolute_error: 9.927189, mean_q: 11.039119
step 1198; T: 631650; average reward: 11.88 - action: nw; reward: 15.52
  732/2000: episode: 436, duration: 1.923s, episode steps: 2, steps per second: 1, episode reward: 5.516, mean reward: 2.758 [-10.000, 15.516], mean action: 6.500 [6.000, 7.000], mean observation: 0.481 [0.000, 3.000], loss: 566.037964, mean_absolute_error: 10.423508, mean_q: 12.203543
step 1200; T: 632550; average reward: 11.84 - action: ne; reward: -5.00
  733/2000: episode: 437, duration: 1.254s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.267 [0.000, 2.000], loss: 313.593292, mean_absolute_error: 10.355613, mean_q: 14.212620
step 1201; T: 634050; average reward: 11.65 - action: nw; reward: -70.17
  734/2000: episode: 438, duration: 0.688s, episode steps: 1, steps per second: 1, episode reward: -70.175, mean reward: -70.175 [-70.175, -70.175], mean action: 7.000 [7.000, 7.000], mean observation: 0.232 [0.000, 4.000], loss: 592.133301, mean_absolute_error: 10.489099, mean_q: 7.910810
step 1202; T: 635100; average reward: 11.62 - action: n; reward: -5.00
  735/2000: episode: 439, duration: 0.684s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.240 [0.000, 3.000], loss: 776.677307, mean_absolute_error: 10.043335, mean_q: 11.797159
step 1203; T: 636450; average reward: 11.62 - action: ne; reward: 13.92
  736/2000: episode: 440, duration: 0.679s, episode steps: 1, steps per second: 1, episode reward: 13.923, mean reward: 13.923 [13.923, 13.923], mean action: 1.000 [1.000, 1.000], mean observation: 0.301 [0.000, 2.000], loss: 893.518494, mean_absolute_error: 11.071959, mean_q: 12.602311
step 1204; T: 637350; average reward: 11.75 - action: nw; reward: 68.56
  737/2000: episode: 441, duration: 0.753s, episode steps: 1, steps per second: 1, episode reward: 68.559, mean reward: 68.559 [68.559, 68.559], mean action: 7.000 [7.000, 7.000], mean observation: 0.171 [0.000, 3.000], loss: 1107.934570, mean_absolute_error: 11.150587, mean_q: 12.759325
step 1206; T: 637950; average reward: 11.81 - action: n; reward: 39.99
  738/2000: episode: 442, duration: 1.184s, episode steps: 1, steps per second: 1, episode reward: 34.986, mean reward: 34.986 [34.986, 34.986], mean action: 8.000 [8.000, 8.000], mean observation: 0.227 [0.000, 3.000], loss: 640.792664, mean_absolute_error: 10.571163, mean_q: 10.139519
step 1207; T: 639750; average reward: 11.95 - action: e; reward: 73.79
  739/2000: episode: 443, duration: 0.689s, episode steps: 1, steps per second: 1, episode reward: 73.790, mean reward: 73.790 [73.790, 73.790], mean action: 2.000 [2.000, 2.000], mean observation: 0.389 [0.000, 3.000], loss: 350.155029, mean_absolute_error: 10.306624, mean_q: 14.201934
step 1209; T: 640350; average reward: 11.46 - action: nw; reward: -206.11
  740/2000: episode: 444, duration: 1.207s, episode steps: 1, steps per second: 1, episode reward: -211.107, mean reward: -211.107 [-211.107, -211.107], mean action: 7.000 [7.000, 7.000], mean observation: 0.413 [0.000, 3.000], loss: 488.694763, mean_absolute_error: 10.909334, mean_q: 13.901987
step 1211; T: 645900; average reward: 11.43 - action: ne; reward: -5.00
  741/2000: episode: 445, duration: 1.579s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.213 [0.000, 2.000], loss: 631.014587, mean_absolute_error: 9.629603, mean_q: 11.949175
step 1214; T: 646800; average reward: 11.41 - action: nw; reward: 5.12
  743/2000: episode: 446, duration: 2.075s, episode steps: 2, steps per second: 1, episode reward: -4.881, mean reward: -2.440 [-10.000, 5.119], mean action: 4.500 [2.000, 7.000], mean observation: 0.265 [0.000, 3.000], loss: 859.100281, mean_absolute_error: 11.438871, mean_q: 14.377487
step 1221; T: 648600; average reward: 11.38 - action: nw; reward: -5.00
  747/2000: episode: 447, duration: 4.604s, episode steps: 4, steps per second: 1, episode reward: -35.000, mean reward: -8.750 [-10.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.267 [0.000, 3.000], loss: 747.334229, mean_absolute_error: 10.510243, mean_q: 13.002163
step 1222; T: 653250; average reward: 11.74 - action: sw; reward: 174.85
  748/2000: episode: 448, duration: 0.865s, episode steps: 1, steps per second: 1, episode reward: 174.852, mean reward: 174.852 [174.852, 174.852], mean action: 5.000 [5.000, 5.000], mean observation: 0.235 [0.000, 3.000], loss: 805.701294, mean_absolute_error: 11.516752, mean_q: 14.284803
step 1225; T: 656400; average reward: 11.75 - action: e; reward: 17.45
  750/2000: episode: 449, duration: 2.087s, episode steps: 2, steps per second: 1, episode reward: 7.454, mean reward: 3.727 [-10.000, 17.454], mean action: 5.000 [2.000, 8.000], mean observation: 0.212 [0.000, 4.000], loss: 574.948730, mean_absolute_error: 10.300076, mean_q: 13.198734
step 1227; T: 658650; average reward: 11.76 - action: w; reward: 14.17
  751/2000: episode: 450, duration: 1.377s, episode steps: 1, steps per second: 1, episode reward: 9.174, mean reward: 9.174 [9.174, 9.174], mean action: 6.000 [6.000, 6.000], mean observation: 0.421 [0.000, 3.000], loss: 886.371948, mean_absolute_error: 10.545557, mean_q: 12.889859
step 1228; T: 659250; average reward: 11.79 - action: nw; reward: 25.88
  752/2000: episode: 451, duration: 0.684s, episode steps: 1, steps per second: 1, episode reward: 25.879, mean reward: 25.879 [25.879, 25.879], mean action: 7.000 [7.000, 7.000], mean observation: 0.229 [0.000, 2.000], loss: 1227.144409, mean_absolute_error: 11.744411, mean_q: 15.693668
step 1230; T: 660300; average reward: 11.73 - action: e; reward: -15.48
  753/2000: episode: 452, duration: 1.559s, episode steps: 1, steps per second: 1, episode reward: -20.482, mean reward: -20.482 [-20.482, -20.482], mean action: 2.000 [2.000, 2.000], mean observation: 0.216 [0.000, 3.000], loss: 1029.859497, mean_absolute_error: 11.904608, mean_q: 13.670765
step 1233; T: 661350; average reward: 11.77 - action: w; reward: 30.66
  755/2000: episode: 453, duration: 1.920s, episode steps: 2, steps per second: 1, episode reward: 20.660, mean reward: 10.330 [-10.000, 30.660], mean action: 6.500 [6.000, 7.000], mean observation: 0.395 [0.000, 4.000], loss: 573.634949, mean_absolute_error: 10.154278, mean_q: 13.513572
step 1234; T: 662550; average reward: 11.86 - action: w; reward: 54.25
  756/2000: episode: 454, duration: 0.707s, episode steps: 1, steps per second: 1, episode reward: 54.249, mean reward: 54.249 [54.249, 54.249], mean action: 6.000 [6.000, 6.000], mean observation: 0.269 [0.000, 3.000], loss: 335.323669, mean_absolute_error: 10.046175, mean_q: 12.922006
step 1245; T: 664950; average reward: 11.82 - action: nw; reward: -6.42
  762/2000: episode: 455, duration: 7.596s, episode steps: 6, steps per second: 1, episode reward: -56.418, mean reward: -9.403 [-10.000, -6.418], mean action: 4.667 [1.000, 7.000], mean observation: 0.306 [0.000, 4.000], loss: 794.771729, mean_absolute_error: 10.927128, mean_q: 11.960903
step 1246; T: 666450; average reward: 11.88 - action: n; reward: 35.88
  763/2000: episode: 456, duration: 0.818s, episode steps: 1, steps per second: 1, episode reward: 35.884, mean reward: 35.884 [35.884, 35.884], mean action: 8.000 [8.000, 8.000], mean observation: 0.456 [0.000, 3.000], loss: 535.687256, mean_absolute_error: 10.221039, mean_q: 13.488829
step 1249; T: 669150; average reward: 12.72 - action: nw; reward: 397.43
  765/2000: episode: 457, duration: 2.150s, episode steps: 2, steps per second: 1, episode reward: 387.435, mean reward: 193.717 [-10.000, 397.435], mean action: 4.000 [1.000, 7.000], mean observation: 0.256 [0.000, 3.000], loss: 811.308716, mean_absolute_error: 10.788393, mean_q: 11.151613
step 1253; T: 670500; average reward: 12.68 - action: e; reward: -5.00
  767/2000: episode: 458, duration: 2.737s, episode steps: 2, steps per second: 1, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 4.500 [2.000, 7.000], mean observation: 0.308 [0.000, 3.000], loss: 539.454834, mean_absolute_error: 10.750404, mean_q: 11.673531
step 1257; T: 673650; average reward: 12.67 - action: n; reward: 8.08
  769/2000: episode: 459, duration: 2.702s, episode steps: 2, steps per second: 1, episode reward: -6.921, mean reward: -3.460 [-10.000, 3.079], mean action: 8.000 [8.000, 8.000], mean observation: 0.351 [0.000, 3.000], loss: 579.812744, mean_absolute_error: 10.924748, mean_q: 11.713914
step 1260; T: 674700; average reward: 12.66 - action: n; reward: 6.54
  771/2000: episode: 460, duration: 2.299s, episode steps: 2, steps per second: 1, episode reward: -3.460, mean reward: -1.730 [-10.000, 6.540], mean action: 8.000 [8.000, 8.000], mean observation: 0.312 [0.000, 4.000], loss: 717.343384, mean_absolute_error: 11.231691, mean_q: 12.909461
step 1261; T: 675300; average reward: 12.24 - action: nw; reward: -182.68
  772/2000: episode: 461, duration: 0.681s, episode steps: 1, steps per second: 1, episode reward: -182.676, mean reward: -182.676 [-182.676, -182.676], mean action: 7.000 [7.000, 7.000], mean observation: 0.323 [0.000, 3.000], loss: 1285.709961, mean_absolute_error: 12.356060, mean_q: 13.572388
step 1262; T: 678150; average reward: 12.37 - action: nw; reward: 73.59
  773/2000: episode: 462, duration: 0.789s, episode steps: 1, steps per second: 1, episode reward: 73.587, mean reward: 73.587 [73.587, 73.587], mean action: 7.000 [7.000, 7.000], mean observation: 0.232 [0.000, 2.000], loss: 699.092346, mean_absolute_error: 11.924900, mean_q: 12.763769
step 1265; T: 679350; average reward: 12.23 - action: sw; reward: -49.93
  775/2000: episode: 463, duration: 2.114s, episode steps: 2, steps per second: 1, episode reward: -59.932, mean reward: -29.966 [-49.932, -10.000], mean action: 4.000 [3.000, 5.000], mean observation: 0.288 [0.000, 3.000], loss: 688.628174, mean_absolute_error: 11.277208, mean_q: 11.217905
step 1266; T: 679650; average reward: 12.16 - action: nw; reward: -20.84
  776/2000: episode: 464, duration: 0.671s, episode steps: 1, steps per second: 1, episode reward: -20.840, mean reward: -20.840 [-20.840, -20.840], mean action: 7.000 [7.000, 7.000], mean observation: 0.325 [0.000, 2.000], loss: 524.024475, mean_absolute_error: 10.051329, mean_q: 11.060261
step 1268; T: 682800; average reward: 12.22 - action: nw; reward: 39.06
  777/2000: episode: 465, duration: 1.513s, episode steps: 1, steps per second: 1, episode reward: 34.061, mean reward: 34.061 [34.061, 34.061], mean action: 7.000 [7.000, 7.000], mean observation: 0.360 [0.000, 2.000], loss: 779.427185, mean_absolute_error: 11.369754, mean_q: 16.525177
step 1270; T: 683700; average reward: 12.03 - action: n; reward: -77.45
  778/2000: episode: 466, duration: 1.503s, episode steps: 1, steps per second: 1, episode reward: -82.451, mean reward: -82.451 [-82.451, -82.451], mean action: 8.000 [8.000, 8.000], mean observation: 0.288 [0.000, 3.000], loss: 769.379150, mean_absolute_error: 10.385307, mean_q: 11.002406
step 1273; T: 684450; average reward: 11.94 - action: nw; reward: -30.63
  780/2000: episode: 467, duration: 2.278s, episode steps: 2, steps per second: 1, episode reward: -40.630, mean reward: -20.315 [-30.630, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.408 [0.000, 3.000], loss: 544.449951, mean_absolute_error: 11.369980, mean_q: 11.775536
step 1277; T: 686100; average reward: 12.01 - action: sw; reward: 46.72
  782/2000: episode: 468, duration: 2.750s, episode steps: 2, steps per second: 1, episode reward: 31.716, mean reward: 15.858 [-10.000, 41.716], mean action: 6.000 [5.000, 7.000], mean observation: 0.436 [0.000, 4.000], loss: 422.165894, mean_absolute_error: 11.172297, mean_q: 15.534421
step 1279; T: 686850; average reward: 11.97 - action: sw; reward: -9.11
  783/2000: episode: 469, duration: 1.563s, episode steps: 1, steps per second: 1, episode reward: -14.108, mean reward: -14.108 [-14.108, -14.108], mean action: 5.000 [5.000, 5.000], mean observation: 0.160 [0.000, 2.000], loss: 605.474609, mean_absolute_error: 10.269424, mean_q: 14.687309
step 1286; T: 690150; average reward: 11.97 - action: nw; reward: 12.48
  787/2000: episode: 470, duration: 4.885s, episode steps: 4, steps per second: 1, episode reward: -17.518, mean reward: -4.379 [-10.000, 12.482], mean action: 6.750 [6.000, 7.000], mean observation: 0.231 [0.000, 3.000], loss: 1890.751465, mean_absolute_error: 12.369006, mean_q: 12.573828
step 1290; T: 692700; average reward: 11.89 - action: nw; reward: -22.38
  789/2000: episode: 471, duration: 2.996s, episode steps: 2, steps per second: 1, episode reward: -37.380, mean reward: -18.690 [-27.380, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.339 [0.000, 3.000], loss: 884.174194, mean_absolute_error: 11.235256, mean_q: 13.037424
step 1291; T: 694500; average reward: 11.97 - action: e; reward: 47.50
  790/2000: episode: 472, duration: 0.732s, episode steps: 1, steps per second: 1, episode reward: 47.498, mean reward: 47.498 [47.498, 47.498], mean action: 2.000 [2.000, 2.000], mean observation: 0.213 [0.000, 3.000], loss: 466.696106, mean_absolute_error: 10.683635, mean_q: 13.424953
step 1292; T: 695400; average reward: 11.94 - action: n; reward: -0.63
  791/2000: episode: 473, duration: 0.669s, episode steps: 1, steps per second: 1, episode reward: -0.628, mean reward: -0.628 [-0.628, -0.628], mean action: 8.000 [8.000, 8.000], mean observation: 0.277 [0.000, 4.000], loss: 626.030518, mean_absolute_error: 10.536520, mean_q: 13.323583
step 1295; T: 697050; average reward: 11.89 - action: e; reward: -10.62
  793/2000: episode: 474, duration: 2.076s, episode steps: 2, steps per second: 1, episode reward: -20.619, mean reward: -10.310 [-10.619, -10.000], mean action: 3.500 [2.000, 5.000], mean observation: 0.355 [0.000, 3.000], loss: 577.027161, mean_absolute_error: 10.908390, mean_q: 14.085470
step 1296; T: 698250; average reward: 11.86 - action: nw; reward: -5.00
  794/2000: episode: 475, duration: 0.708s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.328 [0.000, 3.000], loss: 1779.810913, mean_absolute_error: 11.459553, mean_q: 15.149902
step 1298; T: 699750; average reward: 11.87 - action: nw; reward: 18.61
  795/2000: episode: 476, duration: 1.365s, episode steps: 1, steps per second: 1, episode reward: 13.613, mean reward: 13.613 [13.613, 13.613], mean action: 7.000 [7.000, 7.000], mean observation: 0.189 [0.000, 3.000], loss: 592.914246, mean_absolute_error: 11.129692, mean_q: 12.565319
step 1300; T: 701550; average reward: 11.87 - action: n; reward: 10.47
  796/2000: episode: 477, duration: 1.398s, episode steps: 1, steps per second: 1, episode reward: 5.471, mean reward: 5.471 [5.471, 5.471], mean action: 8.000 [8.000, 8.000], mean observation: 0.431 [0.000, 3.000], loss: 645.090698, mean_absolute_error: 11.551097, mean_q: 15.668143
step 1301; T: 702000; average reward: 11.86 - action: nw; reward: 6.68
  797/2000: episode: 478, duration: 0.791s, episode steps: 1, steps per second: 1, episode reward: 6.680, mean reward: 6.680 [6.680, 6.680], mean action: 7.000 [7.000, 7.000], mean observation: 0.325 [0.000, 2.000], loss: 336.360199, mean_absolute_error: 10.533843, mean_q: 13.008343
step 1304; T: 703500; average reward: 11.81 - action: sw; reward: -12.77
  799/2000: episode: 479, duration: 2.088s, episode steps: 2, steps per second: 1, episode reward: -22.773, mean reward: -11.387 [-12.773, -10.000], mean action: 5.500 [5.000, 6.000], mean observation: 0.267 [0.000, 3.000], loss: 362.290405, mean_absolute_error: 11.505194, mean_q: 17.316332
step 1305; T: 705150; average reward: 11.80 - action: nw; reward: 8.57
  800/2000: episode: 480, duration: 0.741s, episode steps: 1, steps per second: 1, episode reward: 8.570, mean reward: 8.570 [8.570, 8.570], mean action: 7.000 [7.000, 7.000], mean observation: 0.187 [0.000, 4.000], loss: 355.256958, mean_absolute_error: 10.807377, mean_q: 14.257254
step 1306; T: 707250; average reward: 11.70 - action: noop; reward: -38.47
  801/2000: episode: 481, duration: 0.787s, episode steps: 1, steps per second: 1, episode reward: -38.469, mean reward: -38.469 [-38.469, -38.469], mean action: 0.000 [0.000, 0.000], mean observation: 0.251 [0.000, 3.000], loss: 886.235168, mean_absolute_error: 11.673603, mean_q: 15.580685
step 1308; T: 707850; average reward: 11.84 - action: nw; reward: 79.08
  802/2000: episode: 482, duration: 1.413s, episode steps: 1, steps per second: 1, episode reward: 74.077, mean reward: 74.077 [74.077, 74.077], mean action: 7.000 [7.000, 7.000], mean observation: 0.328 [0.000, 3.000], loss: 1841.953125, mean_absolute_error: 13.378203, mean_q: 17.260780
step 1320; T: 710700; average reward: 11.87 - action: se; reward: 26.65
  808/2000: episode: 483, duration: 8.291s, episode steps: 6, steps per second: 1, episode reward: -28.349, mean reward: -4.725 [-10.000, 21.651], mean action: 5.500 [2.000, 8.000], mean observation: 0.335 [0.000, 4.000], loss: 1144.396118, mean_absolute_error: 11.306041, mean_q: 13.816322
step 1321; T: 711150; average reward: 11.83 - action: sw; reward: -4.93
  809/2000: episode: 484, duration: 0.726s, episode steps: 1, steps per second: 1, episode reward: -4.929, mean reward: -4.929 [-4.929, -4.929], mean action: 5.000 [5.000, 5.000], mean observation: 0.307 [0.000, 3.000], loss: 225.021698, mean_absolute_error: 10.601847, mean_q: 14.416172
step 1323; T: 714450; average reward: 11.80 - action: sw; reward: -5.00
  810/2000: episode: 485, duration: 1.545s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.227 [0.000, 3.000], loss: 750.859375, mean_absolute_error: 13.197565, mean_q: 14.476375
step 1335; T: 719250; average reward: 11.76 - action: e; reward: -5.00
  816/2000: episode: 486, duration: 9.024s, episode steps: 6, steps per second: 1, episode reward: -60.000, mean reward: -10.000 [-10.000, -10.000], mean action: 3.667 [2.000, 6.000], mean observation: 0.260 [0.000, 5.000], loss: 705.014404, mean_absolute_error: 11.215820, mean_q: 13.907742
step 1337; T: 719850; average reward: 11.73 - action: ne; reward: -5.00
  817/2000: episode: 487, duration: 1.451s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.243 [0.000, 4.000], loss: 205.205002, mean_absolute_error: 9.897636, mean_q: 9.485697
step 1346; T: 721800; average reward: 11.84 - action: nw; reward: 65.21
  822/2000: episode: 488, duration: 6.300s, episode steps: 5, steps per second: 1, episode reward: 25.209, mean reward: 5.042 [-10.000, 65.209], mean action: 4.200 [1.000, 7.000], mean observation: 0.238 [0.000, 4.000], loss: 472.198090, mean_absolute_error: 11.151346, mean_q: 13.204599
step 1347; T: 722400; average reward: 12.08 - action: n; reward: 128.33
  823/2000: episode: 489, duration: 0.802s, episode steps: 1, steps per second: 1, episode reward: 128.326, mean reward: 128.326 [128.326, 128.326], mean action: 8.000 [8.000, 8.000], mean observation: 0.411 [0.000, 3.000], loss: 607.855286, mean_absolute_error: 10.674021, mean_q: 13.040189
step 1351; T: 724950; average reward: 12.09 - action: n; reward: 19.97
  825/2000: episode: 490, duration: 2.787s, episode steps: 2, steps per second: 1, episode reward: 4.974, mean reward: 2.487 [-10.000, 14.974], mean action: 6.500 [5.000, 8.000], mean observation: 0.372 [0.000, 3.000], loss: 508.406860, mean_absolute_error: 10.603985, mean_q: 10.505142
step 1353; T: 727500; average reward: 12.46 - action: n; reward: 192.10
  826/2000: episode: 491, duration: 1.597s, episode steps: 1, steps per second: 1, episode reward: 187.099, mean reward: 187.099 [187.099, 187.099], mean action: 8.000 [8.000, 8.000], mean observation: 0.264 [0.000, 3.000], loss: 1012.944153, mean_absolute_error: 11.566719, mean_q: 11.309723
step 1356; T: 730050; average reward: 12.39 - action: e; reward: -22.07
  828/2000: episode: 492, duration: 2.207s, episode steps: 2, steps per second: 1, episode reward: -32.071, mean reward: -16.036 [-22.071, -10.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.303 [0.000, 3.000], loss: 1615.333984, mean_absolute_error: 10.973877, mean_q: 9.798836
step 1358; T: 730650; average reward: 12.36 - action: n; reward: -0.54
  829/2000: episode: 493, duration: 1.496s, episode steps: 1, steps per second: 1, episode reward: -5.540, mean reward: -5.540 [-5.540, -5.540], mean action: 8.000 [8.000, 8.000], mean observation: 0.451 [0.000, 5.000], loss: 531.917480, mean_absolute_error: 11.163418, mean_q: 7.494762
step 1363; T: 732450; average reward: 12.33 - action: nw; reward: -5.00
  832/2000: episode: 494, duration: 3.505s, episode steps: 3, steps per second: 1, episode reward: -25.000, mean reward: -8.333 [-10.000, -5.000], mean action: 5.000 [1.000, 7.000], mean observation: 0.291 [0.000, 4.000], loss: 1001.231262, mean_absolute_error: 11.121274, mean_q: 11.095413
step 1367; T: 733350; average reward: 12.34 - action: nw; reward: 17.99
  834/2000: episode: 495, duration: 2.915s, episode steps: 2, steps per second: 1, episode reward: 2.989, mean reward: 1.495 [-10.000, 12.989], mean action: 7.500 [7.000, 8.000], mean observation: 0.360 [0.000, 4.000], loss: 410.325073, mean_absolute_error: 10.156734, mean_q: 14.083376
step 1369; T: 734100; average reward: 12.30 - action: sw; reward: -5.00
  835/2000: episode: 496, duration: 1.511s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 1134.567627, mean_absolute_error: 10.949101, mean_q: 12.417015
step 1375; T: 737700; average reward: 12.27 - action: e; reward: -5.00
  838/2000: episode: 497, duration: 4.546s, episode steps: 3, steps per second: 1, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 3.000 [2.000, 5.000], mean observation: 0.264 [0.000, 3.000], loss: 536.171326, mean_absolute_error: 10.793002, mean_q: 11.093867
step 1382; T: 739500; average reward: 12.23 - action: sw; reward: -5.00
  842/2000: episode: 498, duration: 4.991s, episode steps: 4, steps per second: 1, episode reward: -35.000, mean reward: -8.750 [-10.000, -5.000], mean action: 5.250 [5.000, 6.000], mean observation: 0.299 [0.000, 3.000], loss: 650.719360, mean_absolute_error: 11.002978, mean_q: 10.122337
step 1383; T: 741150; average reward: 12.15 - action: sw; reward: -28.15
  843/2000: episode: 499, duration: 0.776s, episode steps: 1, steps per second: 1, episode reward: -28.148, mean reward: -28.148 [-28.148, -28.148], mean action: 5.000 [5.000, 5.000], mean observation: 0.344 [0.000, 2.000], loss: 864.885620, mean_absolute_error: 11.022598, mean_q: 13.309464
step 1384; T: 741900; average reward: 11.95 - action: n; reward: -90.16
  844/2000: episode: 500, duration: 0.756s, episode steps: 1, steps per second: 1, episode reward: -90.164, mean reward: -90.164 [-90.164, -90.164], mean action: 8.000 [8.000, 8.000], mean observation: 0.251 [0.000, 2.000], loss: 705.388977, mean_absolute_error: 11.759594, mean_q: 13.561516
step 1385; T: 743100; average reward: 11.95 - action: w; reward: 14.42
  845/2000: episode: 501, duration: 0.759s, episode steps: 1, steps per second: 1, episode reward: 14.415, mean reward: 14.415 [14.415, 14.415], mean action: 6.000 [6.000, 6.000], mean observation: 0.248 [0.000, 3.000], loss: 261.561462, mean_absolute_error: 10.604647, mean_q: 11.673615
step 1386; T: 744900; average reward: 12.25 - action: ne; reward: 160.49
  846/2000: episode: 502, duration: 0.929s, episode steps: 1, steps per second: 1, episode reward: 160.486, mean reward: 160.486 [160.486, 160.486], mean action: 1.000 [1.000, 1.000], mean observation: 0.235 [0.000, 3.000], loss: 490.170380, mean_absolute_error: 10.422670, mean_q: 11.646128
step 1387; T: 746700; average reward: 12.39 - action: e; reward: 83.75
  847/2000: episode: 503, duration: 0.826s, episode steps: 1, steps per second: 1, episode reward: 83.749, mean reward: 83.749 [83.749, 83.749], mean action: 2.000 [2.000, 2.000], mean observation: 0.224 [0.000, 3.000], loss: 302.057617, mean_absolute_error: 10.630098, mean_q: 12.360628
step 1388; T: 750000; average reward: 12.36 - action: n; reward: -4.89
  848/2000: episode: 504, duration: 0.870s, episode steps: 1, steps per second: 1, episode reward: -4.886, mean reward: -4.886 [-4.886, -4.886], mean action: 8.000 [8.000, 8.000], mean observation: 0.237 [0.000, 3.000], loss: 344.319824, mean_absolute_error: 11.273241, mean_q: 15.507180
step 1389; T: 751800; average reward: 12.66 - action: ne; reward: 166.84
  849/2000: episode: 505, duration: 0.797s, episode steps: 1, steps per second: 1, episode reward: 166.838, mean reward: 166.838 [166.838, 166.838], mean action: 1.000 [1.000, 1.000], mean observation: 0.251 [0.000, 4.000], loss: 1343.614136, mean_absolute_error: 12.763405, mean_q: 12.280799
step 1392; T: 752850; average reward: 12.76 - action: e; reward: 62.26
  851/2000: episode: 506, duration: 2.445s, episode steps: 2, steps per second: 1, episode reward: 52.257, mean reward: 26.129 [-10.000, 62.257], mean action: 3.500 [2.000, 5.000], mean observation: 0.213 [0.000, 3.000], loss: 706.658264, mean_absolute_error: 11.743862, mean_q: 12.250515
step 1393; T: 757050; average reward: 12.68 - action: nw; reward: -27.37
  852/2000: episode: 507, duration: 0.986s, episode steps: 1, steps per second: 1, episode reward: -27.372, mean reward: -27.372 [-27.372, -27.372], mean action: 7.000 [7.000, 7.000], mean observation: 0.291 [0.000, 3.000], loss: 737.040283, mean_absolute_error: 11.106248, mean_q: 11.998884
step 1398; T: 760500; average reward: 12.67 - action: n; reward: 6.47
  855/2000: episode: 508, duration: 4.141s, episode steps: 3, steps per second: 1, episode reward: -13.526, mean reward: -4.509 [-10.000, 6.474], mean action: 6.000 [5.000, 8.000], mean observation: 0.301 [0.000, 3.000], loss: 1119.639526, mean_absolute_error: 11.005630, mean_q: 11.398356
step 1399; T: 760950; average reward: 12.79 - action: w; reward: 75.67
  856/2000: episode: 509, duration: 0.790s, episode steps: 1, steps per second: 1, episode reward: 75.668, mean reward: 75.668 [75.668, 75.668], mean action: 6.000 [6.000, 6.000], mean observation: 0.357 [0.000, 2.000], loss: 1701.694824, mean_absolute_error: 13.254452, mean_q: 15.143076
step 1400; T: 763200; average reward: 12.64 - action: nw; reward: -65.38
  857/2000: episode: 510, duration: 0.824s, episode steps: 1, steps per second: 1, episode reward: -65.379, mean reward: -65.379 [-65.379, -65.379], mean action: 7.000 [7.000, 7.000], mean observation: 0.213 [0.000, 4.000], loss: 1130.042236, mean_absolute_error: 11.943253, mean_q: 9.010969
step 1407; T: 765000; average reward: 12.54 - action: n; reward: -39.77
  861/2000: episode: 511, duration: 5.720s, episode steps: 4, steps per second: 1, episode reward: -69.767, mean reward: -17.442 [-39.767, -10.000], mean action: 5.750 [2.000, 8.000], mean observation: 0.231 [0.000, 3.000], loss: 1205.023804, mean_absolute_error: 11.700090, mean_q: 14.813034
step 1410; T: 766500; average reward: 12.50 - action: sw; reward: -5.00
  863/2000: episode: 512, duration: 2.312s, episode steps: 2, steps per second: 1, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 3.500 [2.000, 5.000], mean observation: 0.385 [0.000, 3.000], loss: 1069.419556, mean_absolute_error: 11.464422, mean_q: 10.774162
step 1411; T: 768000; average reward: 12.64 - action: nw; reward: 81.58
  864/2000: episode: 513, duration: 0.825s, episode steps: 1, steps per second: 1, episode reward: 81.581, mean reward: 81.581 [81.581, 81.581], mean action: 7.000 [7.000, 7.000], mean observation: 0.304 [0.000, 2.000], loss: 526.523315, mean_absolute_error: 11.111900, mean_q: 8.324721
step 1418; T: 770550; average reward: 12.66 - action: n; reward: 22.76
  868/2000: episode: 514, duration: 5.639s, episode steps: 4, steps per second: 1, episode reward: -7.240, mean reward: -1.810 [-10.000, 22.760], mean action: 5.500 [2.000, 8.000], mean observation: 0.323 [0.000, 3.000], loss: 776.343750, mean_absolute_error: 11.210884, mean_q: 13.731795
step 1420; T: 771600; average reward: 12.62 - action: sw; reward: -5.22
  869/2000: episode: 515, duration: 1.395s, episode steps: 1, steps per second: 1, episode reward: -10.217, mean reward: -10.217 [-10.217, -10.217], mean action: 5.000 [5.000, 5.000], mean observation: 0.261 [0.000, 3.000], loss: 668.098694, mean_absolute_error: 10.830826, mean_q: 13.869673
step 1423; T: 772800; average reward: 12.79 - action: e; reward: 100.20
  871/2000: episode: 516, duration: 2.176s, episode steps: 2, steps per second: 1, episode reward: 90.203, mean reward: 45.102 [-10.000, 100.203], mean action: 5.000 [2.000, 8.000], mean observation: 0.201 [0.000, 3.000], loss: 498.726562, mean_absolute_error: 11.519133, mean_q: 13.998690
step 1424; T: 773550; average reward: 12.91 - action: e; reward: 71.58
  872/2000: episode: 517, duration: 0.713s, episode steps: 1, steps per second: 1, episode reward: 71.584, mean reward: 71.584 [71.584, 71.584], mean action: 2.000 [2.000, 2.000], mean observation: 0.448 [0.000, 3.000], loss: 783.452332, mean_absolute_error: 11.269464, mean_q: 18.258511
step 1425; T: 774600; average reward: 12.87 - action: ne; reward: -8.50
  873/2000: episode: 518, duration: 0.721s, episode steps: 1, steps per second: 1, episode reward: -8.503, mean reward: -8.503 [-8.503, -8.503], mean action: 1.000 [1.000, 1.000], mean observation: 0.373 [0.000, 3.000], loss: 238.041153, mean_absolute_error: 10.073427, mean_q: 14.718122
step 1426; T: 775800; average reward: 13.26 - action: nw; reward: 219.58
  874/2000: episode: 519, duration: 0.759s, episode steps: 1, steps per second: 1, episode reward: 219.579, mean reward: 219.579 [219.579, 219.579], mean action: 7.000 [7.000, 7.000], mean observation: 0.277 [0.000, 3.000], loss: 844.116333, mean_absolute_error: 11.468147, mean_q: 12.312355
step 1429; T: 776400; average reward: 13.12 - action: w; reward: -59.89
  876/2000: episode: 520, duration: 2.512s, episode steps: 2, steps per second: 1, episode reward: -69.891, mean reward: -34.945 [-59.891, -10.000], mean action: 4.000 [2.000, 6.000], mean observation: 0.195 [0.000, 3.000], loss: 715.599854, mean_absolute_error: 11.097113, mean_q: 13.984523
step 1433; T: 777300; average reward: 13.09 - action: nw; reward: -5.00
  878/2000: episode: 521, duration: 3.224s, episode steps: 2, steps per second: 1, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.405 [0.000, 3.000], loss: 474.250519, mean_absolute_error: 9.941460, mean_q: 15.110561
step 1436; T: 778800; average reward: 13.42 - action: nw; reward: 187.38
  880/2000: episode: 522, duration: 3.016s, episode steps: 2, steps per second: 1, episode reward: 177.378, mean reward: 88.689 [-10.000, 187.378], mean action: 4.000 [1.000, 7.000], mean observation: 0.293 [0.000, 3.000], loss: 1345.893799, mean_absolute_error: 12.327877, mean_q: 17.954388
step 1439; T: 779550; average reward: 13.56 - action: nw; reward: 87.65
  882/2000: episode: 523, duration: 2.473s, episode steps: 2, steps per second: 1, episode reward: 77.651, mean reward: 38.826 [-10.000, 87.651], mean action: 7.500 [7.000, 8.000], mean observation: 0.447 [0.000, 3.000], loss: 466.848785, mean_absolute_error: 10.567645, mean_q: 13.439087
step 1444; T: 783600; average reward: 13.61 - action: nw; reward: 38.07
  885/2000: episode: 524, duration: 4.222s, episode steps: 3, steps per second: 1, episode reward: 18.071, mean reward: 6.024 [-10.000, 38.071], mean action: 7.333 [7.000, 8.000], mean observation: 0.380 [0.000, 3.000], loss: 518.165710, mean_absolute_error: 10.733609, mean_q: 12.324001
step 1464; T: 787350; average reward: 13.58 - action: w; reward: -5.00
  895/2000: episode: 525, duration: 15.801s, episode steps: 10, steps per second: 1, episode reward: -100.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.279 [0.000, 2.000], loss: 635.517639, mean_absolute_error: 11.724911, mean_q: 15.244990
step 1467; T: 788700; average reward: 13.80 - action: n; reward: 130.93
  897/2000: episode: 526, duration: 2.340s, episode steps: 2, steps per second: 1, episode reward: 120.927, mean reward: 60.463 [-10.000, 130.927], mean action: 7.000 [6.000, 8.000], mean observation: 0.272 [0.000, 3.000], loss: 851.057922, mean_absolute_error: 11.291903, mean_q: 14.409163
step 1468; T: 791250; average reward: 13.76 - action: nw; reward: -5.00
  898/2000: episode: 527, duration: 0.928s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.277 [0.000, 2.000], loss: 521.655762, mean_absolute_error: 11.849283, mean_q: 16.083456
step 1470; T: 792000; average reward: 13.74 - action: noop; reward: 0.00
  899/2000: episode: 528, duration: 1.706s, episode steps: 1, steps per second: 1, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.299 [0.000, 3.000], loss: 856.707275, mean_absolute_error: 13.102722, mean_q: 16.799778
step 1471; T: 793950; average reward: 13.86 - action: sw; reward: 80.17
  900/2000: episode: 529, duration: 0.883s, episode steps: 1, steps per second: 1, episode reward: 80.167, mean reward: 80.167 [80.167, 80.167], mean action: 5.000 [5.000, 5.000], mean observation: 0.347 [0.000, 3.000], loss: 535.050598, mean_absolute_error: 10.867167, mean_q: 8.527384
step 1472; T: 795300; average reward: 13.57 - action: nw; reward: -140.19
  901/2000: episode: 530, duration: 0.872s, episode steps: 1, steps per second: 1, episode reward: -140.190, mean reward: -140.190 [-140.190, -140.190], mean action: 7.000 [7.000, 7.000], mean observation: 0.205 [0.000, 2.000], loss: 512.680725, mean_absolute_error: 12.293909, mean_q: 14.487934
step 1474; T: 798000; average reward: 13.42 - action: n; reward: -65.45
  902/2000: episode: 531, duration: 1.915s, episode steps: 1, steps per second: 1, episode reward: -70.451, mean reward: -70.451 [-70.451, -70.451], mean action: 8.000 [8.000, 8.000], mean observation: 0.339 [0.000, 3.000], loss: 1346.007080, mean_absolute_error: 12.179793, mean_q: 15.668772
step 1476; T: 799050; average reward: 13.51 - action: nw; reward: 58.80
  903/2000: episode: 532, duration: 1.587s, episode steps: 1, steps per second: 1, episode reward: 53.804, mean reward: 53.804 [53.804, 53.804], mean action: 7.000 [7.000, 7.000], mean observation: 0.219 [0.000, 4.000], loss: 277.018005, mean_absolute_error: 11.256831, mean_q: 10.484331
step 1478; T: 799650; average reward: 13.47 - action: w; reward: -5.00
  904/2000: episode: 533, duration: 1.653s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.408 [0.000, 3.000], loss: 343.086395, mean_absolute_error: 11.177944, mean_q: 14.145680
step 1483; T: 800850; average reward: 13.50 - action: ne; reward: 30.05
  907/2000: episode: 534, duration: 3.873s, episode steps: 3, steps per second: 1, episode reward: 10.050, mean reward: 3.350 [-10.000, 30.050], mean action: 4.667 [1.000, 7.000], mean observation: 0.434 [0.000, 5.000], loss: 642.214783, mean_absolute_error: 11.865849, mean_q: 13.640832
step 1485; T: 802050; average reward: 13.56 - action: nw; reward: 41.21
  908/2000: episode: 535, duration: 1.657s, episode steps: 1, steps per second: 1, episode reward: 36.210, mean reward: 36.210 [36.210, 36.210], mean action: 7.000 [7.000, 7.000], mean observation: 0.187 [0.000, 4.000], loss: 906.501831, mean_absolute_error: 13.006775, mean_q: 12.298273
step 1487; T: 803250; average reward: 13.58 - action: w; reward: 27.51
  909/2000: episode: 536, duration: 1.624s, episode steps: 1, steps per second: 1, episode reward: 22.506, mean reward: 22.506 [22.506, 22.506], mean action: 6.000 [6.000, 6.000], mean observation: 0.376 [0.000, 3.000], loss: 307.137665, mean_absolute_error: 11.134983, mean_q: 12.871345
step 1488; T: 805200; average reward: 13.56 - action: ne; reward: -0.44
  910/2000: episode: 537, duration: 0.934s, episode steps: 1, steps per second: 1, episode reward: -0.445, mean reward: -0.445 [-0.445, -0.445], mean action: 1.000 [1.000, 1.000], mean observation: 0.408 [0.000, 4.000], loss: 433.493225, mean_absolute_error: 11.983109, mean_q: 13.844740
step 1490; T: 806400; average reward: 13.55 - action: nw; reward: 11.01
  911/2000: episode: 538, duration: 1.644s, episode steps: 1, steps per second: 1, episode reward: 6.013, mean reward: 6.013 [6.013, 6.013], mean action: 7.000 [7.000, 7.000], mean observation: 0.237 [0.000, 4.000], loss: 1807.574707, mean_absolute_error: 12.910258, mean_q: 13.801432
step 1494; T: 807600; average reward: 13.52 - action: ne; reward: -5.00
  913/2000: episode: 539, duration: 3.270s, episode steps: 2, steps per second: 1, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 4.500 [1.000, 8.000], mean observation: 0.295 [0.000, 4.000], loss: 399.406189, mean_absolute_error: 11.236168, mean_q: 13.160528
step 1495; T: 808950; average reward: 13.54 - action: n; reward: 25.45
  914/2000: episode: 540, duration: 0.858s, episode steps: 1, steps per second: 1, episode reward: 25.446, mean reward: 25.446 [25.446, 25.446], mean action: 8.000 [8.000, 8.000], mean observation: 0.200 [0.000, 3.000], loss: 2920.762695, mean_absolute_error: 13.626873, mean_q: 12.590662
step 1496; T: 810150; average reward: 13.51 - action: n; reward: -5.00
  915/2000: episode: 541, duration: 0.841s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.205 [0.000, 4.000], loss: 2224.500488, mean_absolute_error: 14.368017, mean_q: 17.132660
step 1497; T: 811050; average reward: 13.47 - action: w; reward: -5.00
  916/2000: episode: 542, duration: 0.800s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.216 [0.000, 3.000], loss: 763.476868, mean_absolute_error: 10.999367, mean_q: 11.810586
step 1498; T: 811500; average reward: 13.44 - action: ne; reward: -5.00
  917/2000: episode: 543, duration: 0.765s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.235 [0.000, 4.000], loss: 467.306305, mean_absolute_error: 10.629403, mean_q: 12.425123
step 1499; T: 812550; average reward: 13.34 - action: e; reward: -39.49
  918/2000: episode: 544, duration: 0.934s, episode steps: 1, steps per second: 1, episode reward: -39.492, mean reward: -39.492 [-39.492, -39.492], mean action: 2.000 [2.000, 2.000], mean observation: 0.235 [0.000, 2.000], loss: 148.569382, mean_absolute_error: 10.508294, mean_q: 13.677094
step 1501; T: 814500; average reward: 13.31 - action: n; reward: -5.00
  919/2000: episode: 545, duration: 1.709s, episode steps: 1, steps per second: 1, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.299 [0.000, 3.000], loss: 694.021729, mean_absolute_error: 11.665280, mean_q: 15.031309
step 1503; T: 815850; average reward: 13.19 - action: ne; reward: -49.82
  920/2000: episode: 546, duration: 1.641s, episode steps: 1, steps per second: 1, episode reward: -54.816, mean reward: -54.816 [-54.816, -54.816], mean action: 1.000 [1.000, 1.000], mean observation: 0.192 [0.000, 4.000], loss: 383.457794, mean_absolute_error: 10.453456, mean_q: 14.739867
step 1512; T: 819600; average reward: 13.16 - action: ne; reward: -4.14
  925/2000: episode: 547, duration: 7.192s, episode steps: 5, steps per second: 1, episode reward: -44.139, mean reward: -8.828 [-10.000, -4.139], mean action: 4.400 [1.000, 7.000], mean observation: 0.375 [0.000, 5.000], loss: 1483.989746, mean_absolute_error: 12.545343, mean_q: 16.512243
step 1513; T: 821100; average reward: 13.21 - action: nw; reward: 39.68
  926/2000: episode: 548, duration: 0.884s, episode steps: 1, steps per second: 1, episode reward: 39.684, mean reward: 39.684 [39.684, 39.684], mean action: 7.000 [7.000, 7.000], mean observation: 0.432 [0.000, 3.000], loss: 261.531494, mean_absolute_error: 10.895218, mean_q: 16.784405
step 1514; T: 822000; average reward: 13.47 - action: nw; reward: 160.08
  927/2000: episode: 549, duration: 0.935s, episode steps: 1, steps per second: 1, episode reward: 160.078, mean reward: 160.078 [160.078, 160.078], mean action: 7.000 [7.000, 7.000], mean observation: 0.421 [0.000, 3.000], loss: 1529.994385, mean_absolute_error: 12.739130, mean_q: 15.726891
step 1516; T: 822600; average reward: 13.50 - action: sw; reward: 29.86
  928/2000: episode: 550, duration: 1.547s, episode steps: 1, steps per second: 1, episode reward: 24.864, mean reward: 24.864 [24.864, 24.864], mean action: 5.000 [5.000, 5.000], mean observation: 0.355 [0.000, 2.000], loss: 409.101196, mean_absolute_error: 10.618547, mean_q: 20.489681
step 1518; T: 823050; average reward: 13.41 - action: nw; reward: -37.16
  929/2000: episode: 551, duration: 1.595s, episode steps: 1, steps per second: 1, episode reward: -42.158, mean reward: -42.158 [-42.158, -42.158], mean action: 7.000 [7.000, 7.000], mean observation: 0.317 [0.000, 3.000], loss: 2385.024658, mean_absolute_error: 14.076508, mean_q: 17.297077
step 1523; T: 825450; average reward: 13.43 - action: n; reward: 25.40
  932/2000: episode: 552, duration: 4.348s, episode steps: 3, steps per second: 1, episode reward: 5.400, mean reward: 1.800 [-10.000, 25.400], mean action: 7.333 [6.000, 8.000], mean observation: 0.310 [0.000, 3.000], loss: 529.689697, mean_absolute_error: 11.306735, mean_q: 15.707814
step 1529; T: 828300; average reward: 13.40 - action: w; reward: -5.00
  935/2000: episode: 553, duration: 5.133s, episode steps: 3, steps per second: 1, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.193 [0.000, 3.000], loss: 620.964905, mean_absolute_error: 11.661385, mean_q: 15.259873
step 1531; T: 829350; average reward: 13.36 - action: ne; reward: -6.64
  936/2000: episode: 554, duration: 1.635s, episode steps: 1, steps per second: 1, episode reward: -11.637, mean reward: -11.637 [-11.637, -11.637], mean action: 1.000 [1.000, 1.000], mean observation: 0.285 [0.000, 3.000], loss: 695.773682, mean_absolute_error: 12.819679, mean_q: 14.176933
step 1532; T: 830400; average reward: 13.52 - action: w; reward: 99.94
  937/2000: episode: 555, duration: 0.844s, episode steps: 1, steps per second: 1, episode reward: 99.937, mean reward: 99.937 [99.937, 99.937], mean action: 6.000 [6.000, 6.000], mean observation: 0.243 [0.000, 2.000], loss: 1108.803345, mean_absolute_error: 12.517934, mean_q: 18.247808
step 1539; T: 832200; average reward: 13.41 - action: nw; reward: -48.88
  941/2000: episode: 556, duration: 5.839s, episode steps: 4, steps per second: 1, episode reward: -78.882, mean reward: -19.720 [-48.882, -10.000], mean action: 4.250 [1.000, 8.000], mean observation: 0.349 [0.000, 4.000], loss: 582.678833, mean_absolute_error: 11.345838, mean_q: 12.299020
step 1544; T: 833550; average reward: 13.50 - action: nw; reward: 66.92
  944/2000: episode: 557, duration: 4.165s, episode steps: 3, steps per second: 1, episode reward: 46.918, mean reward: 15.639 [-10.000, 66.918], mean action: 6.333 [6.000, 7.000], mean observation: 0.285 [0.000, 4.000], loss: 706.743164, mean_absolute_error: 11.995580, mean_q: 15.580841
step 1545; T: 834450; average reward: 13.63 - action: w; reward: 80.65
  945/2000: episode: 558, duration: 0.790s, episode steps: 1, steps per second: 1, episode reward: 80.651, mean reward: 80.651 [80.651, 80.651], mean action: 6.000 [6.000, 6.000], mean observation: 0.240 [0.000, 3.000], loss: 250.579193, mean_absolute_error: 11.391040, mean_q: 12.461593
step 1548; T: 836400; average reward: 13.59 - action: n; reward: -5.00
  947/2000: episode: 559, duration: 2.504s, episode steps: 2, steps per second: 1, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.207 [0.000, 3.000], loss: 451.772858, mean_absolute_error: 11.762489, mean_q: 14.682161
step 1554; T: 840000; average reward: 13.56 - action: ne; reward: -4.02
  950/2000: episode: 560, duration: 5.617s, episode steps: 3, steps per second: 1, episode reward: -29.023, mean reward: -9.674 [-10.000, -9.023], mean action: 4.333 [1.000, 6.000], mean observation: 0.270 [0.000, 3.000], loss: 452.876740, mean_absolute_error: 11.673378, mean_q: 14.424226
step 1556; T: 840900; average reward: 13.50 - action: ne; reward: -17.64
  951/2000: episode: 561, duration: 1.776s, episode steps: 1, steps per second: 1, episode reward: -22.642, mean reward: -22.642 [-22.642, -22.642], mean action: 1.000 [1.000, 1.000], mean observation: 0.301 [0.000, 3.000], loss: 1173.173828, mean_absolute_error: 14.412313, mean_q: 16.989149
step 1558; T: 841650; average reward: 13.49 - action: nw; reward: 2.90
  952/2000: episode: 562, duration: 1.663s, episode steps: 1, steps per second: 1, episode reward: -2.098, mean reward: -2.098 [-2.098, -2.098], mean action: 7.000 [7.000, 7.000], mean observation: 0.309 [0.000, 3.000], loss: 690.611206, mean_absolute_error: 10.306662, mean_q: 10.792986
step 1561; T: 842400; average reward: 13.52 - action: nw; reward: 32.80
  954/2000: episode: 563, duration: 2.534s, episode steps: 2, steps per second: 1, episode reward: 22.805, mean reward: 11.402 [-10.000, 32.805], mean action: 7.000 [7.000, 7.000], mean observation: 0.497 [0.000, 4.000], loss: 799.542419, mean_absolute_error: 11.863851, mean_q: 15.739843
step 1562; T: 845100; average reward: 13.56 - action: nw; reward: 37.35
  955/2000: episode: 564, duration: 0.962s, episode steps: 1, steps per second: 1, episode reward: 37.354, mean reward: 37.354 [37.354, 37.354], mean action: 7.000 [7.000, 7.000], mean observation: 0.216 [0.000, 3.000], loss: 1114.762939, mean_absolute_error: 13.093357, mean_q: 14.961246
step 1563; T: 846000; average reward: 13.67 - action: ne; reward: 72.57
  956/2000: episode: 565, duration: 0.901s, episode steps: 1, steps per second: 1, episode reward: 72.573, mean reward: 72.573 [72.573, 72.573], mean action: 1.000 [1.000, 1.000], mean observation: 0.285 [0.000, 3.000], loss: 522.781128, mean_absolute_error: 11.732494, mean_q: 15.239324
step 1564; T: 847050; average reward: 13.66 - action: nw; reward: 9.68
  957/2000: episode: 566, duration: 1.033s, episode steps: 1, steps per second: 1, episode reward: 9.682, mean reward: 9.682 [9.682, 9.682], mean action: 7.000 [7.000, 7.000], mean observation: 0.264 [0.000, 2.000], loss: 1027.883667, mean_absolute_error: 12.534492, mean_q: 15.714140
step 1567; T: 848700; average reward: 13.69 - action: e; reward: 31.43
  959/2000: episode: 567, duration: 2.528s, episode steps: 2, steps per second: 1, episode reward: 21.431, mean reward: 10.716 [-10.000, 31.431], mean action: 5.000 [2.000, 8.000], mean observation: 0.263 [0.000, 3.000], loss: 1240.047852, mean_absolute_error: 12.857491, mean_q: 15.977148
step 1568; T: 849600; average reward: 13.55 - action: nw; reward: -64.54
  960/2000: episode: 568, duration: 0.851s, episode steps: 1, steps per second: 1, episode reward: -64.543, mean reward: -64.543 [-64.543, -64.543], mean action: 7.000 [7.000, 7.000], mean observation: 0.451 [0.000, 3.000], loss: 305.199432, mean_absolute_error: 11.014394, mean_q: 15.215618
step 1570; T: 852450; average reward: 13.52 - action: nw; reward: -2.68
  961/2000: episode: 569, duration: 1.875s, episode steps: 1, steps per second: 1, episode reward: -7.682, mean reward: -7.682 [-7.682, -7.682], mean action: 7.000 [7.000, 7.000], mean observation: 0.237 [0.000, 2.000], loss: 1271.012695, mean_absolute_error: 12.979029, mean_q: 13.712050
step 1574; T: 854100; average reward: 13.39 - action: nw; reward: -61.38
  963/2000: episode: 570, duration: 3.504s, episode steps: 2, steps per second: 1, episode reward: -76.385, mean reward: -38.192 [-66.385, -10.000], mean action: 4.500 [2.000, 7.000], mean observation: 0.351 [0.000, 3.000], loss: 779.435364, mean_absolute_error: 12.335596, mean_q: 14.064888
step 1577; T: 857250; average reward: 13.36 - action: ne; reward: -5.00
  965/2000: episode: 571, duration: 2.856s, episode steps: 2, steps per second: 1, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 4.000 [1.000, 7.000], mean observation: 0.195 [0.000, 3.000], loss: 757.818604, mean_absolute_error: 11.380459, mean_q: 12.355194
step 1578; T: 858300; average reward: 13.44 - action: n; reward: 57.52
  966/2000: episode: 572, duration: 0.946s, episode steps: 1, steps per second: 1, episode reward: 57.515, mean reward: 57.515 [57.515, 57.515], mean action: 8.000 [8.000, 8.000], mean observation: 0.376 [0.000, 3.000], loss: 183.426651, mean_absolute_error: 11.060652, mean_q: 11.918024
step 1581; T: 860700; average reward: 13.70 - action: nw; reward: 164.34
  968/2000: episode: 573, duration: 2.839s, episode steps: 2, steps per second: 1, episode reward: 154.339, mean reward: 77.169 [-10.000, 164.339], mean action: 7.000 [7.000, 7.000], mean observation: 0.375 [0.000, 4.000], loss: 861.997070, mean_absolute_error: 12.260632, mean_q: 13.340076
step 1582; T: 861900; average reward: 13.73 - action: ne; reward: 32.12
  969/2000: episode: 574, duration: 0.888s, episode steps: 1, steps per second: 1, episode reward: 32.116, mean reward: 32.116 [32.116, 32.116], mean action: 1.000 [1.000, 1.000], mean observation: 0.224 [0.000, 3.000], loss: 1227.874146, mean_absolute_error: 12.772478, mean_q: 15.101133
step 1583; T: 862350; average reward: 13.72 - action: nw; reward: 8.47
  970/2000: episode: 575, duration: 0.983s, episode steps: 1, steps per second: 1, episode reward: 8.467, mean reward: 8.467 [8.467, 8.467], mean action: 7.000 [7.000, 7.000], mean observation: 0.221 [0.000, 3.000], loss: 414.624084, mean_absolute_error: 11.202536, mean_q: 14.802259
step 1590; T: 864600; average reward: 13.92 - action: nw; reward: 123.58
  974/2000: episode: 576, duration: 5.927s, episode steps: 4, steps per second: 1, episode reward: 93.579, mean reward: 23.395 [-10.000, 123.579], mean action: 5.250 [1.000, 7.000], mean observation: 0.360 [0.000, 3.000], loss: 1798.720093, mean_absolute_error: 13.138317, mean_q: 17.160606
step 1598; T: 867000; average reward: 13.88 - action: n; reward: -5.00
  978/2000: episode: 577, duration: 7.567s, episode steps: 4, steps per second: 1, episode reward: -40.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.250 [2.000, 8.000], mean observation: 0.319 [0.000, 4.000], loss: 469.288696, mean_absolute_error: 11.403961, mean_q: 14.630074
step 1599; T: 867300; average reward: 13.93 - action: sw; reward: 38.71
  979/2000: episode: 578, duration: 0.985s, episode steps: 1, steps per second: 1, episode reward: 38.709, mean reward: 38.709 [38.709, 38.709], mean action: 5.000 [5.000, 5.000], mean observation: 0.392 [0.000, 3.000], loss: 621.860107, mean_absolute_error: 11.686952, mean_q: 12.880504
step 1601; T: 868950; average reward: 13.88 - action: nw; reward: -10.59
  980/2000: episode: 579, duration: 1.931s, episode steps: 1, steps per second: 1, episode reward: -15.592, mean reward: -15.592 [-15.592, -15.592], mean action: 7.000 [7.000, 7.000], mean observation: 0.283 [0.000, 3.000], loss: 411.539551, mean_absolute_error: 12.139490, mean_q: 17.157339
step 1604; T: 870900; average reward: 13.79 - action: n; reward: -37.67
  982/2000: episode: 580, duration: 2.904s, episode steps: 2, steps per second: 1, episode reward: -47.671, mean reward: -23.836 [-37.671, -10.000], mean action: 7.000 [6.000, 8.000], mean observation: 0.243 [0.000, 4.000], loss: 323.431854, mean_absolute_error: 10.785658, mean_q: 14.263166
step 1613; T: 873000; average reward: 13.84 - action: nw; reward: 39.63
  987/2000: episode: 581, duration: 8.757s, episode steps: 5, steps per second: 1, episode reward: -0.368, mean reward: -0.074 [-10.000, 39.632], mean action: 6.200 [4.000, 8.000], mean observation: 0.317 [0.000, 5.000], loss: 703.719727, mean_absolute_error: 11.993331, mean_q: 15.533264
step 1615; T: 876450; average reward: 13.67 - action: nw; reward: -81.77
  988/2000: episode: 582, duration: 2.058s, episode steps: 1, steps per second: 0, episode reward: -86.772, mean reward: -86.772 [-86.772, -86.772], mean action: 7.000 [7.000, 7.000], mean observation: 0.157 [0.000, 4.000], loss: 744.970947, mean_absolute_error: 12.526842, mean_q: 16.858639
step 1626; T: 878850; average reward: 13.66 - action: nw; reward: 7.07
  994/2000: episode: 583, duration: 10.295s, episode steps: 6, steps per second: 1, episode reward: -42.930, mean reward: -7.155 [-10.000, 7.070], mean action: 5.000 [1.000, 7.000], mean observation: 0.246 [0.000, 4.000], loss: 638.558105, mean_absolute_error: 12.288217, mean_q: 14.421182
step 1629; T: 880950; average reward: 13.80 - action: nw; reward: 93.65
  996/2000: episode: 584, duration: 2.917s, episode steps: 2, steps per second: 1, episode reward: 83.653, mean reward: 41.826 [-10.000, 93.653], mean action: 4.500 [2.000, 7.000], mean observation: 0.271 [0.000, 3.000], loss: 1123.106934, mean_absolute_error: 13.517597, mean_q: 16.990177
step 1630; T: 882900; average reward: 13.80 - action: nw; reward: 14.48
  997/2000: episode: 585, duration: 1.219s, episode steps: 1, steps per second: 1, episode reward: 14.480, mean reward: 14.480 [14.480, 14.480], mean action: 7.000 [7.000, 7.000], mean observation: 0.328 [0.000, 2.000], loss: 2081.903564, mean_absolute_error: 14.256740, mean_q: 15.399704
step 1635; T: 886650; average reward: 13.82 - action: ne; reward: 22.86
 1000/2000: episode: 586, duration: 5.225s, episode steps: 3, steps per second: 1, episode reward: 2.857, mean reward: 0.952 [-10.000, 22.857], mean action: 4.667 [1.000, 7.000], mean observation: 0.408 [0.000, 5.000], loss: 542.844055, mean_absolute_error: 11.781369, mean_q: 15.832210
step 1639; T: 888150; average reward: 13.88 - action: n; reward: 51.97
 1002/2000: episode: 587, duration: 3.713s, episode steps: 2, steps per second: 1, episode reward: 36.965, mean reward: 18.483 [-10.000, 46.965], mean action: 6.500 [5.000, 8.000], mean observation: 0.295 [0.000, 3.000], loss: 611.787781, mean_absolute_error: 12.493907, mean_q: 13.133865
step 1641; T: 888900; average reward: 13.85 - action: w; reward: -2.73
 1003/2000: episode: 588, duration: 2.065s, episode steps: 1, steps per second: 0, episode reward: -7.730, mean reward: -7.730 [-7.730, -7.730], mean action: 6.000 [6.000, 6.000], mean observation: 0.221 [0.000, 2.000], loss: 776.469910, mean_absolute_error: 12.596979, mean_q: 16.800278
step 1644; T: 890100; average reward: 13.85 - action: nw; reward: 10.63
 1005/2000: episode: 589, duration: 3.008s, episode steps: 2, steps per second: 1, episode reward: 0.625, mean reward: 0.313 [-10.000, 10.625], mean action: 7.000 [7.000, 7.000], mean observation: 0.379 [0.000, 4.000], loss: 1085.601929, mean_absolute_error: 12.221064, mean_q: 14.684333
step 1645; T: 890700; average reward: 13.83 - action: ne; reward: 2.59
 1006/2000: episode: 590, duration: 0.941s, episode steps: 1, steps per second: 1, episode reward: 2.594, mean reward: 2.594 [2.594, 2.594], mean action: 1.000 [1.000, 1.000], mean observation: 0.451 [0.000, 3.000], loss: 1235.456299, mean_absolute_error: 12.705585, mean_q: 13.726284
step 1647; T: 892950; average reward: 13.86 - action: nw; reward: 32.24
 1007/2000: episode: 591, duration: 2.168s, episode steps: 1, steps per second: 0, episode reward: 27.240, mean reward: 27.240 [27.240, 27.240], mean action: 7.000 [7.000, 7.000], mean observation: 0.251 [0.000, 3.000], loss: 587.460266, mean_absolute_error: 11.589659, mean_q: 17.087917
step 1649; T: 893700; average reward: 13.74 - action: nw; reward: -59.70
 1008/2000: episode: 592, duration: 1.880s, episode steps: 1, steps per second: 1, episode reward: -64.702, mean reward: -64.702 [-64.702, -64.702], mean action: 7.000 [7.000, 7.000], mean observation: 0.384 [0.000, 3.000], loss: 571.320679, mean_absolute_error: 12.354571, mean_q: 16.234348
step 1655; T: 895950; average reward: 13.76 - action: noop; reward: 29.45
 1011/2000: episode: 593, duration: 5.544s, episode steps: 3, steps per second: 1, episode reward: 9.451, mean reward: 3.150 [-10.000, 29.451], mean action: 4.000 [0.000, 7.000], mean observation: 0.227 [0.000, 4.000], loss: 1842.455078, mean_absolute_error: 13.160933, mean_q: 15.804997
step 1663; T: 898950; average reward: 13.96 - action: nw; reward: 130.69
 1015/2000: episode: 594, duration: 8.390s, episode steps: 4, steps per second: 0, episode reward: 95.693, mean reward: 23.923 [-10.000, 125.693], mean action: 5.000 [1.000, 7.000], mean observation: 0.229 [0.000, 3.000], loss: 551.672302, mean_absolute_error: 12.054744, mean_q: 16.483404
step 1665; T: 902100; average reward: 14.07 - action: w; reward: 80.53
 1016/2000: episode: 595, duration: 2.037s, episode steps: 1, steps per second: 0, episode reward: 75.528, mean reward: 75.528 [75.528, 75.528], mean action: 6.000 [6.000, 6.000], mean observation: 0.269 [0.000, 4.000], loss: 1064.576660, mean_absolute_error: 12.562470, mean_q: 13.891804
step 1667; T: 903600; average reward: 13.86 - action: sw; reward: -111.85
 1017/2000: episode: 596, duration: 1.952s, episode steps: 1, steps per second: 1, episode reward: -116.851, mean reward: -116.851 [-116.851, -116.851], mean action: 5.000 [5.000, 5.000], mean observation: 0.307 [0.000, 4.000], loss: 558.117249, mean_absolute_error: 13.053831, mean_q: 19.399792
step 1668; T: 904500; average reward: 13.86 - action: nw; reward: 13.70
 1018/2000: episode: 597, duration: 1.000s, episode steps: 1, steps per second: 1, episode reward: 13.702, mean reward: 13.702 [13.702, 13.702], mean action: 7.000 [7.000, 7.000], mean observation: 0.280 [0.000, 3.000], loss: 1200.418823, mean_absolute_error: 13.051578, mean_q: 15.680166
step 1669; T: 905550; average reward: 13.83 - action: ne; reward: -5.00
 1019/2000: episode: 598, duration: 1.077s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.264 [0.000, 4.000], loss: 1091.633545, mean_absolute_error: 13.526333, mean_q: 19.116966
step 1671; T: 907350; average reward: 13.86 - action: e; reward: 33.29
 1020/2000: episode: 599, duration: 2.634s, episode steps: 1, steps per second: 0, episode reward: 28.294, mean reward: 28.294 [28.294, 28.294], mean action: 2.000 [2.000, 2.000], mean observation: 0.253 [0.000, 2.000], loss: 572.158264, mean_absolute_error: 11.376982, mean_q: 17.711941
step 1672; T: 908700; average reward: 13.67 - action: sw; reward: -97.64
 1021/2000: episode: 600, duration: 1.056s, episode steps: 1, steps per second: 1, episode reward: -97.645, mean reward: -97.645 [-97.645, -97.645], mean action: 5.000 [5.000, 5.000], mean observation: 0.368 [0.000, 3.000], loss: 2672.021729, mean_absolute_error: 13.107840, mean_q: 15.587460
step 1675; T: 909900; average reward: 13.79 - action: nw; reward: 83.63
 1023/2000: episode: 601, duration: 2.852s, episode steps: 2, steps per second: 1, episode reward: 73.625, mean reward: 36.813 [-10.000, 83.625], mean action: 5.000 [3.000, 7.000], mean observation: 0.372 [0.000, 4.000], loss: 475.271545, mean_absolute_error: 11.766414, mean_q: 15.637178
step 1676; T: 910500; average reward: 13.76 - action: ne; reward: -5.00
 1024/2000: episode: 602, duration: 1.057s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.205 [0.000, 2.000], loss: 497.770386, mean_absolute_error: 12.860047, mean_q: 15.127347
step 1679; T: 914100; average reward: 13.78 - action: nw; reward: 24.22
 1026/2000: episode: 603, duration: 3.007s, episode steps: 2, steps per second: 1, episode reward: 14.218, mean reward: 7.109 [-10.000, 24.218], mean action: 4.500 [2.000, 7.000], mean observation: 0.232 [0.000, 3.000], loss: 594.943237, mean_absolute_error: 12.626345, mean_q: 16.966902
step 1680; T: 915150; average reward: 13.77 - action: e; reward: 7.75
 1027/2000: episode: 604, duration: 1.125s, episode steps: 1, steps per second: 1, episode reward: 7.750, mean reward: 7.750 [7.750, 7.750], mean action: 2.000 [2.000, 2.000], mean observation: 0.421 [0.000, 3.000], loss: 777.627808, mean_absolute_error: 12.290115, mean_q: 13.823099
step 1681; T: 916350; average reward: 13.78 - action: s; reward: 21.59
 1028/2000: episode: 605, duration: 0.991s, episode steps: 1, steps per second: 1, episode reward: 21.585, mean reward: 21.585 [21.585, 21.585], mean action: 4.000 [4.000, 4.000], mean observation: 0.229 [0.000, 3.000], loss: 2494.779785, mean_absolute_error: 13.429849, mean_q: 18.472507
step 1682; T: 917100; average reward: 13.75 - action: w; reward: -5.00
 1029/2000: episode: 606, duration: 0.954s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.397 [0.000, 3.000], loss: 3101.021484, mean_absolute_error: 15.311576, mean_q: 19.433168
step 1683; T: 917400; average reward: 13.73 - action: sw; reward: 3.33
 1030/2000: episode: 607, duration: 0.911s, episode steps: 1, steps per second: 1, episode reward: 3.334, mean reward: 3.334 [3.334, 3.334], mean action: 5.000 [5.000, 5.000], mean observation: 0.277 [0.000, 2.000], loss: 789.621948, mean_absolute_error: 13.597920, mean_q: 18.233177
step 1684; T: 918450; average reward: 13.78 - action: w; reward: 39.97
 1031/2000: episode: 608, duration: 0.995s, episode steps: 1, steps per second: 1, episode reward: 39.973, mean reward: 39.973 [39.973, 39.973], mean action: 6.000 [6.000, 6.000], mean observation: 0.285 [0.000, 3.000], loss: 1374.425781, mean_absolute_error: 12.800261, mean_q: 18.307129
step 1688; T: 920250; average reward: 13.74 - action: ne; reward: -5.00
 1033/2000: episode: 609, duration: 4.170s, episode steps: 2, steps per second: 0, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.287 [0.000, 3.000], loss: 820.904541, mean_absolute_error: 13.584003, mean_q: 20.370199
step 1691; T: 921600; average reward: 13.63 - action: e; reward: -56.72
 1035/2000: episode: 610, duration: 3.107s, episode steps: 2, steps per second: 1, episode reward: -66.724, mean reward: -33.362 [-56.724, -10.000], mean action: 4.500 [2.000, 7.000], mean observation: 0.249 [0.000, 3.000], loss: 815.725891, mean_absolute_error: 12.666921, mean_q: 15.007230
step 1693; T: 925050; average reward: 13.69 - action: ne; reward: 52.11
 1036/2000: episode: 611, duration: 2.529s, episode steps: 1, steps per second: 0, episode reward: 47.107, mean reward: 47.107 [47.107, 47.107], mean action: 1.000 [1.000, 1.000], mean observation: 0.328 [0.000, 2.000], loss: 1668.186157, mean_absolute_error: 13.954122, mean_q: 13.778124
step 1694; T: 926550; average reward: 13.68 - action: nw; reward: 6.60
 1037/2000: episode: 612, duration: 1.181s, episode steps: 1, steps per second: 1, episode reward: 6.604, mean reward: 6.604 [6.604, 6.604], mean action: 7.000 [7.000, 7.000], mean observation: 0.277 [0.000, 3.000], loss: 1000.136292, mean_absolute_error: 13.941942, mean_q: 18.185123
step 1695; T: 927900; average reward: 13.71 - action: nw; reward: 31.25
 1038/2000: episode: 613, duration: 1.154s, episode steps: 1, steps per second: 1, episode reward: 31.254, mean reward: 31.254 [31.254, 31.254], mean action: 7.000 [7.000, 7.000], mean observation: 0.403 [0.000, 3.000], loss: 776.496033, mean_absolute_error: 11.790270, mean_q: 17.651255
step 1696; T: 928800; average reward: 13.66 - action: w; reward: -14.08
 1039/2000: episode: 614, duration: 1.082s, episode steps: 1, steps per second: 1, episode reward: -14.082, mean reward: -14.082 [-14.082, -14.082], mean action: 6.000 [6.000, 6.000], mean observation: 0.232 [0.000, 3.000], loss: 759.478027, mean_absolute_error: 12.563168, mean_q: 17.796080
step 1697; T: 930300; average reward: 13.57 - action: w; reward: -46.98
 1040/2000: episode: 615, duration: 1.040s, episode steps: 1, steps per second: 1, episode reward: -46.979, mean reward: -46.979 [-46.979, -46.979], mean action: 6.000 [6.000, 6.000], mean observation: 0.371 [0.000, 3.000], loss: 712.610352, mean_absolute_error: 12.359663, mean_q: 16.131330
step 1709; T: 933600; average reward: 13.53 - action: n; reward: -5.00
 1046/2000: episode: 616, duration: 12.559s, episode steps: 6, steps per second: 0, episode reward: -60.000, mean reward: -10.000 [-10.000, -10.000], mean action: 3.167 [1.000, 8.000], mean observation: 0.248 [0.000, 3.000], loss: 817.876465, mean_absolute_error: 11.657886, mean_q: 15.740878
step 1710; T: 937200; average reward: 13.56 - action: nw; reward: 28.82
 1047/2000: episode: 617, duration: 1.223s, episode steps: 1, steps per second: 1, episode reward: 28.820, mean reward: 28.820 [28.820, 28.820], mean action: 7.000 [7.000, 7.000], mean observation: 0.245 [0.000, 4.000], loss: 334.990570, mean_absolute_error: 10.310116, mean_q: 14.174077
step 1711; T: 938550; average reward: 13.62 - action: ne; reward: 51.34
 1048/2000: episode: 618, duration: 1.223s, episode steps: 1, steps per second: 1, episode reward: 51.342, mean reward: 51.342 [51.342, 51.342], mean action: 1.000 [1.000, 1.000], mean observation: 0.213 [0.000, 3.000], loss: 781.545044, mean_absolute_error: 12.320402, mean_q: 15.206825
step 1712; T: 939450; average reward: 13.65 - action: w; reward: 29.30
 1049/2000: episode: 619, duration: 1.079s, episode steps: 1, steps per second: 1, episode reward: 29.298, mean reward: 29.298 [29.298, 29.298], mean action: 6.000 [6.000, 6.000], mean observation: 0.336 [0.000, 3.000], loss: 622.264709, mean_absolute_error: 11.667059, mean_q: 10.641282
step 1715; T: 940500; average reward: 13.66 - action: e; reward: 23.02
 1051/2000: episode: 620, duration: 3.196s, episode steps: 2, steps per second: 1, episode reward: 13.021, mean reward: 6.510 [-10.000, 23.021], mean action: 5.000 [2.000, 8.000], mean observation: 0.251 [0.000, 5.000], loss: 759.272949, mean_absolute_error: 11.737651, mean_q: 15.499386
step 1717; T: 943500; average reward: 13.61 - action: nw; reward: -18.68
 1052/2000: episode: 621, duration: 2.118s, episode steps: 1, steps per second: 0, episode reward: -23.679, mean reward: -23.679 [-23.679, -23.679], mean action: 7.000 [7.000, 7.000], mean observation: 0.245 [0.000, 2.000], loss: 1547.363037, mean_absolute_error: 13.717318, mean_q: 17.656199
step 1719; T: 944400; average reward: 13.58 - action: w; reward: -5.00
 1053/2000: episode: 622, duration: 2.097s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.365 [0.000, 3.000], loss: 1309.663940, mean_absolute_error: 11.672153, mean_q: 14.149927
step 1720; T: 945600; average reward: 13.46 - action: ne; reward: -61.99
 1054/2000: episode: 623, duration: 1.171s, episode steps: 1, steps per second: 1, episode reward: -61.994, mean reward: -61.994 [-61.994, -61.994], mean action: 1.000 [1.000, 1.000], mean observation: 0.173 [0.000, 2.000], loss: 1098.491821, mean_absolute_error: 12.607324, mean_q: 13.677988
step 1723; T: 947850; average reward: 13.43 - action: nw; reward: -5.00
 1056/2000: episode: 624, duration: 3.045s, episode steps: 2, steps per second: 1, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 6.500 [6.000, 7.000], mean observation: 0.232 [0.000, 3.000], loss: 373.761078, mean_absolute_error: 11.737122, mean_q: 12.824934
step 1725; T: 949350; average reward: 13.40 - action: w; reward: -5.00
 1057/2000: episode: 625, duration: 2.404s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.213 [0.000, 3.000], loss: 2085.298584, mean_absolute_error: 13.384206, mean_q: 13.909868
step 1726; T: 953550; average reward: 13.40 - action: w; reward: 11.65
 1058/2000: episode: 626, duration: 1.367s, episode steps: 1, steps per second: 1, episode reward: 11.646, mean reward: 11.646 [11.646, 11.646], mean action: 6.000 [6.000, 6.000], mean observation: 0.469 [0.000, 3.000], loss: 220.364319, mean_absolute_error: 9.923025, mean_q: 12.570726
step 1728; T: 954150; average reward: 13.38 - action: nw; reward: 3.89
 1059/2000: episode: 627, duration: 2.094s, episode steps: 1, steps per second: 0, episode reward: -1.113, mean reward: -1.113 [-1.113, -1.113], mean action: 7.000 [7.000, 7.000], mean observation: 0.432 [0.000, 4.000], loss: 244.217529, mean_absolute_error: 11.575872, mean_q: 13.405764
step 1729; T: 955200; average reward: 13.35 - action: n; reward: -3.06
 1060/2000: episode: 628, duration: 1.047s, episode steps: 1, steps per second: 1, episode reward: -3.061, mean reward: -3.061 [-3.061, -3.061], mean action: 8.000 [8.000, 8.000], mean observation: 0.184 [0.000, 4.000], loss: 476.648926, mean_absolute_error: 13.155140, mean_q: 14.979143
step 1730; T: 956700; average reward: 13.43 - action: w; reward: 62.00
 1061/2000: episode: 629, duration: 1.090s, episode steps: 1, steps per second: 1, episode reward: 62.003, mean reward: 62.003 [62.003, 62.003], mean action: 6.000 [6.000, 6.000], mean observation: 0.277 [0.000, 3.000], loss: 649.718628, mean_absolute_error: 12.685010, mean_q: 16.364555
step 1741; T: 958950; average reward: 13.39 - action: n; reward: -12.48
 1067/2000: episode: 630, duration: 11.406s, episode steps: 6, steps per second: 1, episode reward: -62.485, mean reward: -10.414 [-12.485, -10.000], mean action: 7.167 [6.000, 8.000], mean observation: 0.318 [0.000, 4.000], loss: 810.614746, mean_absolute_error: 12.375480, mean_q: 13.495831
step 1742; T: 960300; average reward: 13.50 - action: nw; reward: 82.16
 1068/2000: episode: 631, duration: 1.188s, episode steps: 1, steps per second: 1, episode reward: 82.163, mean reward: 82.163 [82.163, 82.163], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 3.000], loss: 618.048706, mean_absolute_error: 11.830990, mean_q: 11.738132
step 1746; T: 961650; average reward: 13.47 - action: n; reward: -5.00
 1070/2000: episode: 632, duration: 4.908s, episode steps: 2, steps per second: 0, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.241 [0.000, 4.000], loss: 566.365967, mean_absolute_error: 12.126213, mean_q: 12.890106
step 1749; T: 964950; average reward: 13.61 - action: n; reward: 100.14
 1072/2000: episode: 633, duration: 3.525s, episode steps: 2, steps per second: 1, episode reward: 90.143, mean reward: 45.071 [-10.000, 100.143], mean action: 5.000 [2.000, 8.000], mean observation: 0.291 [0.000, 3.000], loss: 684.826660, mean_absolute_error: 12.121905, mean_q: 13.738897
step 1751; T: 966750; average reward: 13.58 - action: ne; reward: -6.64
 1073/2000: episode: 634, duration: 2.118s, episode steps: 1, steps per second: 0, episode reward: -11.636, mean reward: -11.636 [-11.636, -11.636], mean action: 1.000 [1.000, 1.000], mean observation: 0.349 [0.000, 4.000], loss: 2617.522949, mean_absolute_error: 12.686332, mean_q: 15.947087
step 1752; T: 968100; average reward: 13.58 - action: nw; reward: 19.37
 1074/2000: episode: 635, duration: 1.069s, episode steps: 1, steps per second: 1, episode reward: 19.369, mean reward: 19.369 [19.369, 19.369], mean action: 7.000 [7.000, 7.000], mean observation: 0.331 [0.000, 4.000], loss: 2569.245361, mean_absolute_error: 12.753628, mean_q: 18.064270
step 1756; T: 969000; average reward: 13.56 - action: noop; reward: 0.00
 1076/2000: episode: 636, duration: 4.112s, episode steps: 2, steps per second: 0, episode reward: -10.000, mean reward: -5.000 [-10.000, 0.000], mean action: 3.500 [0.000, 7.000], mean observation: 0.364 [0.000, 3.000], loss: 1141.503662, mean_absolute_error: 12.548390, mean_q: 15.838060
step 1758; T: 969600; average reward: 13.53 - action: se; reward: -5.00
 1077/2000: episode: 637, duration: 2.196s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.432 [0.000, 3.000], loss: 1204.077637, mean_absolute_error: 14.339149, mean_q: 19.783550
step 1759; T: 970950; average reward: 13.51 - action: nw; reward: -1.05
 1078/2000: episode: 638, duration: 1.042s, episode steps: 1, steps per second: 1, episode reward: -1.049, mean reward: -1.049 [-1.049, -1.049], mean action: 7.000 [7.000, 7.000], mean observation: 0.320 [0.000, 3.000], loss: 427.514679, mean_absolute_error: 11.596893, mean_q: 17.428102
step 1763; T: 971850; average reward: 13.46 - action: w; reward: -16.91
 1080/2000: episode: 639, duration: 4.052s, episode steps: 2, steps per second: 0, episode reward: -31.909, mean reward: -15.955 [-21.909, -10.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.284 [0.000, 3.000], loss: 675.401917, mean_absolute_error: 12.121883, mean_q: 15.572245
step 1765; T: 972900; average reward: 13.49 - action: e; reward: 30.72
 1081/2000: episode: 640, duration: 1.964s, episode steps: 1, steps per second: 1, episode reward: 25.721, mean reward: 25.721 [25.721, 25.721], mean action: 2.000 [2.000, 2.000], mean observation: 0.285 [0.000, 3.000], loss: 450.517273, mean_absolute_error: 11.076920, mean_q: 14.543017
step 1766; T: 973500; average reward: 13.52 - action: n; reward: 32.01
 1082/2000: episode: 641, duration: 0.993s, episode steps: 1, steps per second: 1, episode reward: 32.008, mean reward: 32.008 [32.008, 32.008], mean action: 8.000 [8.000, 8.000], mean observation: 0.277 [0.000, 3.000], loss: 1050.808105, mean_absolute_error: 11.615363, mean_q: 16.618217
step 1768; T: 977250; average reward: 13.49 - action: nw; reward: -7.33
 1083/2000: episode: 642, duration: 2.264s, episode steps: 1, steps per second: 0, episode reward: -12.326, mean reward: -12.326 [-12.326, -12.326], mean action: 7.000 [7.000, 7.000], mean observation: 0.208 [0.000, 2.000], loss: 719.588989, mean_absolute_error: 12.178692, mean_q: 14.368138
step 1773; T: 978450; average reward: 13.46 - action: nw; reward: -5.00
 1086/2000: episode: 643, duration: 5.198s, episode steps: 3, steps per second: 1, episode reward: -25.000, mean reward: -8.333 [-10.000, -5.000], mean action: 6.667 [6.000, 7.000], mean observation: 0.263 [0.000, 3.000], loss: 549.575195, mean_absolute_error: 11.127513, mean_q: 17.261669
step 1775; T: 980850; average reward: 13.37 - action: nw; reward: -45.71
 1087/2000: episode: 644, duration: 2.326s, episode steps: 1, steps per second: 0, episode reward: -50.708, mean reward: -50.708 [-50.708, -50.708], mean action: 7.000 [7.000, 7.000], mean observation: 0.133 [0.000, 2.000], loss: 1041.764404, mean_absolute_error: 11.623979, mean_q: 15.096420
step 1779; T: 982200; average reward: 13.34 - action: nw; reward: -5.00
 1089/2000: episode: 645, duration: 4.083s, episode steps: 2, steps per second: 0, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.500 [6.000, 7.000], mean observation: 0.269 [0.000, 4.000], loss: 894.908997, mean_absolute_error: 12.599472, mean_q: 16.672945
step 1780; T: 983550; average reward: 13.28 - action: e; reward: -26.21
 1090/2000: episode: 646, duration: 1.022s, episode steps: 1, steps per second: 1, episode reward: -26.213, mean reward: -26.213 [-26.213, -26.213], mean action: 2.000 [2.000, 2.000], mean observation: 0.184 [0.000, 2.000], loss: 428.650116, mean_absolute_error: 12.212755, mean_q: 14.970558
step 1793; T: 988650; average reward: 13.25 - action: w; reward: -5.00
 1097/2000: episode: 647, duration: 15.574s, episode steps: 7, steps per second: 0, episode reward: -65.000, mean reward: -9.286 [-10.000, -5.000], mean action: 6.143 [6.000, 7.000], mean observation: 0.215 [0.000, 3.000], loss: 893.680054, mean_absolute_error: 12.605928, mean_q: 15.785158
step 1794; T: 991350; average reward: 13.22 - action: nw; reward: -5.00
 1098/2000: episode: 648, duration: 1.212s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.192 [0.000, 2.000], loss: 2602.046143, mean_absolute_error: 13.817375, mean_q: 14.258200
step 1797; T: 992550; average reward: 13.23 - action: nw; reward: 22.15
 1100/2000: episode: 649, duration: 3.447s, episode steps: 2, steps per second: 1, episode reward: 12.150, mean reward: 6.075 [-10.000, 22.150], mean action: 7.500 [7.000, 8.000], mean observation: 0.308 [0.000, 3.000], loss: 747.331299, mean_absolute_error: 12.016768, mean_q: 13.429544
step 1801; T: 994200; average reward: 13.00 - action: se; reward: -138.60
 1102/2000: episode: 650, duration: 4.148s, episode steps: 2, steps per second: 0, episode reward: -153.600, mean reward: -76.800 [-143.600, -10.000], mean action: 4.500 [3.000, 6.000], mean observation: 0.200 [0.000, 3.000], loss: 687.265198, mean_absolute_error: 11.645107, mean_q: 15.055214
step 1802; T: 999750; average reward: 12.97 - action: w; reward: -5.00
 1103/2000: episode: 651, duration: 1.408s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.176 [0.000, 4.000], loss: 799.731079, mean_absolute_error: 12.661552, mean_q: 14.459332
step 1810; T: 1002900; average reward: 12.95 - action: nw; reward: -5.00
 1107/2000: episode: 652, duration: 8.603s, episode steps: 4, steps per second: 0, episode reward: -40.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.000 [3.000, 7.000], mean observation: 0.246 [0.000, 4.000], loss: 767.886414, mean_absolute_error: 12.779356, mean_q: 15.294899
step 1812; T: 1004700; average reward: 13.01 - action: n; reward: 57.52
 1108/2000: episode: 653, duration: 2.349s, episode steps: 1, steps per second: 0, episode reward: 52.516, mean reward: 52.516 [52.516, 52.516], mean action: 8.000 [8.000, 8.000], mean observation: 0.251 [0.000, 4.000], loss: 776.922852, mean_absolute_error: 11.768351, mean_q: 15.967557
step 1814; T: 1006650; average reward: 13.08 - action: w; reward: 55.85
 1109/2000: episode: 654, duration: 2.195s, episode steps: 1, steps per second: 0, episode reward: 50.852, mean reward: 50.852 [50.852, 50.852], mean action: 6.000 [6.000, 6.000], mean observation: 0.133 [0.000, 2.000], loss: 918.235657, mean_absolute_error: 12.562294, mean_q: 17.017117
step 1817; T: 1007850; average reward: 12.98 - action: e; reward: -49.82
 1111/2000: episode: 655, duration: 3.292s, episode steps: 2, steps per second: 1, episode reward: -59.820, mean reward: -29.910 [-49.820, -10.000], mean action: 4.000 [2.000, 6.000], mean observation: 0.253 [0.000, 3.000], loss: 767.713501, mean_absolute_error: 11.855288, mean_q: 15.787608
step 1818; T: 1008300; average reward: 13.04 - action: ne; reward: 52.92
 1112/2000: episode: 656, duration: 1.047s, episode steps: 1, steps per second: 1, episode reward: 52.919, mean reward: 52.919 [52.919, 52.919], mean action: 1.000 [1.000, 1.000], mean observation: 0.267 [0.000, 3.000], loss: 142.080505, mean_absolute_error: 11.200716, mean_q: 15.343330
step 1821; T: 1009950; average reward: 13.07 - action: nw; reward: 28.63
 1114/2000: episode: 657, duration: 3.517s, episode steps: 2, steps per second: 1, episode reward: 18.630, mean reward: 9.315 [-10.000, 28.630], mean action: 7.500 [7.000, 8.000], mean observation: 0.285 [0.000, 3.000], loss: 420.427277, mean_absolute_error: 11.667264, mean_q: 14.542068
step 1827; T: 1012350; average reward: 13.04 - action: nw; reward: -5.00
 1117/2000: episode: 658, duration: 6.674s, episode steps: 3, steps per second: 0, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.000 [1.000, 7.000], mean observation: 0.330 [0.000, 3.000], loss: 738.584717, mean_absolute_error: 12.828584, mean_q: 15.335120
step 1829; T: 1012950; average reward: 13.06 - action: e; reward: 28.33
 1118/2000: episode: 659, duration: 1.953s, episode steps: 1, steps per second: 1, episode reward: 23.332, mean reward: 23.332 [23.332, 23.332], mean action: 2.000 [2.000, 2.000], mean observation: 0.360 [0.000, 3.000], loss: 406.115723, mean_absolute_error: 11.486282, mean_q: 17.210438
step 1831; T: 1014000; average reward: 13.00 - action: w; reward: -30.71
 1119/2000: episode: 660, duration: 1.978s, episode steps: 1, steps per second: 1, episode reward: -35.708, mean reward: -35.708 [-35.708, -35.708], mean action: 6.000 [6.000, 6.000], mean observation: 0.251 [0.000, 3.000], loss: 459.151764, mean_absolute_error: 11.253597, mean_q: 14.187291
step 1833; T: 1015800; average reward: 12.97 - action: nw; reward: -5.00
 1120/2000: episode: 661, duration: 2.109s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.307 [0.000, 3.000], loss: 370.272705, mean_absolute_error: 11.237285, mean_q: 10.190823
step 1835; T: 1016550; average reward: 12.83 - action: w; reward: -82.62
 1121/2000: episode: 662, duration: 2.030s, episode steps: 1, steps per second: 0, episode reward: -87.619, mean reward: -87.619 [-87.619, -87.619], mean action: 6.000 [6.000, 6.000], mean observation: 0.259 [0.000, 2.000], loss: 396.832214, mean_absolute_error: 11.237541, mean_q: 18.427843
step 1839; T: 1019850; average reward: 12.87 - action: ne; reward: 40.43
 1123/2000: episode: 663, duration: 5.313s, episode steps: 2, steps per second: 0, episode reward: 35.433, mean reward: 17.716 [0.000, 35.433], mean action: 0.500 [0.000, 1.000], mean observation: 0.233 [0.000, 3.000], loss: 762.123779, mean_absolute_error: 12.501640, mean_q: 16.339233
step 1846; T: 1021800; average reward: 12.50 - action: sw; reward: -229.21
 1127/2000: episode: 664, duration: 7.677s, episode steps: 4, steps per second: 1, episode reward: -259.212, mean reward: -64.803 [-229.212, -10.000], mean action: 3.250 [1.000, 6.000], mean observation: 0.321 [0.000, 4.000], loss: 714.726807, mean_absolute_error: 12.290471, mean_q: 14.195612
step 1850; T: 1022550; average reward: 12.48 - action: e; reward: -5.00
 1129/2000: episode: 665, duration: 4.403s, episode steps: 2, steps per second: 0, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 4.000 [2.000, 6.000], mean observation: 0.228 [0.000, 3.000], loss: 1965.218018, mean_absolute_error: 12.580557, mean_q: 14.022334
step 1851; T: 1023450; average reward: 12.49 - action: w; reward: 24.83
 1130/2000: episode: 666, duration: 1.144s, episode steps: 1, steps per second: 1, episode reward: 24.826, mean reward: 24.826 [24.826, 24.826], mean action: 6.000 [6.000, 6.000], mean observation: 0.205 [0.000, 3.000], loss: 616.111328, mean_absolute_error: 10.819885, mean_q: 10.589241
step 1856; T: 1025100; average reward: 12.47 - action: ne; reward: -5.00
 1133/2000: episode: 667, duration: 5.511s, episode steps: 3, steps per second: 1, episode reward: -25.000, mean reward: -8.333 [-10.000, -5.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.241 [0.000, 4.000], loss: 331.367584, mean_absolute_error: 11.402287, mean_q: 14.407184
step 1860; T: 1026450; average reward: 12.44 - action: ne; reward: -5.00
 1135/2000: episode: 668, duration: 4.117s, episode steps: 2, steps per second: 0, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.265 [0.000, 4.000], loss: 556.559448, mean_absolute_error: 11.690998, mean_q: 14.368496
step 1861; T: 1027200; average reward: 12.42 - action: ne; reward: 0.40
 1136/2000: episode: 669, duration: 1.015s, episode steps: 1, steps per second: 1, episode reward: 0.401, mean reward: 0.401 [0.401, 0.401], mean action: 1.000 [1.000, 1.000], mean observation: 0.397 [0.000, 3.000], loss: 782.720703, mean_absolute_error: 13.105200, mean_q: 14.644091
step 1863; T: 1027950; average reward: 12.21 - action: w; reward: -134.55
 1137/2000: episode: 670, duration: 2.030s, episode steps: 1, steps per second: 0, episode reward: -139.550, mean reward: -139.550 [-139.550, -139.550], mean action: 6.000 [6.000, 6.000], mean observation: 0.304 [0.000, 3.000], loss: 346.645569, mean_absolute_error: 11.245943, mean_q: 12.859712
step 1865; T: 1028850; average reward: 12.18 - action: sw; reward: -5.00
 1138/2000: episode: 671, duration: 2.008s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.296 [0.000, 2.000], loss: 825.594604, mean_absolute_error: 12.951214, mean_q: 15.992728
step 1869; T: 1030650; average reward: 12.20 - action: ne; reward: 28.50
 1140/2000: episode: 672, duration: 3.857s, episode steps: 2, steps per second: 1, episode reward: 13.497, mean reward: 6.749 [-10.000, 23.497], mean action: 3.500 [1.000, 6.000], mean observation: 0.196 [0.000, 3.000], loss: 818.690857, mean_absolute_error: 12.467113, mean_q: 14.164562
step 1870; T: 1031700; average reward: 12.31 - action: w; reward: 84.01
 1141/2000: episode: 673, duration: 0.974s, episode steps: 1, steps per second: 1, episode reward: 84.007, mean reward: 84.007 [84.007, 84.007], mean action: 6.000 [6.000, 6.000], mean observation: 0.304 [0.000, 3.000], loss: 422.433472, mean_absolute_error: 10.427163, mean_q: 12.824444
step 1880; T: 1034550; average reward: 12.28 - action: ne; reward: -5.00
 1146/2000: episode: 674, duration: 9.402s, episode steps: 5, steps per second: 1, episode reward: -50.000, mean reward: -10.000 [-10.000, -10.000], mean action: 3.200 [1.000, 8.000], mean observation: 0.268 [0.000, 3.000], loss: 448.469666, mean_absolute_error: 11.023075, mean_q: 11.605065
step 1881; T: 1036350; average reward: 12.53 - action: se; reward: 179.93
 1147/2000: episode: 675, duration: 1.093s, episode steps: 1, steps per second: 1, episode reward: 179.930, mean reward: 179.930 [179.930, 179.930], mean action: 3.000 [3.000, 3.000], mean observation: 0.373 [0.000, 3.000], loss: 591.633179, mean_absolute_error: 11.921738, mean_q: 15.347020
step 1882; T: 1037550; average reward: 12.63 - action: nw; reward: 74.91
 1148/2000: episode: 676, duration: 1.017s, episode steps: 1, steps per second: 1, episode reward: 74.908, mean reward: 74.908 [74.908, 74.908], mean action: 7.000 [7.000, 7.000], mean observation: 0.317 [0.000, 3.000], loss: 1448.534912, mean_absolute_error: 12.407744, mean_q: 14.705370
step 1884; T: 1038150; average reward: 12.60 - action: nw; reward: -2.23
 1149/2000: episode: 677, duration: 2.382s, episode steps: 1, steps per second: 0, episode reward: -7.232, mean reward: -7.232 [-7.232, -7.232], mean action: 7.000 [7.000, 7.000], mean observation: 0.416 [0.000, 3.000], loss: 248.948090, mean_absolute_error: 10.821435, mean_q: 12.161340
step 1886; T: 1039500; average reward: 12.71 - action: n; reward: 88.22
 1150/2000: episode: 678, duration: 2.283s, episode steps: 1, steps per second: 0, episode reward: 83.220, mean reward: 83.220 [83.220, 83.220], mean action: 8.000 [8.000, 8.000], mean observation: 0.415 [0.000, 3.000], loss: 1519.232422, mean_absolute_error: 12.330719, mean_q: 11.273478
step 1888; T: 1040250; average reward: 12.56 - action: nw; reward: -94.85
 1151/2000: episode: 679, duration: 2.293s, episode steps: 1, steps per second: 0, episode reward: -99.851, mean reward: -99.851 [-99.851, -99.851], mean action: 7.000 [7.000, 7.000], mean observation: 0.405 [0.000, 4.000], loss: 1086.148926, mean_absolute_error: 12.196905, mean_q: 13.477837
step 1891; T: 1042800; average reward: 12.68 - action: ne; reward: 96.45
 1153/2000: episode: 680, duration: 3.486s, episode steps: 2, steps per second: 1, episode reward: 86.448, mean reward: 43.224 [-10.000, 96.448], mean action: 3.500 [1.000, 6.000], mean observation: 0.344 [0.000, 3.000], loss: 991.365234, mean_absolute_error: 12.523511, mean_q: 12.078215
step 1893; T: 1045500; average reward: 12.69 - action: n; reward: 22.60
 1154/2000: episode: 681, duration: 2.281s, episode steps: 1, steps per second: 0, episode reward: 17.603, mean reward: 17.603 [17.603, 17.603], mean action: 8.000 [8.000, 8.000], mean observation: 0.245 [0.000, 3.000], loss: 534.987427, mean_absolute_error: 12.408842, mean_q: 16.764557
step 1894; T: 1047300; average reward: 12.80 - action: n; reward: 84.98
 1155/2000: episode: 682, duration: 1.173s, episode steps: 1, steps per second: 1, episode reward: 84.978, mean reward: 84.978 [84.978, 84.978], mean action: 8.000 [8.000, 8.000], mean observation: 0.488 [0.000, 3.000], loss: 738.981323, mean_absolute_error: 11.157178, mean_q: 13.677781
step 1896; T: 1048050; average reward: 12.80 - action: nw; reward: 10.69
 1156/2000: episode: 683, duration: 2.269s, episode steps: 1, steps per second: 0, episode reward: 5.693, mean reward: 5.693 [5.693, 5.693], mean action: 7.000 [7.000, 7.000], mean observation: 0.261 [0.000, 2.000], loss: 612.867920, mean_absolute_error: 11.157381, mean_q: 11.765074
step 1897; T: 1049250; average reward: 12.67 - action: sw; reward: -75.44
 1157/2000: episode: 684, duration: 1.115s, episode steps: 1, steps per second: 1, episode reward: -75.436, mean reward: -75.436 [-75.436, -75.436], mean action: 5.000 [5.000, 5.000], mean observation: 0.403 [0.000, 3.000], loss: 859.875122, mean_absolute_error: 12.406662, mean_q: 15.358788
step 1899; T: 1049850; average reward: 12.63 - action: nw; reward: -14.78
 1158/2000: episode: 685, duration: 2.281s, episode steps: 1, steps per second: 0, episode reward: -19.777, mean reward: -19.777 [-19.777, -19.777], mean action: 7.000 [7.000, 7.000], mean observation: 0.376 [0.000, 3.000], loss: 679.921387, mean_absolute_error: 13.506388, mean_q: 13.265686
step 1900; T: 1050300; average reward: 12.62 - action: w; reward: 7.91
 1159/2000: episode: 686, duration: 1.117s, episode steps: 1, steps per second: 1, episode reward: 7.908, mean reward: 7.908 [7.908, 7.908], mean action: 6.000 [6.000, 6.000], mean observation: 0.435 [0.000, 3.000], loss: 985.658569, mean_absolute_error: 12.470734, mean_q: 11.481085
step 1901; T: 1051500; average reward: 12.60 - action: ne; reward: -5.00
 1160/2000: episode: 687, duration: 1.223s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.277 [0.000, 3.000], loss: 976.238586, mean_absolute_error: 12.049269, mean_q: 12.500671
step 1903; T: 1057650; average reward: 12.48 - action: e; reward: -64.09
 1161/2000: episode: 688, duration: 2.695s, episode steps: 1, steps per second: 0, episode reward: -69.094, mean reward: -69.094 [-69.094, -69.094], mean action: 2.000 [2.000, 2.000], mean observation: 0.253 [0.000, 3.000], loss: 529.745483, mean_absolute_error: 11.636114, mean_q: 13.769144
step 1915; T: 1060500; average reward: 12.46 - action: nw; reward: -5.00
 1167/2000: episode: 689, duration: 13.648s, episode steps: 6, steps per second: 0, episode reward: -60.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.333 [2.000, 7.000], mean observation: 0.270 [0.000, 3.000], loss: 985.154541, mean_absolute_error: 12.450011, mean_q: 12.243356
step 1917; T: 1061100; average reward: 12.49 - action: nw; reward: 31.16
 1168/2000: episode: 690, duration: 2.201s, episode steps: 1, steps per second: 0, episode reward: 26.162, mean reward: 26.162 [26.162, 26.162], mean action: 7.000 [7.000, 7.000], mean observation: 0.227 [0.000, 3.000], loss: 3118.632812, mean_absolute_error: 13.418202, mean_q: 16.057585
step 1919; T: 1061850; average reward: 12.50 - action: e; reward: 22.18
 1169/2000: episode: 691, duration: 2.349s, episode steps: 1, steps per second: 0, episode reward: 17.177, mean reward: 17.177 [17.177, 17.177], mean action: 2.000 [2.000, 2.000], mean observation: 0.245 [0.000, 2.000], loss: 686.368225, mean_absolute_error: 12.374631, mean_q: 12.891675
step 1920; T: 1062750; average reward: 12.47 - action: e; reward: -5.01
 1170/2000: episode: 692, duration: 1.132s, episode steps: 1, steps per second: 1, episode reward: -5.007, mean reward: -5.007 [-5.007, -5.007], mean action: 2.000 [2.000, 2.000], mean observation: 0.437 [0.000, 3.000], loss: 1074.587402, mean_absolute_error: 12.895206, mean_q: 11.016184
step 1928; T: 1064700; average reward: 12.45 - action: n; reward: -5.00
 1174/2000: episode: 693, duration: 8.380s, episode steps: 4, steps per second: 0, episode reward: -40.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.750 [6.000, 8.000], mean observation: 0.345 [0.000, 4.000], loss: 442.784729, mean_absolute_error: 11.745113, mean_q: 12.562073
step 1935; T: 1066800; average reward: 12.42 - action: nw; reward: -5.00
 1178/2000: episode: 694, duration: 7.372s, episode steps: 4, steps per second: 1, episode reward: -35.000, mean reward: -8.750 [-10.000, -5.000], mean action: 5.250 [3.000, 8.000], mean observation: 0.232 [0.000, 4.000], loss: 923.240356, mean_absolute_error: 12.525702, mean_q: 12.229524
step 1936; T: 1067250; average reward: 12.45 - action: nw; reward: 33.37
 1179/2000: episode: 695, duration: 1.056s, episode steps: 1, steps per second: 1, episode reward: 33.373, mean reward: 33.373 [33.373, 33.373], mean action: 7.000 [7.000, 7.000], mean observation: 0.360 [0.000, 3.000], loss: 1002.451904, mean_absolute_error: 12.739408, mean_q: 16.816029
step 1939; T: 1068450; average reward: 12.33 - action: nw; reward: -76.39
 1181/2000: episode: 696, duration: 3.354s, episode steps: 2, steps per second: 1, episode reward: -86.387, mean reward: -43.193 [-76.387, -10.000], mean action: 6.500 [6.000, 7.000], mean observation: 0.295 [0.000, 4.000], loss: 1091.108032, mean_absolute_error: 12.859785, mean_q: 15.344645
step 1940; T: 1069050; average reward: 12.46 - action: e; reward: 103.67
 1182/2000: episode: 697, duration: 1.153s, episode steps: 1, steps per second: 1, episode reward: 103.667, mean reward: 103.667 [103.667, 103.667], mean action: 2.000 [2.000, 2.000], mean observation: 0.307 [0.000, 2.000], loss: 664.007935, mean_absolute_error: 12.234579, mean_q: 15.708248
step 1942; T: 1071300; average reward: 12.40 - action: w; reward: -24.68
 1183/2000: episode: 698, duration: 2.242s, episode steps: 1, steps per second: 0, episode reward: -29.680, mean reward: -29.680 [-29.680, -29.680], mean action: 6.000 [6.000, 6.000], mean observation: 0.333 [0.000, 3.000], loss: 278.314758, mean_absolute_error: 12.060195, mean_q: 13.050200
step 1943; T: 1072500; average reward: 12.44 - action: n; reward: 34.25
 1184/2000: episode: 699, duration: 1.230s, episode steps: 1, steps per second: 1, episode reward: 34.249, mean reward: 34.249 [34.249, 34.249], mean action: 8.000 [8.000, 8.000], mean observation: 0.160 [0.000, 3.000], loss: 1107.208740, mean_absolute_error: 12.445015, mean_q: 16.704216
step 1944; T: 1072800; average reward: 12.42 - action: ne; reward: 4.11
 1185/2000: episode: 700, duration: 1.045s, episode steps: 1, steps per second: 1, episode reward: 4.108, mean reward: 4.108 [4.108, 4.108], mean action: 1.000 [1.000, 1.000], mean observation: 0.200 [0.000, 2.000], loss: 419.892456, mean_absolute_error: 11.628191, mean_q: 14.749884
step 1947; T: 1075200; average reward: 12.54 - action: e; reward: 91.05
 1187/2000: episode: 701, duration: 3.306s, episode steps: 2, steps per second: 1, episode reward: 81.048, mean reward: 40.524 [-10.000, 91.048], mean action: 4.000 [2.000, 6.000], mean observation: 0.207 [0.000, 3.000], loss: 493.030151, mean_absolute_error: 11.935885, mean_q: 12.978456
step 1952; T: 1077000; average reward: 12.37 - action: nw; reward: -104.53
 1190/2000: episode: 702, duration: 5.311s, episode steps: 3, steps per second: 1, episode reward: -124.532, mean reward: -41.511 [-104.532, -10.000], mean action: 3.000 [1.000, 7.000], mean observation: 0.328 [0.000, 5.000], loss: 1192.707886, mean_absolute_error: 12.764316, mean_q: 14.834685
step 1953; T: 1078200; average reward: 12.37 - action: nw; reward: 13.68
 1191/2000: episode: 703, duration: 1.048s, episode steps: 1, steps per second: 1, episode reward: 13.681, mean reward: 13.681 [13.681, 13.681], mean action: 7.000 [7.000, 7.000], mean observation: 0.272 [0.000, 3.000], loss: 574.290405, mean_absolute_error: 11.414034, mean_q: 10.523516
step 1957; T: 1081050; average reward: 12.38 - action: ne; reward: 17.38
 1193/2000: episode: 704, duration: 4.518s, episode steps: 2, steps per second: 0, episode reward: 2.377, mean reward: 1.188 [-10.000, 12.377], mean action: 4.500 [1.000, 8.000], mean observation: 0.329 [0.000, 4.000], loss: 661.112305, mean_absolute_error: 11.384499, mean_q: 13.765647
step 1958; T: 1082700; average reward: 12.33 - action: n; reward: -21.78
 1194/2000: episode: 705, duration: 1.434s, episode steps: 1, steps per second: 1, episode reward: -21.783, mean reward: -21.783 [-21.783, -21.783], mean action: 8.000 [8.000, 8.000], mean observation: 0.333 [0.000, 3.000], loss: 1993.319336, mean_absolute_error: 14.215861, mean_q: 15.377237
step 1960; T: 1083750; average reward: 12.41 - action: nw; reward: 71.70
 1195/2000: episode: 706, duration: 2.323s, episode steps: 1, steps per second: 0, episode reward: 66.700, mean reward: 66.700 [66.700, 66.700], mean action: 7.000 [7.000, 7.000], mean observation: 0.211 [0.000, 2.000], loss: 308.033325, mean_absolute_error: 12.828993, mean_q: 13.912422
step 1968; T: 1086150; average reward: 12.39 - action: ne; reward: -5.00
 1199/2000: episode: 707, duration: 9.669s, episode steps: 4, steps per second: 0, episode reward: -40.000, mean reward: -10.000 [-10.000, -10.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.218 [0.000, 3.000], loss: 506.461456, mean_absolute_error: 12.106175, mean_q: 12.293112
step 1971; T: 1087650; average reward: 12.48 - action: ne; reward: 75.85
 1201/2000: episode: 708, duration: 3.461s, episode steps: 2, steps per second: 1, episode reward: 65.847, mean reward: 32.924 [-10.000, 75.847], mean action: 1.500 [1.000, 2.000], mean observation: 0.313 [0.000, 3.000], loss: 737.191711, mean_absolute_error: 12.941563, mean_q: 15.879320
step 1974; T: 1088850; average reward: 12.55 - action: se; reward: 65.23
 1203/2000: episode: 709, duration: 3.476s, episode steps: 2, steps per second: 1, episode reward: 55.229, mean reward: 27.615 [-10.000, 65.229], mean action: 5.500 [3.000, 8.000], mean observation: 0.232 [0.000, 3.000], loss: 1774.308838, mean_absolute_error: 12.686615, mean_q: 14.887579
step 1975; T: 1089600; average reward: 12.61 - action: ne; reward: 55.72
 1204/2000: episode: 710, duration: 1.290s, episode steps: 1, steps per second: 1, episode reward: 55.723, mean reward: 55.723 [55.723, 55.723], mean action: 1.000 [1.000, 1.000], mean observation: 0.416 [0.000, 4.000], loss: 562.408630, mean_absolute_error: 11.295025, mean_q: 12.817249
step 1981; T: 1091400; average reward: 12.59 - action: nw; reward: -5.00
 1207/2000: episode: 711, duration: 7.011s, episode steps: 3, steps per second: 0, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.333 [7.000, 8.000], mean observation: 0.381 [0.000, 4.000], loss: 918.725098, mean_absolute_error: 12.552528, mean_q: 12.912346
step 1987; T: 1092900; average reward: 12.51 - action: nw; reward: -42.61
 1210/2000: episode: 712, duration: 6.943s, episode steps: 3, steps per second: 0, episode reward: -67.615, mean reward: -22.538 [-47.615, -10.000], mean action: 4.667 [1.000, 7.000], mean observation: 0.330 [0.000, 3.000], loss: 424.520386, mean_absolute_error: 11.845502, mean_q: 13.358032
step 1990; T: 1094250; average reward: 12.50 - action: nw; reward: 7.23
 1212/2000: episode: 713, duration: 3.344s, episode steps: 2, steps per second: 1, episode reward: -2.771, mean reward: -1.385 [-10.000, 7.229], mean action: 5.000 [3.000, 7.000], mean observation: 0.355 [0.000, 3.000], loss: 1063.991943, mean_absolute_error: 12.886017, mean_q: 15.606372
step 1991; T: 1095900; average reward: 12.63 - action: ne; reward: 100.77
 1213/2000: episode: 714, duration: 1.191s, episode steps: 1, steps per second: 1, episode reward: 100.772, mean reward: 100.772 [100.772, 100.772], mean action: 1.000 [1.000, 1.000], mean observation: 0.472 [0.000, 3.000], loss: 352.795776, mean_absolute_error: 11.770080, mean_q: 11.924257
step 1992; T: 1097550; average reward: 12.67 - action: w; reward: 45.62
 1214/2000: episode: 715, duration: 1.356s, episode steps: 1, steps per second: 1, episode reward: 45.621, mean reward: 45.621 [45.621, 45.621], mean action: 6.000 [6.000, 6.000], mean observation: 0.224 [0.000, 3.000], loss: 1078.322388, mean_absolute_error: 11.522978, mean_q: 12.814152
step 1994; T: 1099500; average reward: 12.65 - action: n; reward: -5.00
 1215/2000: episode: 716, duration: 2.370s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.448 [0.000, 3.000], loss: 338.897888, mean_absolute_error: 11.249844, mean_q: 11.214729
step 1996; T: 1100250; average reward: 12.91 - action: w; reward: 200.68
 1216/2000: episode: 717, duration: 2.373s, episode steps: 1, steps per second: 0, episode reward: 195.680, mean reward: 195.680 [195.680, 195.680], mean action: 6.000 [6.000, 6.000], mean observation: 0.304 [0.000, 3.000], loss: 1035.837280, mean_absolute_error: 12.552890, mean_q: 15.052140
step 1999; T: 1101300; average reward: 12.89 - action: ne; reward: -5.00
 1218/2000: episode: 718, duration: 3.322s, episode steps: 2, steps per second: 1, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 3.000 [1.000, 5.000], mean observation: 0.223 [0.000, 4.000], loss: 682.945312, mean_absolute_error: 12.052343, mean_q: 15.269226
step 2023; T: 1105350; average reward: 12.86 - action: nw; reward: -5.00
 1230/2000: episode: 719, duration: 28.147s, episode steps: 12, steps per second: 0, episode reward: -120.000, mean reward: -10.000 [-10.000, -10.000], mean action: 3.083 [1.000, 7.000], mean observation: 0.182 [0.000, 4.000], loss: 794.688232, mean_absolute_error: 12.559159, mean_q: 14.740627
step 2026; T: 1106850; average reward: 12.94 - action: w; reward: 70.24
 1232/2000: episode: 720, duration: 3.973s, episode steps: 2, steps per second: 1, episode reward: 60.245, mean reward: 30.122 [-10.000, 70.245], mean action: 6.500 [6.000, 7.000], mean observation: 0.240 [0.000, 4.000], loss: 329.551819, mean_absolute_error: 11.271214, mean_q: 16.137016
step 2038; T: 1109100; average reward: 12.92 - action: se; reward: -5.00
 1238/2000: episode: 721, duration: 14.792s, episode steps: 6, steps per second: 0, episode reward: -60.000, mean reward: -10.000 [-10.000, -10.000], mean action: 4.833 [2.000, 6.000], mean observation: 0.245 [0.000, 4.000], loss: 718.253113, mean_absolute_error: 12.317047, mean_q: 14.258447
step 2039; T: 1110000; average reward: 12.95 - action: w; reward: 39.21
 1239/2000: episode: 722, duration: 1.262s, episode steps: 1, steps per second: 1, episode reward: 39.213, mean reward: 39.213 [39.213, 39.213], mean action: 6.000 [6.000, 6.000], mean observation: 0.293 [0.000, 3.000], loss: 367.179016, mean_absolute_error: 10.396456, mean_q: 14.795212
step 2041; T: 1111650; average reward: 12.93 - action: sw; reward: -5.00
 1240/2000: episode: 723, duration: 2.460s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.277 [0.000, 2.000], loss: 1009.212036, mean_absolute_error: 12.082445, mean_q: 16.075409
step 2047; T: 1113900; average reward: 12.92 - action: w; reward: 9.65
 1243/2000: episode: 724, duration: 7.326s, episode steps: 3, steps per second: 0, episode reward: -15.355, mean reward: -5.118 [-10.000, 4.645], mean action: 4.667 [1.000, 7.000], mean observation: 0.337 [0.000, 3.000], loss: 463.992523, mean_absolute_error: 11.583092, mean_q: 13.118398
step 2049; T: 1114500; average reward: 12.90 - action: nw; reward: -5.00
 1244/2000: episode: 725, duration: 2.516s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.352 [0.000, 3.000], loss: 1131.160767, mean_absolute_error: 13.093354, mean_q: 13.825289
step 2054; T: 1116000; average reward: 12.87 - action: nw; reward: -5.00
 1247/2000: episode: 726, duration: 6.201s, episode steps: 3, steps per second: 0, episode reward: -25.000, mean reward: -8.333 [-10.000, -5.000], mean action: 4.000 [1.000, 7.000], mean observation: 0.221 [0.000, 3.000], loss: 694.634460, mean_absolute_error: 11.401168, mean_q: 13.894717
step 2059; T: 1118250; average reward: 12.85 - action: ne; reward: -5.00
 1250/2000: episode: 727, duration: 6.249s, episode steps: 3, steps per second: 0, episode reward: -25.000, mean reward: -8.333 [-10.000, -5.000], mean action: 3.000 [1.000, 7.000], mean observation: 0.321 [0.000, 3.000], loss: 448.937408, mean_absolute_error: 11.469592, mean_q: 11.906688
step 2063; T: 1119300; average reward: 12.72 - action: ne; reward: -83.02
 1252/2000: episode: 728, duration: 4.972s, episode steps: 2, steps per second: 0, episode reward: -98.023, mean reward: -49.012 [-88.023, -10.000], mean action: 4.000 [1.000, 7.000], mean observation: 0.456 [0.000, 4.000], loss: 422.855591, mean_absolute_error: 11.913395, mean_q: 13.505049
step 2065; T: 1121700; average reward: 12.70 - action: nw; reward: 2.14
 1253/2000: episode: 729, duration: 2.803s, episode steps: 1, steps per second: 0, episode reward: -2.860, mean reward: -2.860 [-2.860, -2.860], mean action: 7.000 [7.000, 7.000], mean observation: 0.227 [0.000, 3.000], loss: 421.220856, mean_absolute_error: 11.899070, mean_q: 10.909006
step 2066; T: 1122750; average reward: 12.72 - action: ne; reward: 22.56
 1254/2000: episode: 730, duration: 1.256s, episode steps: 1, steps per second: 1, episode reward: 22.563, mean reward: 22.563 [22.563, 22.563], mean action: 1.000 [1.000, 1.000], mean observation: 0.259 [0.000, 3.000], loss: 599.164490, mean_absolute_error: 10.430403, mean_q: 10.980020
step 2068; T: 1123200; average reward: 12.69 - action: se; reward: -5.00
 1255/2000: episode: 731, duration: 2.499s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.347 [0.000, 3.000], loss: 1151.891479, mean_absolute_error: 11.376921, mean_q: 11.032545
step 2069; T: 1124550; average reward: 12.50 - action: nw; reward: -127.99
 1256/2000: episode: 732, duration: 1.442s, episode steps: 1, steps per second: 1, episode reward: -127.989, mean reward: -127.989 [-127.989, -127.989], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 2.000], loss: 681.699646, mean_absolute_error: 12.530030, mean_q: 14.207549
step 2070; T: 1125600; average reward: 12.46 - action: ne; reward: -20.65
 1257/2000: episode: 733, duration: 1.201s, episode steps: 1, steps per second: 1, episode reward: -20.651, mean reward: -20.651 [-20.651, -20.651], mean action: 1.000 [1.000, 1.000], mean observation: 0.429 [0.000, 4.000], loss: 877.230347, mean_absolute_error: 12.918556, mean_q: 13.976711
step 2072; T: 1126350; average reward: 12.46 - action: w; reward: 13.39
 1258/2000: episode: 734, duration: 2.524s, episode steps: 1, steps per second: 0, episode reward: 8.391, mean reward: 8.391 [8.391, 8.391], mean action: 6.000 [6.000, 6.000], mean observation: 0.380 [0.000, 3.000], loss: 462.712982, mean_absolute_error: 11.495028, mean_q: 19.421997
step 2074; T: 1127250; average reward: 12.43 - action: ne; reward: -5.00
 1259/2000: episode: 735, duration: 2.288s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.259 [0.000, 2.000], loss: 686.488403, mean_absolute_error: 10.172384, mean_q: 10.220224
step 2075; T: 1127400; average reward: 12.36 - action: se; reward: -42.73
 1260/2000: episode: 736, duration: 1.232s, episode steps: 1, steps per second: 1, episode reward: -42.726, mean reward: -42.726 [-42.726, -42.726], mean action: 3.000 [3.000, 3.000], mean observation: 0.211 [0.000, 2.000], loss: 913.162292, mean_absolute_error: 12.692064, mean_q: 13.201748
step 2077; T: 1128600; average reward: 12.35 - action: ne; reward: 6.21
 1261/2000: episode: 737, duration: 2.818s, episode steps: 1, steps per second: 0, episode reward: 1.207, mean reward: 1.207 [1.207, 1.207], mean action: 1.000 [1.000, 1.000], mean observation: 0.192 [0.000, 4.000], loss: 618.330139, mean_absolute_error: 12.944927, mean_q: 13.632313
step 2080; T: 1129500; average reward: 12.50 - action: e; reward: 121.07
 1263/2000: episode: 738, duration: 3.804s, episode steps: 2, steps per second: 1, episode reward: 111.068, mean reward: 55.534 [-10.000, 121.068], mean action: 4.000 [2.000, 6.000], mean observation: 0.329 [0.000, 4.000], loss: 704.525391, mean_absolute_error: 12.102715, mean_q: 14.925370
step 2082; T: 1132800; average reward: 12.57 - action: n; reward: 64.13
 1264/2000: episode: 739, duration: 2.933s, episode steps: 1, steps per second: 0, episode reward: 59.133, mean reward: 59.133 [59.133, 59.133], mean action: 8.000 [8.000, 8.000], mean observation: 0.317 [0.000, 3.000], loss: 1253.921875, mean_absolute_error: 13.600245, mean_q: 10.741549
step 2085; T: 1134300; average reward: 12.59 - action: w; reward: 32.32
 1266/2000: episode: 740, duration: 4.257s, episode steps: 2, steps per second: 0, episode reward: 22.323, mean reward: 11.162 [-10.000, 32.323], mean action: 6.500 [6.000, 7.000], mean observation: 0.385 [0.000, 3.000], loss: 1618.726929, mean_absolute_error: 13.308640, mean_q: 13.584627
step 2090; T: 1138050; average reward: 12.48 - action: n; reward: -75.33
 1269/2000: episode: 741, duration: 6.489s, episode steps: 3, steps per second: 0, episode reward: -95.331, mean reward: -31.777 [-75.331, -10.000], mean action: 5.333 [1.000, 8.000], mean observation: 0.345 [0.000, 3.000], loss: 911.232727, mean_absolute_error: 12.180507, mean_q: 13.287936
step 2093; T: 1141200; average reward: 12.56 - action: ne; reward: 78.40
 1271/2000: episode: 742, duration: 3.612s, episode steps: 2, steps per second: 1, episode reward: 68.400, mean reward: 34.200 [-10.000, 78.400], mean action: 4.000 [1.000, 7.000], mean observation: 0.309 [0.000, 3.000], loss: 1028.211426, mean_absolute_error: 12.041614, mean_q: 15.380497
step 2095; T: 1142400; average reward: 12.54 - action: ne; reward: -3.37
 1272/2000: episode: 743, duration: 3.314s, episode steps: 1, steps per second: 0, episode reward: -8.372, mean reward: -8.372 [-8.372, -8.372], mean action: 1.000 [1.000, 1.000], mean observation: 0.456 [0.000, 3.000], loss: 205.802399, mean_absolute_error: 11.044621, mean_q: 11.032101
step 2096; T: 1144350; average reward: 12.52 - action: n; reward: -2.98
 1273/2000: episode: 744, duration: 1.574s, episode steps: 1, steps per second: 1, episode reward: -2.979, mean reward: -2.979 [-2.979, -2.979], mean action: 8.000 [8.000, 8.000], mean observation: 0.267 [0.000, 3.000], loss: 507.825623, mean_absolute_error: 10.450274, mean_q: 11.060509
step 2113; T: 1148550; average reward: 12.50 - action: e; reward: -5.00
 1282/2000: episode: 745, duration: 21.582s, episode steps: 9, steps per second: 0, episode reward: -85.000, mean reward: -9.444 [-10.000, -5.000], mean action: 2.444 [2.000, 6.000], mean observation: 0.220 [0.000, 3.000], loss: 885.025818, mean_absolute_error: 12.399704, mean_q: 12.566502
step 2115; T: 1149300; average reward: 12.47 - action: ne; reward: -5.00
 1283/2000: episode: 746, duration: 2.596s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.224 [0.000, 2.000], loss: 4456.808594, mean_absolute_error: 14.585833, mean_q: 12.898603
step 2120; T: 1150200; average reward: 12.40 - action: nw; reward: -46.60
 1286/2000: episode: 747, duration: 6.244s, episode steps: 3, steps per second: 0, episode reward: -66.596, mean reward: -22.199 [-46.596, -10.000], mean action: 5.333 [2.000, 7.000], mean observation: 0.308 [0.000, 4.000], loss: 600.489258, mean_absolute_error: 11.914500, mean_q: 15.298703
step 2122; T: 1150950; average reward: 12.36 - action: w; reward: -10.92
 1287/2000: episode: 748, duration: 2.330s, episode steps: 1, steps per second: 0, episode reward: -15.919, mean reward: -15.919 [-15.919, -15.919], mean action: 6.000 [6.000, 6.000], mean observation: 0.232 [0.000, 4.000], loss: 253.197708, mean_absolute_error: 11.161287, mean_q: 13.367892
step 2138; T: 1154400; average reward: 12.37 - action: w; reward: 16.74
 1295/2000: episode: 749, duration: 19.478s, episode steps: 8, steps per second: 0, episode reward: -38.260, mean reward: -4.782 [-10.000, 11.740], mean action: 3.500 [0.000, 7.000], mean observation: 0.258 [0.000, 3.000], loss: 883.659180, mean_absolute_error: 12.284515, mean_q: 15.680431
step 2139; T: 1155150; average reward: 12.29 - action: nw; reward: -49.08
 1296/2000: episode: 750, duration: 1.184s, episode steps: 1, steps per second: 1, episode reward: -49.079, mean reward: -49.079 [-49.079, -49.079], mean action: 7.000 [7.000, 7.000], mean observation: 0.240 [0.000, 3.000], loss: 443.195831, mean_absolute_error: 11.980717, mean_q: 16.970156
step 2150; T: 1157850; average reward: 12.27 - action: e; reward: -5.00
 1302/2000: episode: 751, duration: 13.663s, episode steps: 6, steps per second: 0, episode reward: -55.000, mean reward: -9.167 [-10.000, -5.000], mean action: 3.833 [2.000, 8.000], mean observation: 0.261 [0.000, 4.000], loss: 573.796387, mean_absolute_error: 11.676861, mean_q: 14.146735
step 2151; T: 1158150; average reward: 12.29 - action: e; reward: 31.90
 1303/2000: episode: 752, duration: 1.436s, episode steps: 1, steps per second: 1, episode reward: 31.896, mean reward: 31.896 [31.896, 31.896], mean action: 2.000 [2.000, 2.000], mean observation: 0.215 [0.000, 3.000], loss: 1298.129639, mean_absolute_error: 13.801933, mean_q: 17.166214
step 2157; T: 1159950; average reward: 12.27 - action: n; reward: -5.00
 1306/2000: episode: 753, duration: 7.644s, episode steps: 3, steps per second: 0, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.333 [5.000, 8.000], mean observation: 0.244 [0.000, 3.000], loss: 752.585754, mean_absolute_error: 12.282467, mean_q: 14.350358
step 2160; T: 1160700; average reward: 12.09 - action: nw; reward: -121.49
 1308/2000: episode: 754, duration: 3.554s, episode steps: 2, steps per second: 1, episode reward: -131.488, mean reward: -65.744 [-121.488, -10.000], mean action: 5.000 [3.000, 7.000], mean observation: 0.275 [0.000, 3.000], loss: 481.401428, mean_absolute_error: 12.440808, mean_q: 13.438353
step 2161; T: 1162800; average reward: 12.32 - action: nw; reward: 186.87
 1309/2000: episode: 755, duration: 1.649s, episode steps: 1, steps per second: 1, episode reward: 186.866, mean reward: 186.866 [186.866, 186.866], mean action: 7.000 [7.000, 7.000], mean observation: 0.395 [0.000, 3.000], loss: 524.772461, mean_absolute_error: 12.100489, mean_q: 19.216183
step 2162; T: 1163700; average reward: 12.26 - action: sw; reward: -37.64
 1310/2000: episode: 756, duration: 1.325s, episode steps: 1, steps per second: 1, episode reward: -37.645, mean reward: -37.645 [-37.645, -37.645], mean action: 5.000 [5.000, 5.000], mean observation: 0.317 [0.000, 3.000], loss: 442.936035, mean_absolute_error: 11.871942, mean_q: 13.812145
step 2163; T: 1164750; average reward: 12.23 - action: nw; reward: -10.50
 1311/2000: episode: 757, duration: 1.304s, episode steps: 1, steps per second: 1, episode reward: -10.496, mean reward: -10.496 [-10.496, -10.496], mean action: 7.000 [7.000, 7.000], mean observation: 0.259 [0.000, 2.000], loss: 473.608826, mean_absolute_error: 11.924421, mean_q: 10.329395
step 2166; T: 1165650; average reward: 12.08 - action: e; reward: -98.23
 1313/2000: episode: 758, duration: 4.036s, episode steps: 2, steps per second: 0, episode reward: -108.230, mean reward: -54.115 [-98.230, -10.000], mean action: 2.500 [2.000, 3.000], mean observation: 0.412 [0.000, 3.000], loss: 612.104614, mean_absolute_error: 11.413375, mean_q: 12.293389
step 2168; T: 1166700; average reward: 12.24 - action: nw; reward: 129.94
 1314/2000: episode: 759, duration: 2.649s, episode steps: 1, steps per second: 0, episode reward: 124.940, mean reward: 124.940 [124.940, 124.940], mean action: 7.000 [7.000, 7.000], mean observation: 0.267 [0.000, 2.000], loss: 737.226318, mean_absolute_error: 11.559907, mean_q: 13.250829
step 2172; T: 1167900; average reward: 12.27 - action: n; reward: 39.49
 1316/2000: episode: 760, duration: 4.917s, episode steps: 2, steps per second: 0, episode reward: 24.485, mean reward: 12.243 [-10.000, 34.485], mean action: 8.000 [8.000, 8.000], mean observation: 0.389 [0.000, 3.000], loss: 598.074768, mean_absolute_error: 11.903038, mean_q: 10.564276
step 2175; T: 1169250; average reward: 12.38 - action: w; reward: 91.20
 1318/2000: episode: 761, duration: 3.850s, episode steps: 2, steps per second: 1, episode reward: 81.199, mean reward: 40.599 [-10.000, 91.199], mean action: 4.500 [3.000, 6.000], mean observation: 0.244 [0.000, 2.000], loss: 2009.558594, mean_absolute_error: 14.402987, mean_q: 12.642654
step 2176; T: 1170300; average reward: 12.52 - action: se; reward: 121.10
 1319/2000: episode: 762, duration: 1.312s, episode steps: 1, steps per second: 1, episode reward: 121.102, mean reward: 121.102 [121.102, 121.102], mean action: 3.000 [3.000, 3.000], mean observation: 0.459 [0.000, 3.000], loss: 675.374207, mean_absolute_error: 12.108406, mean_q: 10.267803
step 2178; T: 1171800; average reward: 12.66 - action: w; reward: 121.81
 1320/2000: episode: 763, duration: 2.778s, episode steps: 1, steps per second: 0, episode reward: 116.811, mean reward: 116.811 [116.811, 116.811], mean action: 6.000 [6.000, 6.000], mean observation: 0.227 [0.000, 3.000], loss: 1499.710205, mean_absolute_error: 14.287457, mean_q: 16.354883
step 2192; T: 1174200; average reward: 12.53 - action: w; reward: -88.40
 1327/2000: episode: 764, duration: 17.360s, episode steps: 7, steps per second: 0, episode reward: -153.403, mean reward: -21.915 [-93.403, -10.000], mean action: 3.857 [1.000, 6.000], mean observation: 0.360 [0.000, 5.000], loss: 704.756653, mean_absolute_error: 12.412452, mean_q: 12.299911
step 2193; T: 1174950; average reward: 12.54 - action: e; reward: 21.97
 1328/2000: episode: 765, duration: 1.296s, episode steps: 1, steps per second: 1, episode reward: 21.967, mean reward: 21.967 [21.967, 21.967], mean action: 2.000 [2.000, 2.000], mean observation: 0.379 [0.000, 3.000], loss: 1463.043579, mean_absolute_error: 14.205454, mean_q: 13.685557
step 2194; T: 1176450; average reward: 12.52 - action: nw; reward: -5.00
 1329/2000: episode: 766, duration: 1.318s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.419 [0.000, 3.000], loss: 1158.229614, mean_absolute_error: 13.784962, mean_q: 16.448444
step 2197; T: 1179750; average reward: 12.36 - action: n; reward: -107.32
 1331/2000: episode: 767, duration: 3.947s, episode steps: 2, steps per second: 1, episode reward: -117.324, mean reward: -58.662 [-107.324, -10.000], mean action: 6.500 [5.000, 8.000], mean observation: 0.340 [0.000, 3.000], loss: 1147.726318, mean_absolute_error: 12.429546, mean_q: 12.990976
step 2198; T: 1180800; average reward: 12.34 - action: w; reward: -5.00
 1332/2000: episode: 768, duration: 1.302s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.357 [0.000, 3.000], loss: 784.226562, mean_absolute_error: 12.268458, mean_q: 12.894646
step 2200; T: 1181550; average reward: 12.30 - action: nw; reward: -15.03
 1333/2000: episode: 769, duration: 2.563s, episode steps: 1, steps per second: 0, episode reward: -20.025, mean reward: -20.025 [-20.025, -20.025], mean action: 7.000 [7.000, 7.000], mean observation: 0.283 [0.000, 3.000], loss: 851.866272, mean_absolute_error: 12.110184, mean_q: 15.074331
step 2203; T: 1182150; average reward: 12.28 - action: se; reward: -5.00
 1335/2000: episode: 770, duration: 3.839s, episode steps: 2, steps per second: 1, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.328 [0.000, 3.000], loss: 832.278809, mean_absolute_error: 11.624632, mean_q: 16.761496
step 2205; T: 1182900; average reward: 12.41 - action: nw; reward: 111.46
 1336/2000: episode: 771, duration: 2.293s, episode steps: 1, steps per second: 0, episode reward: 106.463, mean reward: 106.463 [106.463, 106.463], mean action: 7.000 [7.000, 7.000], mean observation: 0.453 [0.000, 3.000], loss: 625.562744, mean_absolute_error: 11.366367, mean_q: 14.294948
step 2206; T: 1187400; average reward: 12.44 - action: nw; reward: 36.63
 1337/2000: episode: 772, duration: 1.321s, episode steps: 1, steps per second: 1, episode reward: 36.626, mean reward: 36.626 [36.626, 36.626], mean action: 7.000 [7.000, 7.000], mean observation: 0.304 [0.000, 3.000], loss: 1398.462769, mean_absolute_error: 14.366133, mean_q: 15.323000
step 2208; T: 1188450; average reward: 12.46 - action: n; reward: 22.76
 1338/2000: episode: 773, duration: 2.328s, episode steps: 1, steps per second: 0, episode reward: 17.760, mean reward: 17.760 [17.760, 17.760], mean action: 8.000 [8.000, 8.000], mean observation: 0.440 [0.000, 3.000], loss: 697.640259, mean_absolute_error: 12.790705, mean_q: 14.274729
step 2214; T: 1190400; average reward: 12.51 - action: nw; reward: 54.90
 1341/2000: episode: 774, duration: 6.678s, episode steps: 3, steps per second: 0, episode reward: 29.901, mean reward: 9.967 [-10.000, 49.901], mean action: 5.667 [3.000, 7.000], mean observation: 0.428 [0.000, 3.000], loss: 1400.147461, mean_absolute_error: 12.343734, mean_q: 14.356438
step 2215; T: 1193400; average reward: 12.57 - action: w; reward: 61.85
 1342/2000: episode: 775, duration: 1.274s, episode steps: 1, steps per second: 1, episode reward: 61.851, mean reward: 61.851 [61.851, 61.851], mean action: 6.000 [6.000, 6.000], mean observation: 0.315 [0.000, 2.000], loss: 396.584381, mean_absolute_error: 12.231036, mean_q: 17.533638
step 2218; T: 1195200; average reward: 12.55 - action: nw; reward: -5.00
 1344/2000: episode: 776, duration: 3.477s, episode steps: 2, steps per second: 1, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.320 [0.000, 3.000], loss: 424.710388, mean_absolute_error: 12.003958, mean_q: 15.269305
step 2224; T: 1196850; average reward: 12.62 - action: nw; reward: 64.02
 1347/2000: episode: 777, duration: 6.912s, episode steps: 3, steps per second: 0, episode reward: 39.016, mean reward: 13.005 [-10.000, 59.016], mean action: 3.667 [1.000, 7.000], mean observation: 0.312 [0.000, 3.000], loss: 801.712402, mean_absolute_error: 13.402667, mean_q: 14.916543
step 2225; T: 1197450; average reward: 12.61 - action: e; reward: 7.47
 1348/2000: episode: 778, duration: 1.113s, episode steps: 1, steps per second: 1, episode reward: 7.470, mean reward: 7.470 [7.470, 7.470], mean action: 2.000 [2.000, 2.000], mean observation: 0.296 [0.000, 2.000], loss: 1199.376099, mean_absolute_error: 12.707293, mean_q: 13.070202
step 2227; T: 1198350; average reward: 12.59 - action: n; reward: -5.00
 1349/2000: episode: 779, duration: 2.340s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.405 [0.000, 3.000], loss: 1117.784546, mean_absolute_error: 11.913432, mean_q: 13.974767
step 2229; T: 1199550; average reward: 12.58 - action: nw; reward: 7.38
 1350/2000: episode: 780, duration: 2.273s, episode steps: 1, steps per second: 0, episode reward: 2.376, mean reward: 2.376 [2.376, 2.376], mean action: 7.000 [7.000, 7.000], mean observation: 0.363 [0.000, 3.000], loss: 266.250183, mean_absolute_error: 10.563258, mean_q: 9.522020
step 2237; T: 1201650; average reward: 12.56 - action: ne; reward: -5.00
 1354/2000: episode: 781, duration: 8.878s, episode steps: 4, steps per second: 0, episode reward: -40.000, mean reward: -10.000 [-10.000, -10.000], mean action: 3.250 [1.000, 6.000], mean observation: 0.234 [0.000, 3.000], loss: 1170.557495, mean_absolute_error: 13.169741, mean_q: 13.202527
step 2238; T: 1202400; average reward: 12.59 - action: ne; reward: 38.45
 1355/2000: episode: 782, duration: 1.299s, episode steps: 1, steps per second: 1, episode reward: 38.449, mean reward: 38.449 [38.449, 38.449], mean action: 1.000 [1.000, 1.000], mean observation: 0.435 [0.000, 3.000], loss: 318.957397, mean_absolute_error: 13.043964, mean_q: 12.282808
step 2239; T: 1203300; average reward: 12.61 - action: n; reward: 29.16
 1356/2000: episode: 783, duration: 1.174s, episode steps: 1, steps per second: 1, episode reward: 29.161, mean reward: 29.161 [29.161, 29.161], mean action: 8.000 [8.000, 8.000], mean observation: 0.232 [0.000, 2.000], loss: 1091.092529, mean_absolute_error: 13.342472, mean_q: 11.781956
step 2256; T: 1207050; average reward: 12.59 - action: nw; reward: -5.00
 1365/2000: episode: 784, duration: 19.297s, episode steps: 9, steps per second: 0, episode reward: -85.000, mean reward: -9.444 [-10.000, -5.000], mean action: 6.333 [6.000, 7.000], mean observation: 0.228 [0.000, 3.000], loss: 774.406433, mean_absolute_error: 13.358015, mean_q: 13.293640
step 2258; T: 1209150; average reward: 12.64 - action: n; reward: 49.89
 1366/2000: episode: 785, duration: 2.361s, episode steps: 1, steps per second: 0, episode reward: 44.886, mean reward: 44.886 [44.886, 44.886], mean action: 8.000 [8.000, 8.000], mean observation: 0.269 [0.000, 2.000], loss: 312.340454, mean_absolute_error: 12.849089, mean_q: 13.303627
step 2259; T: 1210650; average reward: 12.62 - action: nw; reward: -5.00
 1367/2000: episode: 786, duration: 1.306s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.373 [0.000, 3.000], loss: 771.732422, mean_absolute_error: 12.476728, mean_q: 15.828387
step 2261; T: 1211100; average reward: 12.57 - action: w; reward: -19.54
 1368/2000: episode: 787, duration: 2.236s, episode steps: 1, steps per second: 0, episode reward: -24.537, mean reward: -24.537 [-24.537, -24.537], mean action: 6.000 [6.000, 6.000], mean observation: 0.360 [0.000, 5.000], loss: 541.607178, mean_absolute_error: 11.601818, mean_q: 12.618139
step 2262; T: 1211550; average reward: 12.58 - action: nw; reward: 14.07
 1369/2000: episode: 788, duration: 1.189s, episode steps: 1, steps per second: 1, episode reward: 14.075, mean reward: 14.075 [14.075, 14.075], mean action: 7.000 [7.000, 7.000], mean observation: 0.437 [0.000, 5.000], loss: 1292.559692, mean_absolute_error: 15.198881, mean_q: 11.521461
step 2272; T: 1213950; average reward: 12.55 - action: nw; reward: -5.00
 1374/2000: episode: 789, duration: 11.466s, episode steps: 5, steps per second: 0, episode reward: -50.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.000 [1.000, 7.000], mean observation: 0.271 [0.000, 4.000], loss: 1183.266846, mean_absolute_error: 13.484141, mean_q: 13.367305
step 2281; T: 1216650; average reward: 12.70 - action: w; reward: 125.02
 1379/2000: episode: 790, duration: 10.441s, episode steps: 5, steps per second: 0, episode reward: 85.017, mean reward: 17.003 [-10.000, 125.017], mean action: 3.600 [1.000, 7.000], mean observation: 0.319 [0.000, 4.000], loss: 972.685364, mean_absolute_error: 13.662374, mean_q: 14.031774
step 2282; T: 1219050; average reward: 12.66 - action: nw; reward: -14.14
 1380/2000: episode: 791, duration: 1.244s, episode steps: 1, steps per second: 1, episode reward: -14.138, mean reward: -14.138 [-14.138, -14.138], mean action: 7.000 [7.000, 7.000], mean observation: 0.277 [0.000, 3.000], loss: 1003.894958, mean_absolute_error: 12.424940, mean_q: 12.461508
step 2289; T: 1221450; average reward: 12.73 - action: nw; reward: 63.79
 1384/2000: episode: 792, duration: 8.150s, episode steps: 4, steps per second: 0, episode reward: 33.786, mean reward: 8.446 [-10.000, 63.786], mean action: 3.500 [1.000, 7.000], mean observation: 0.321 [0.000, 4.000], loss: 1160.268188, mean_absolute_error: 13.077343, mean_q: 12.196795
step 2293; T: 1222650; average reward: 12.70 - action: w; reward: -5.00
 1386/2000: episode: 793, duration: 4.534s, episode steps: 2, steps per second: 0, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.307 [0.000, 3.000], loss: 683.092590, mean_absolute_error: 12.670544, mean_q: 10.575291
step 2295; T: 1225350; average reward: 12.73 - action: w; reward: 35.24
 1387/2000: episode: 794, duration: 2.330s, episode steps: 1, steps per second: 0, episode reward: 30.238, mean reward: 30.238 [30.238, 30.238], mean action: 6.000 [6.000, 6.000], mean observation: 0.165 [0.000, 2.000], loss: 1073.455566, mean_absolute_error: 14.787408, mean_q: 15.023214
step 2297; T: 1226400; average reward: 12.71 - action: se; reward: -5.00
 1388/2000: episode: 795, duration: 2.262s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.331 [0.000, 5.000], loss: 188.171875, mean_absolute_error: 12.040099, mean_q: 9.492958
step 2298; T: 1227600; average reward: 12.70 - action: w; reward: 4.13
 1389/2000: episode: 796, duration: 1.130s, episode steps: 1, steps per second: 1, episode reward: 4.133, mean reward: 4.133 [4.133, 4.133], mean action: 6.000 [6.000, 6.000], mean observation: 0.301 [0.000, 3.000], loss: 1286.859375, mean_absolute_error: 12.386597, mean_q: 12.309015
step 2301; T: 1228800; average reward: 12.99 - action: w; reward: 240.26
 1391/2000: episode: 797, duration: 3.504s, episode steps: 2, steps per second: 1, episode reward: 230.258, mean reward: 115.129 [-10.000, 240.258], mean action: 6.500 [6.000, 7.000], mean observation: 0.356 [0.000, 3.000], loss: 1723.346924, mean_absolute_error: 12.943268, mean_q: 10.941482
step 2315; T: 1231050; average reward: 12.96 - action: e; reward: -5.00
 1398/2000: episode: 798, duration: 16.369s, episode steps: 7, steps per second: 0, episode reward: -70.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.857 [2.000, 8.000], mean observation: 0.390 [0.000, 4.000], loss: 896.839050, mean_absolute_error: 12.926695, mean_q: 12.403527
step 2318; T: 1231950; average reward: 12.81 - action: w; reward: -109.04
 1400/2000: episode: 799, duration: 3.463s, episode steps: 2, steps per second: 1, episode reward: -119.036, mean reward: -59.518 [-109.036, -10.000], mean action: 4.500 [3.000, 6.000], mean observation: 0.421 [0.000, 4.000], loss: 839.836365, mean_absolute_error: 11.335062, mean_q: 11.645441
step 2323; T: 1233300; average reward: 12.79 - action: e; reward: -5.00
 1403/2000: episode: 800, duration: 5.826s, episode steps: 3, steps per second: 1, episode reward: -25.000, mean reward: -8.333 [-10.000, -5.000], mean action: 4.000 [2.000, 7.000], mean observation: 0.327 [0.000, 5.000], loss: 795.098633, mean_absolute_error: 13.286456, mean_q: 13.707524
step 2325; T: 1235100; average reward: 12.77 - action: w; reward: -5.00
 1404/2000: episode: 801, duration: 2.578s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.251 [0.000, 3.000], loss: 1465.706055, mean_absolute_error: 13.725342, mean_q: 11.826902
step 2328; T: 1236150; average reward: 12.82 - action: e; reward: 54.83
 1406/2000: episode: 802, duration: 3.583s, episode steps: 2, steps per second: 1, episode reward: 44.826, mean reward: 22.413 [-10.000, 54.826], mean action: 5.000 [2.000, 8.000], mean observation: 0.343 [0.000, 3.000], loss: 852.320312, mean_absolute_error: 12.606416, mean_q: 13.423685
step 2330; T: 1236750; average reward: 12.74 - action: sw; reward: -47.24
 1407/2000: episode: 803, duration: 2.482s, episode steps: 1, steps per second: 0, episode reward: -52.241, mean reward: -52.241 [-52.241, -52.241], mean action: 5.000 [5.000, 5.000], mean observation: 0.357 [0.000, 5.000], loss: 999.555420, mean_absolute_error: 13.779987, mean_q: 13.911365
step 2332; T: 1237500; average reward: 12.77 - action: w; reward: 35.16
 1408/2000: episode: 804, duration: 2.269s, episode steps: 1, steps per second: 0, episode reward: 30.156, mean reward: 30.156 [30.156, 30.156], mean action: 6.000 [6.000, 6.000], mean observation: 0.416 [0.000, 3.000], loss: 692.624023, mean_absolute_error: 13.374008, mean_q: 10.969681
step 2335; T: 1239750; average reward: 12.75 - action: se; reward: -5.00
 1410/2000: episode: 805, duration: 3.603s, episode steps: 2, steps per second: 1, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.256 [0.000, 3.000], loss: 608.369995, mean_absolute_error: 11.926120, mean_q: 11.072632
step 2338; T: 1241100; average reward: 12.89 - action: w; reward: 129.64
 1412/2000: episode: 806, duration: 3.614s, episode steps: 2, steps per second: 1, episode reward: 119.645, mean reward: 59.822 [-10.000, 129.645], mean action: 6.500 [6.000, 7.000], mean observation: 0.393 [0.000, 3.000], loss: 843.862061, mean_absolute_error: 12.168623, mean_q: 11.654686
step 2341; T: 1242150; average reward: 12.84 - action: nw; reward: -27.65
 1414/2000: episode: 807, duration: 3.560s, episode steps: 2, steps per second: 1, episode reward: -37.648, mean reward: -18.824 [-27.648, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.349 [0.000, 3.000], loss: 769.490051, mean_absolute_error: 12.310722, mean_q: 12.058350
step 2342; T: 1244700; average reward: 12.86 - action: se; reward: 29.06
 1415/2000: episode: 808, duration: 1.487s, episode steps: 1, steps per second: 1, episode reward: 29.060, mean reward: 29.060 [29.060, 29.060], mean action: 3.000 [3.000, 3.000], mean observation: 0.213 [0.000, 3.000], loss: 696.768921, mean_absolute_error: 11.644028, mean_q: 13.084724
step 2344; T: 1246050; average reward: 12.82 - action: ne; reward: -19.61
 1416/2000: episode: 809, duration: 2.526s, episode steps: 1, steps per second: 0, episode reward: -24.606, mean reward: -24.606 [-24.606, -24.606], mean action: 1.000 [1.000, 1.000], mean observation: 0.256 [0.000, 4.000], loss: 532.014404, mean_absolute_error: 13.209936, mean_q: 15.694960
step 2345; T: 1246950; average reward: 12.80 - action: w; reward: -7.80
 1417/2000: episode: 810, duration: 1.247s, episode steps: 1, steps per second: 1, episode reward: -7.803, mean reward: -7.803 [-7.803, -7.803], mean action: 6.000 [6.000, 6.000], mean observation: 0.227 [0.000, 3.000], loss: 534.924561, mean_absolute_error: 11.726898, mean_q: 11.237361
step 2353; T: 1248600; average reward: 12.78 - action: w; reward: -5.00
 1421/2000: episode: 811, duration: 9.202s, episode steps: 4, steps per second: 0, episode reward: -40.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.000 [2.000, 8.000], mean observation: 0.326 [0.000, 3.000], loss: 662.660889, mean_absolute_error: 12.619832, mean_q: 11.335066
step 2355; T: 1249500; average reward: 12.75 - action: n; reward: -5.00
 1422/2000: episode: 812, duration: 2.303s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.459 [0.000, 3.000], loss: 1285.181763, mean_absolute_error: 13.434647, mean_q: 10.879083
step 2356; T: 1251000; average reward: 12.81 - action: noop; reward: 59.32
 1423/2000: episode: 813, duration: 1.210s, episode steps: 1, steps per second: 1, episode reward: 59.324, mean reward: 59.324 [59.324, 59.324], mean action: 0.000 [0.000, 0.000], mean observation: 0.384 [0.000, 4.000], loss: 732.225708, mean_absolute_error: 13.202690, mean_q: 11.925575
step 2364; T: 1252800; average reward: 12.74 - action: se; reward: -50.00
 1427/2000: episode: 814, duration: 10.199s, episode steps: 4, steps per second: 0, episode reward: -74.995, mean reward: -18.749 [-54.995, 0.000], mean action: 4.250 [0.000, 7.000], mean observation: 0.440 [0.000, 6.000], loss: 641.288086, mean_absolute_error: 12.517504, mean_q: 13.102694
step 2365; T: 1255050; average reward: 12.80 - action: nw; reward: 65.46
 1428/2000: episode: 815, duration: 1.377s, episode steps: 1, steps per second: 1, episode reward: 65.458, mean reward: 65.458 [65.458, 65.458], mean action: 7.000 [7.000, 7.000], mean observation: 0.264 [0.000, 2.000], loss: 605.521606, mean_absolute_error: 11.734112, mean_q: 9.700378
step 2368; T: 1257600; average reward: 12.78 - action: e; reward: -5.00
 1430/2000: episode: 816, duration: 3.989s, episode steps: 2, steps per second: 1, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 4.000 [2.000, 6.000], mean observation: 0.277 [0.000, 3.000], loss: 850.108887, mean_absolute_error: 13.598014, mean_q: 13.776514
step 2369; T: 1258350; average reward: 12.81 - action: nw; reward: 38.96
 1431/2000: episode: 817, duration: 1.205s, episode steps: 1, steps per second: 1, episode reward: 38.963, mean reward: 38.963 [38.963, 38.963], mean action: 7.000 [7.000, 7.000], mean observation: 0.280 [0.000, 3.000], loss: 1003.919739, mean_absolute_error: 14.232309, mean_q: 14.981974
step 2372; T: 1259550; average reward: 12.76 - action: nw; reward: -27.65
 1433/2000: episode: 818, duration: 3.661s, episode steps: 2, steps per second: 1, episode reward: -37.648, mean reward: -18.824 [-27.648, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.335 [0.000, 3.000], loss: 1112.732422, mean_absolute_error: 13.827633, mean_q: 11.633806
step 2375; T: 1261050; average reward: 12.70 - action: se; reward: -36.70
 1435/2000: episode: 819, duration: 3.729s, episode steps: 2, steps per second: 1, episode reward: -46.704, mean reward: -23.352 [-36.704, -10.000], mean action: 5.000 [3.000, 7.000], mean observation: 0.337 [0.000, 3.000], loss: 725.440918, mean_absolute_error: 12.548777, mean_q: 11.619103
step 2377; T: 1261800; average reward: 12.64 - action: nw; reward: -38.20
 1436/2000: episode: 820, duration: 2.439s, episode steps: 1, steps per second: 0, episode reward: -43.195, mean reward: -43.195 [-43.195, -43.195], mean action: 7.000 [7.000, 7.000], mean observation: 0.349 [0.000, 5.000], loss: 311.835266, mean_absolute_error: 12.587027, mean_q: 9.695746
step 2380; T: 1263000; average reward: 12.62 - action: nw; reward: -5.00
 1438/2000: episode: 821, duration: 3.698s, episode steps: 2, steps per second: 1, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.236 [0.000, 4.000], loss: 712.298767, mean_absolute_error: 13.044333, mean_q: 10.226449
step 2386; T: 1265250; average reward: 12.65 - action: se; reward: 39.13
 1441/2000: episode: 822, duration: 7.168s, episode steps: 3, steps per second: 0, episode reward: 24.133, mean reward: 8.044 [-10.000, 34.133], mean action: 3.333 [0.000, 7.000], mean observation: 0.243 [0.000, 3.000], loss: 533.708801, mean_absolute_error: 12.274213, mean_q: 11.681367
step 2387; T: 1266150; average reward: 12.73 - action: ne; reward: 78.06
 1442/2000: episode: 823, duration: 1.321s, episode steps: 1, steps per second: 1, episode reward: 78.058, mean reward: 78.058 [78.058, 78.058], mean action: 1.000 [1.000, 1.000], mean observation: 0.243 [0.000, 3.000], loss: 289.303223, mean_absolute_error: 10.583771, mean_q: 10.763550
step 2389; T: 1267200; average reward: 12.51 - action: se; reward: -164.64
 1443/2000: episode: 824, duration: 2.371s, episode steps: 1, steps per second: 0, episode reward: -169.636, mean reward: -169.636 [-169.636, -169.636], mean action: 3.000 [3.000, 3.000], mean observation: 0.251 [0.000, 2.000], loss: 1043.825195, mean_absolute_error: 12.584856, mean_q: 11.182180
step 2390; T: 1270500; average reward: 12.51 - action: e; reward: 11.10
 1444/2000: episode: 825, duration: 1.411s, episode steps: 1, steps per second: 1, episode reward: 11.103, mean reward: 11.103 [11.103, 11.103], mean action: 2.000 [2.000, 2.000], mean observation: 0.243 [0.000, 3.000], loss: 580.760681, mean_absolute_error: 13.420784, mean_q: 10.407555
step 2391; T: 1271550; average reward: 12.45 - action: ne; reward: -39.39
 1445/2000: episode: 826, duration: 1.183s, episode steps: 1, steps per second: 1, episode reward: -39.390, mean reward: -39.390 [-39.390, -39.390], mean action: 1.000 [1.000, 1.000], mean observation: 0.285 [0.000, 3.000], loss: 340.816162, mean_absolute_error: 11.917424, mean_q: 12.345051
step 2393; T: 1272750; average reward: 12.36 - action: s; reward: -60.03
 1446/2000: episode: 827, duration: 2.475s, episode steps: 1, steps per second: 0, episode reward: -65.033, mean reward: -65.033 [-65.033, -65.033], mean action: 4.000 [4.000, 4.000], mean observation: 0.259 [0.000, 2.000], loss: 830.222839, mean_absolute_error: 13.722322, mean_q: 9.679859
step 2395; T: 1273500; average reward: 12.34 - action: w; reward: -5.00
 1447/2000: episode: 828, duration: 2.498s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.475 [0.000, 4.000], loss: 714.302002, mean_absolute_error: 11.589939, mean_q: 11.811062
step 2396; T: 1274550; average reward: 12.27 - action: s; reward: -47.40
 1448/2000: episode: 829, duration: 1.259s, episode steps: 1, steps per second: 1, episode reward: -47.398, mean reward: -47.398 [-47.398, -47.398], mean action: 4.000 [4.000, 4.000], mean observation: 0.435 [0.000, 3.000], loss: 593.729919, mean_absolute_error: 13.383096, mean_q: 13.663838
step 2397; T: 1276800; average reward: 12.25 - action: n; reward: -2.67
 1449/2000: episode: 830, duration: 1.323s, episode steps: 1, steps per second: 1, episode reward: -2.666, mean reward: -2.666 [-2.666, -2.666], mean action: 8.000 [8.000, 8.000], mean observation: 0.288 [0.000, 2.000], loss: 619.076538, mean_absolute_error: 14.278517, mean_q: 9.297304
step 2400; T: 1279050; average reward: 12.24 - action: w; reward: 1.03
 1451/2000: episode: 831, duration: 3.563s, episode steps: 2, steps per second: 1, episode reward: -8.967, mean reward: -4.484 [-10.000, 1.033], mean action: 7.000 [6.000, 8.000], mean observation: 0.453 [0.000, 3.000], loss: 411.087769, mean_absolute_error: 13.039564, mean_q: 11.815893
step 2401; T: 1282800; average reward: 12.11 - action: n; reward: -96.30
 1452/2000: episode: 832, duration: 1.313s, episode steps: 1, steps per second: 1, episode reward: -96.296, mean reward: -96.296 [-96.296, -96.296], mean action: 8.000 [8.000, 8.000], mean observation: 0.259 [0.000, 3.000], loss: 737.007568, mean_absolute_error: 13.037972, mean_q: 14.637373
step 2403; T: 1283850; average reward: 11.99 - action: w; reward: -85.25
 1453/2000: episode: 833, duration: 2.349s, episode steps: 1, steps per second: 0, episode reward: -90.250, mean reward: -90.250 [-90.250, -90.250], mean action: 6.000 [6.000, 6.000], mean observation: 0.237 [0.000, 3.000], loss: 382.971802, mean_absolute_error: 12.663876, mean_q: 14.283583
step 2406; T: 1284900; average reward: 11.96 - action: ne; reward: -11.33
 1455/2000: episode: 834, duration: 3.608s, episode steps: 2, steps per second: 1, episode reward: -21.330, mean reward: -10.665 [-11.330, -10.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.321 [0.000, 3.000], loss: 826.030579, mean_absolute_error: 13.823865, mean_q: 12.790311
step 2407; T: 1285800; average reward: 11.94 - action: w; reward: -5.00
 1456/2000: episode: 835, duration: 1.389s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.325 [0.000, 5.000], loss: 539.292114, mean_absolute_error: 14.495945, mean_q: 12.449278
step 2411; T: 1289550; average reward: 11.92 - action: n; reward: -5.00
 1458/2000: episode: 836, duration: 5.242s, episode steps: 2, steps per second: 0, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [6.000, 8.000], mean observation: 0.292 [0.000, 4.000], loss: 1000.352661, mean_absolute_error: 14.237843, mean_q: 12.394012
step 2412; T: 1291050; average reward: 11.95 - action: n; reward: 39.93
 1459/2000: episode: 837, duration: 1.278s, episode steps: 1, steps per second: 1, episode reward: 39.929, mean reward: 39.929 [39.929, 39.929], mean action: 8.000 [8.000, 8.000], mean observation: 0.149 [0.000, 2.000], loss: 134.926804, mean_absolute_error: 11.727131, mean_q: 11.790114
step 2414; T: 1292250; average reward: 12.03 - action: w; reward: 73.12
 1460/2000: episode: 838, duration: 2.425s, episode steps: 1, steps per second: 0, episode reward: 68.122, mean reward: 68.122 [68.122, 68.122], mean action: 6.000 [6.000, 6.000], mean observation: 0.219 [0.000, 2.000], loss: 734.034912, mean_absolute_error: 12.713972, mean_q: 15.594985
step 2416; T: 1293450; average reward: 12.03 - action: n; reward: 18.38
 1461/2000: episode: 839, duration: 3.236s, episode steps: 1, steps per second: 0, episode reward: 13.378, mean reward: 13.378 [13.378, 13.378], mean action: 8.000 [8.000, 8.000], mean observation: 0.325 [0.000, 3.000], loss: 154.396088, mean_absolute_error: 12.359543, mean_q: 8.872020
step 2417; T: 1296000; average reward: 12.01 - action: w; reward: -5.00
 1462/2000: episode: 840, duration: 1.509s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.400 [0.000, 3.000], loss: 872.936340, mean_absolute_error: 13.725201, mean_q: 12.758847
step 2420; T: 1297950; average reward: 12.00 - action: n; reward: -0.28
 1464/2000: episode: 841, duration: 6.064s, episode steps: 2, steps per second: 0, episode reward: -10.280, mean reward: -5.140 [-10.000, -0.280], mean action: 5.500 [3.000, 8.000], mean observation: 0.239 [0.000, 3.000], loss: 849.261841, mean_absolute_error: 13.702616, mean_q: 11.190938
step 2424; T: 1303050; average reward: 11.98 - action: nw; reward: -5.00
 1466/2000: episode: 842, duration: 6.522s, episode steps: 2, steps per second: 0, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 4.000 [1.000, 7.000], mean observation: 0.364 [0.000, 3.000], loss: 606.554077, mean_absolute_error: 13.222399, mean_q: 12.551579
step 2425; T: 1303500; average reward: 11.97 - action: w; reward: 7.91
 1467/2000: episode: 843, duration: 1.522s, episode steps: 1, steps per second: 1, episode reward: 7.908, mean reward: 7.908 [7.908, 7.908], mean action: 6.000 [6.000, 6.000], mean observation: 0.413 [0.000, 3.000], loss: 795.778259, mean_absolute_error: 12.827177, mean_q: 12.575344
step 2426; T: 1307550; average reward: 11.98 - action: w; reward: 19.29
 1468/2000: episode: 844, duration: 1.803s, episode steps: 1, steps per second: 1, episode reward: 19.287, mean reward: 19.287 [19.287, 19.287], mean action: 6.000 [6.000, 6.000], mean observation: 0.347 [0.000, 3.000], loss: 741.764893, mean_absolute_error: 13.910221, mean_q: 11.196643
step 2427; T: 1308000; average reward: 11.96 - action: w; reward: -5.00
 1469/2000: episode: 845, duration: 1.323s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.235 [0.000, 2.000], loss: 965.083923, mean_absolute_error: 13.853125, mean_q: 14.198967
step 2434; T: 1310100; average reward: 11.94 - action: ne; reward: -5.00
 1473/2000: episode: 846, duration: 12.179s, episode steps: 4, steps per second: 0, episode reward: -35.000, mean reward: -8.750 [-10.000, -5.000], mean action: 2.750 [1.000, 6.000], mean observation: 0.236 [0.000, 3.000], loss: 626.907288, mean_absolute_error: 13.100163, mean_q: 12.553084
step 2436; T: 1313400; average reward: 11.99 - action: w; reward: 54.12
 1474/2000: episode: 847, duration: 3.119s, episode steps: 1, steps per second: 0, episode reward: 49.122, mean reward: 49.122 [49.122, 49.122], mean action: 6.000 [6.000, 6.000], mean observation: 0.328 [0.000, 3.000], loss: 579.314636, mean_absolute_error: 12.352818, mean_q: 14.536905
step 2437; T: 1315200; average reward: 11.97 - action: w; reward: -5.00
 1475/2000: episode: 848, duration: 2.502s, episode steps: 1, steps per second: 0, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.352 [0.000, 2.000], loss: 809.803345, mean_absolute_error: 12.625140, mean_q: 14.056246
step 2438; T: 1316400; average reward: 11.98 - action: w; reward: 16.47
 1476/2000: episode: 849, duration: 2.292s, episode steps: 1, steps per second: 0, episode reward: 16.474, mean reward: 16.474 [16.474, 16.474], mean action: 6.000 [6.000, 6.000], mean observation: 0.277 [0.000, 3.000], loss: 783.050720, mean_absolute_error: 12.575854, mean_q: 15.474358
step 2445; T: 1320750; average reward: 12.11 - action: nw; reward: 127.91
 1480/2000: episode: 850, duration: 13.391s, episode steps: 4, steps per second: 0, episode reward: 97.914, mean reward: 24.478 [-10.000, 127.914], mean action: 7.500 [7.000, 8.000], mean observation: 0.439 [0.000, 4.000], loss: 591.907471, mean_absolute_error: 12.578516, mean_q: 13.153855
step 2446; T: 1321050; average reward: 12.17 - action: w; reward: 57.37
 1481/2000: episode: 851, duration: 1.379s, episode steps: 1, steps per second: 1, episode reward: 57.375, mean reward: 57.375 [57.375, 57.375], mean action: 6.000 [6.000, 6.000], mean observation: 0.253 [0.000, 4.000], loss: 1815.157227, mean_absolute_error: 16.001278, mean_q: 14.993953
step 2447; T: 1321650; average reward: 12.19 - action: ne; reward: 33.76
 1482/2000: episode: 852, duration: 1.819s, episode steps: 1, steps per second: 1, episode reward: 33.758, mean reward: 33.758 [33.758, 33.758], mean action: 1.000 [1.000, 1.000], mean observation: 0.243 [0.000, 4.000], loss: 978.032227, mean_absolute_error: 13.720135, mean_q: 13.896311
step 2457; T: 1323300; average reward: 12.26 - action: nw; reward: 65.18
 1487/2000: episode: 853, duration: 18.598s, episode steps: 5, steps per second: 0, episode reward: 20.183, mean reward: 4.037 [-10.000, 60.183], mean action: 5.600 [2.000, 7.000], mean observation: 0.254 [0.000, 4.000], loss: 574.922546, mean_absolute_error: 12.660823, mean_q: 14.735334
step 2459; T: 1324800; average reward: 12.22 - action: w; reward: -20.79
 1488/2000: episode: 854, duration: 3.146s, episode steps: 1, steps per second: 0, episode reward: -25.791, mean reward: -25.791 [-25.791, -25.791], mean action: 6.000 [6.000, 6.000], mean observation: 0.317 [0.000, 3.000], loss: 1363.020142, mean_absolute_error: 13.180868, mean_q: 13.322415
step 2462; T: 1328850; average reward: 12.19 - action: w; reward: -9.86
 1490/2000: episode: 855, duration: 5.324s, episode steps: 2, steps per second: 0, episode reward: -19.855, mean reward: -9.928 [-10.000, -9.855], mean action: 6.000 [6.000, 6.000], mean observation: 0.216 [0.000, 3.000], loss: 1681.796387, mean_absolute_error: 13.134064, mean_q: 15.576210
step 2465; T: 1329750; average reward: 12.17 - action: nw; reward: -5.00
 1492/2000: episode: 856, duration: 6.089s, episode steps: 2, steps per second: 0, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 6.500 [6.000, 7.000], mean observation: 0.335 [0.000, 3.000], loss: 707.357788, mean_absolute_error: 13.292826, mean_q: 15.640367
step 2466; T: 1330800; average reward: 12.25 - action: nw; reward: 77.37
 1493/2000: episode: 857, duration: 1.636s, episode steps: 1, steps per second: 1, episode reward: 77.366, mean reward: 77.366 [77.366, 77.366], mean action: 7.000 [7.000, 7.000], mean observation: 0.251 [0.000, 2.000], loss: 381.181061, mean_absolute_error: 12.885747, mean_q: 16.510025
step 2470; T: 1333500; average reward: 12.28 - action: nw; reward: 39.08
 1495/2000: episode: 858, duration: 5.838s, episode steps: 2, steps per second: 0, episode reward: 24.081, mean reward: 12.041 [-10.000, 34.081], mean action: 6.500 [6.000, 7.000], mean observation: 0.357 [0.000, 4.000], loss: 457.098938, mean_absolute_error: 12.663248, mean_q: 14.407811
step 2476; T: 1336050; average reward: 12.36 - action: nw; reward: 80.01
 1498/2000: episode: 859, duration: 8.399s, episode steps: 3, steps per second: 0, episode reward: 55.015, mean reward: 18.338 [-10.000, 75.015], mean action: 4.667 [1.000, 7.000], mean observation: 0.355 [0.000, 3.000], loss: 648.497925, mean_absolute_error: 13.219322, mean_q: 15.475438
step 2480; T: 1338450; average reward: 12.34 - action: e; reward: -5.00
 1500/2000: episode: 860, duration: 5.284s, episode steps: 2, steps per second: 0, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 4.000 [2.000, 6.000], mean observation: 0.233 [0.000, 4.000], loss: 610.598694, mean_absolute_error: 13.240089, mean_q: 15.968256
step 2482; T: 1340400; average reward: 12.31 - action: n; reward: -10.29
 1501/2000: episode: 861, duration: 2.709s, episode steps: 1, steps per second: 0, episode reward: -15.288, mean reward: -15.288 [-15.288, -15.288], mean action: 8.000 [8.000, 8.000], mean observation: 0.264 [0.000, 3.000], loss: 239.390228, mean_absolute_error: 12.600349, mean_q: 15.022537
step 2485; T: 1341600; average reward: 12.32 - action: nw; reward: 17.38
 1503/2000: episode: 862, duration: 4.002s, episode steps: 2, steps per second: 0, episode reward: 7.381, mean reward: 3.691 [-10.000, 17.381], mean action: 4.000 [1.000, 7.000], mean observation: 0.393 [0.000, 3.000], loss: 1223.495850, mean_absolute_error: 14.049680, mean_q: 12.696766
step 2486; T: 1342200; average reward: 12.32 - action: nw; reward: 15.74
 1504/2000: episode: 863, duration: 1.329s, episode steps: 1, steps per second: 1, episode reward: 15.738, mean reward: 15.738 [15.738, 15.738], mean action: 7.000 [7.000, 7.000], mean observation: 0.395 [0.000, 3.000], loss: 286.165253, mean_absolute_error: 13.137239, mean_q: 14.863452
step 2487; T: 1343100; average reward: 12.31 - action: nw; reward: 3.94
 1505/2000: episode: 864, duration: 1.716s, episode steps: 1, steps per second: 1, episode reward: 3.937, mean reward: 3.937 [3.937, 3.937], mean action: 7.000 [7.000, 7.000], mean observation: 0.245 [0.000, 3.000], loss: 989.906128, mean_absolute_error: 15.778624, mean_q: 16.748840
step 2490; T: 1344300; average reward: 12.18 - action: w; reward: -102.88
 1507/2000: episode: 865, duration: 4.497s, episode steps: 2, steps per second: 0, episode reward: -112.878, mean reward: -56.439 [-102.878, -10.000], mean action: 4.500 [3.000, 6.000], mean observation: 0.325 [0.000, 4.000], loss: 1101.186157, mean_absolute_error: 13.978123, mean_q: 12.338034
step 2491; T: 1345200; average reward: 12.17 - action: w; reward: 8.87
 1508/2000: episode: 866, duration: 1.456s, episode steps: 1, steps per second: 1, episode reward: 8.875, mean reward: 8.875 [8.875, 8.875], mean action: 6.000 [6.000, 6.000], mean observation: 0.235 [0.000, 3.000], loss: 846.936035, mean_absolute_error: 14.162766, mean_q: 11.693069
step 2492; T: 1345650; average reward: 12.07 - action: ne; reward: -77.49
 1509/2000: episode: 867, duration: 1.694s, episode steps: 1, steps per second: 1, episode reward: -77.490, mean reward: -77.490 [-77.490, -77.490], mean action: 1.000 [1.000, 1.000], mean observation: 0.229 [0.000, 3.000], loss: 275.174011, mean_absolute_error: 14.429556, mean_q: 12.837301
step 2495; T: 1346700; average reward: 12.12 - action: e; reward: 53.41
 1511/2000: episode: 868, duration: 4.977s, episode steps: 2, steps per second: 0, episode reward: 43.406, mean reward: 21.703 [-10.000, 53.406], mean action: 5.000 [2.000, 8.000], mean observation: 0.337 [0.000, 3.000], loss: 957.391846, mean_absolute_error: 14.602954, mean_q: 13.712336
step 2496; T: 1347900; average reward: 12.12 - action: w; reward: 14.33
 1512/2000: episode: 869, duration: 1.525s, episode steps: 1, steps per second: 1, episode reward: 14.327, mean reward: 14.327 [14.327, 14.327], mean action: 6.000 [6.000, 6.000], mean observation: 0.205 [0.000, 2.000], loss: 926.238037, mean_absolute_error: 14.980009, mean_q: 11.557674
step 2499; T: 1349250; average reward: 11.92 - action: n; reward: -163.75
 1514/2000: episode: 870, duration: 5.153s, episode steps: 2, steps per second: 0, episode reward: -173.752, mean reward: -86.876 [-163.752, -10.000], mean action: 4.500 [1.000, 8.000], mean observation: 0.321 [0.000, 3.000], loss: 994.488098, mean_absolute_error: 14.491152, mean_q: 13.974184
step 2506; T: 1350750; average reward: 11.92 - action: nw; reward: 17.50
 1518/2000: episode: 871, duration: 11.386s, episode steps: 4, steps per second: 0, episode reward: -12.503, mean reward: -3.126 [-10.000, 17.497], mean action: 5.250 [3.000, 7.000], mean observation: 0.315 [0.000, 4.000], loss: 824.481689, mean_absolute_error: 14.095260, mean_q: 12.991256
step 2507; T: 1351800; average reward: 11.87 - action: sw; reward: -39.06
 1519/2000: episode: 872, duration: 1.908s, episode steps: 1, steps per second: 1, episode reward: -39.064, mean reward: -39.064 [-39.064, -39.064], mean action: 5.000 [5.000, 5.000], mean observation: 0.157 [0.000, 4.000], loss: 744.377197, mean_absolute_error: 14.420741, mean_q: 13.853244
step 2514; T: 1354200; average reward: 11.70 - action: se; reward: -129.12
 1523/2000: episode: 873, duration: 12.465s, episode steps: 4, steps per second: 0, episode reward: -159.119, mean reward: -39.780 [-129.119, -10.000], mean action: 4.750 [3.000, 7.000], mean observation: 0.290 [0.000, 3.000], loss: 747.832031, mean_absolute_error: 13.422897, mean_q: 12.543537
step 2516; T: 1356000; average reward: 11.90 - action: ne; reward: 182.12
 1524/2000: episode: 874, duration: 2.823s, episode steps: 1, steps per second: 0, episode reward: 177.119, mean reward: 177.119 [177.119, 177.119], mean action: 1.000 [1.000, 1.000], mean observation: 0.187 [0.000, 4.000], loss: 3316.900146, mean_absolute_error: 14.541137, mean_q: 11.618483
step 2521; T: 1357350; average reward: 12.09 - action: n; reward: 178.74
 1527/2000: episode: 875, duration: 6.951s, episode steps: 3, steps per second: 0, episode reward: 158.737, mean reward: 52.912 [-10.000, 178.737], mean action: 5.333 [2.000, 8.000], mean observation: 0.291 [0.000, 3.000], loss: 779.388611, mean_absolute_error: 12.389679, mean_q: 13.154228
step 2525; T: 1359600; average reward: 12.09 - action: w; reward: 10.18
 1529/2000: episode: 876, duration: 6.229s, episode steps: 2, steps per second: 0, episode reward: -4.817, mean reward: -2.409 [-10.000, 5.183], mean action: 7.000 [6.000, 8.000], mean observation: 0.429 [0.000, 3.000], loss: 402.105347, mean_absolute_error: 13.026785, mean_q: 12.129671
step 2527; T: 1361100; average reward: 12.08 - action: nw; reward: 8.42
 1530/2000: episode: 877, duration: 3.253s, episode steps: 1, steps per second: 0, episode reward: 3.417, mean reward: 3.417 [3.417, 3.417], mean action: 7.000 [7.000, 7.000], mean observation: 0.408 [0.000, 3.000], loss: 263.933655, mean_absolute_error: 12.073332, mean_q: 13.487824
step 2528; T: 1362000; average reward: 12.08 - action: n; reward: 4.95
 1531/2000: episode: 878, duration: 2.291s, episode steps: 1, steps per second: 0, episode reward: 4.952, mean reward: 4.952 [4.952, 4.952], mean action: 8.000 [8.000, 8.000], mean observation: 0.211 [0.000, 2.000], loss: 375.263367, mean_absolute_error: 12.746544, mean_q: 12.661831
step 2529; T: 1365150; average reward: 12.08 - action: e; reward: 17.90
 1532/2000: episode: 879, duration: 2.415s, episode steps: 1, steps per second: 0, episode reward: 17.899, mean reward: 17.899 [17.899, 17.899], mean action: 2.000 [2.000, 2.000], mean observation: 0.200 [0.000, 3.000], loss: 776.844971, mean_absolute_error: 11.628222, mean_q: 14.449983
step 2531; T: 1366200; average reward: 12.06 - action: sw; reward: -5.00
 1533/2000: episode: 880, duration: 3.434s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.268 [0.000, 3.000], loss: 308.074341, mean_absolute_error: 12.493153, mean_q: 13.722132
step 2532; T: 1367550; average reward: 11.85 - action: se; reward: -171.97
 1534/2000: episode: 881, duration: 1.438s, episode steps: 1, steps per second: 1, episode reward: -171.973, mean reward: -171.973 [-171.973, -171.973], mean action: 3.000 [3.000, 3.000], mean observation: 0.264 [0.000, 3.000], loss: 1084.663818, mean_absolute_error: 13.967382, mean_q: 11.802443
step 2533; T: 1367850; average reward: 11.93 - action: n; reward: 82.31
 1535/2000: episode: 882, duration: 1.410s, episode steps: 1, steps per second: 1, episode reward: 82.311, mean reward: 82.311 [82.311, 82.311], mean action: 8.000 [8.000, 8.000], mean observation: 0.221 [0.000, 2.000], loss: 947.322021, mean_absolute_error: 13.911108, mean_q: 13.541052
step 2535; T: 1369050; average reward: 11.91 - action: nw; reward: -5.00
 1536/2000: episode: 883, duration: 3.251s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.400 [0.000, 4.000], loss: 472.122375, mean_absolute_error: 13.122950, mean_q: 10.731883
step 2539; T: 1372350; average reward: 11.90 - action: ne; reward: 1.55
 1538/2000: episode: 884, duration: 5.868s, episode steps: 2, steps per second: 0, episode reward: -13.453, mean reward: -6.726 [-10.000, -3.453], mean action: 3.500 [1.000, 6.000], mean observation: 0.252 [0.000, 3.000], loss: 968.725952, mean_absolute_error: 14.046759, mean_q: 12.557261
step 2540; T: 1374000; average reward: 11.90 - action: w; reward: 6.64
 1539/2000: episode: 885, duration: 1.565s, episode steps: 1, steps per second: 1, episode reward: 6.637, mean reward: 6.637 [6.637, 6.637], mean action: 6.000 [6.000, 6.000], mean observation: 0.216 [0.000, 2.000], loss: 710.559937, mean_absolute_error: 12.530554, mean_q: 8.568686
step 2542; T: 1374750; average reward: 11.93 - action: n; reward: 41.13
 1540/2000: episode: 886, duration: 3.232s, episode steps: 1, steps per second: 0, episode reward: 36.125, mean reward: 36.125 [36.125, 36.125], mean action: 8.000 [8.000, 8.000], mean observation: 0.309 [0.000, 3.000], loss: 608.411560, mean_absolute_error: 12.032564, mean_q: 11.034506
step 2543; T: 1375650; average reward: 11.93 - action: nw; reward: 14.94
 1541/2000: episode: 887, duration: 1.861s, episode steps: 1, steps per second: 1, episode reward: 14.942, mean reward: 14.942 [14.942, 14.942], mean action: 7.000 [7.000, 7.000], mean observation: 0.253 [0.000, 3.000], loss: 645.884399, mean_absolute_error: 14.846312, mean_q: 8.794259
step 2544; T: 1376100; average reward: 12.06 - action: se; reward: 122.75
 1542/2000: episode: 888, duration: 2.250s, episode steps: 1, steps per second: 0, episode reward: 122.749, mean reward: 122.749 [122.749, 122.749], mean action: 3.000 [3.000, 3.000], mean observation: 0.325 [0.000, 3.000], loss: 690.670410, mean_absolute_error: 13.817869, mean_q: 14.853886
step 2545; T: 1376850; average reward: 12.02 - action: w; reward: -17.56
 1543/2000: episode: 889, duration: 1.645s, episode steps: 1, steps per second: 1, episode reward: -17.565, mean reward: -17.565 [-17.565, -17.565], mean action: 6.000 [6.000, 6.000], mean observation: 0.429 [0.000, 4.000], loss: 946.896484, mean_absolute_error: 13.826193, mean_q: 11.567567
step 2546; T: 1377750; average reward: 11.92 - action: ne; reward: -78.71
 1544/2000: episode: 890, duration: 1.579s, episode steps: 1, steps per second: 1, episode reward: -78.714, mean reward: -78.714 [-78.714, -78.714], mean action: 1.000 [1.000, 1.000], mean observation: 0.179 [0.000, 4.000], loss: 1218.802612, mean_absolute_error: 12.567602, mean_q: 12.147984
step 2552; T: 1379550; average reward: 12.03 - action: n; reward: 109.02
 1547/2000: episode: 891, duration: 9.586s, episode steps: 3, steps per second: 0, episode reward: 84.018, mean reward: 28.006 [-10.000, 104.018], mean action: 7.333 [7.000, 8.000], mean observation: 0.441 [0.000, 3.000], loss: 907.790527, mean_absolute_error: 12.853492, mean_q: 14.119123
step 2553; T: 1380450; average reward: 12.17 - action: nw; reward: 138.84
 1548/2000: episode: 892, duration: 1.536s, episode steps: 1, steps per second: 1, episode reward: 138.842, mean reward: 138.842 [138.842, 138.842], mean action: 7.000 [7.000, 7.000], mean observation: 0.397 [0.000, 3.000], loss: 971.798279, mean_absolute_error: 14.923912, mean_q: 14.705825
step 2556; T: 1382850; average reward: 12.17 - action: nw; reward: 9.21
 1550/2000: episode: 893, duration: 4.567s, episode steps: 2, steps per second: 0, episode reward: -0.787, mean reward: -0.393 [-10.000, 9.213], mean action: 7.500 [7.000, 8.000], mean observation: 0.388 [0.000, 3.000], loss: 1433.753296, mean_absolute_error: 13.991318, mean_q: 12.782357
step 2557; T: 1384500; average reward: 12.35 - action: nw; reward: 173.24
 1551/2000: episode: 894, duration: 1.558s, episode steps: 1, steps per second: 1, episode reward: 173.245, mean reward: 173.245 [173.245, 173.245], mean action: 7.000 [7.000, 7.000], mean observation: 0.157 [0.000, 4.000], loss: 1099.950684, mean_absolute_error: 13.692912, mean_q: 12.689905
step 2558; T: 1386300; average reward: 12.38 - action: nw; reward: 34.25
 1552/2000: episode: 895, duration: 1.605s, episode steps: 1, steps per second: 1, episode reward: 34.247, mean reward: 34.247 [34.247, 34.247], mean action: 7.000 [7.000, 7.000], mean observation: 0.365 [0.000, 2.000], loss: 838.197998, mean_absolute_error: 14.396555, mean_q: 13.153214
step 2559; T: 1388850; average reward: 12.39 - action: n; reward: 26.98
 1553/2000: episode: 896, duration: 1.568s, episode steps: 1, steps per second: 1, episode reward: 26.977, mean reward: 26.977 [26.977, 26.977], mean action: 8.000 [8.000, 8.000], mean observation: 0.248 [0.000, 4.000], loss: 888.650391, mean_absolute_error: 13.658083, mean_q: 13.541884
step 2561; T: 1389600; average reward: 12.43 - action: w; reward: 50.99
 1554/2000: episode: 897, duration: 3.283s, episode steps: 1, steps per second: 0, episode reward: 45.986, mean reward: 45.986 [45.986, 45.986], mean action: 6.000 [6.000, 6.000], mean observation: 0.416 [0.000, 3.000], loss: 992.900574, mean_absolute_error: 13.665751, mean_q: 14.184323
step 2565; T: 1390800; average reward: 12.42 - action: e; reward: -5.00
 1556/2000: episode: 898, duration: 5.886s, episode steps: 2, steps per second: 0, episode reward: -20.000, mean reward: -10.000 [-10.000, -10.000], mean action: 4.500 [2.000, 7.000], mean observation: 0.413 [0.000, 4.000], loss: 614.460022, mean_absolute_error: 13.486099, mean_q: 13.505322
step 2568; T: 1391850; average reward: 12.35 - action: w; reward: -49.84
 1558/2000: episode: 899, duration: 4.769s, episode steps: 2, steps per second: 0, episode reward: -59.843, mean reward: -29.922 [-49.843, -10.000], mean action: 4.500 [3.000, 6.000], mean observation: 0.348 [0.000, 3.000], loss: 1487.688965, mean_absolute_error: 15.158100, mean_q: 13.203215
step 2569; T: 1392300; average reward: 12.23 - action: n; reward: -91.42
 1559/2000: episode: 900, duration: 1.292s, episode steps: 1, steps per second: 1, episode reward: -91.419, mean reward: -91.419 [-91.419, -91.419], mean action: 8.000 [8.000, 8.000], mean observation: 0.323 [0.000, 3.000], loss: 343.359406, mean_absolute_error: 12.519798, mean_q: 13.015522
step 2572; T: 1392750; average reward: 12.25 - action: sw; reward: 30.92
 1561/2000: episode: 901, duration: 4.946s, episode steps: 2, steps per second: 0, episode reward: 20.919, mean reward: 10.460 [-10.000, 30.919], mean action: 4.500 [4.000, 5.000], mean observation: 0.289 [0.000, 4.000], loss: 868.167480, mean_absolute_error: 13.670799, mean_q: 12.914040
step 2575; T: 1393500; average reward: 12.40 - action: w; reward: 149.31
 1563/2000: episode: 902, duration: 4.394s, episode steps: 2, steps per second: 0, episode reward: 139.310, mean reward: 69.655 [-10.000, 149.310], mean action: 3.500 [1.000, 6.000], mean observation: 0.412 [0.000, 3.000], loss: 581.664429, mean_absolute_error: 15.009109, mean_q: 11.941383
step 2577; T: 1394400; average reward: 12.36 - action: ne; reward: -27.93
 1564/2000: episode: 903, duration: 3.767s, episode steps: 1, steps per second: 0, episode reward: -32.925, mean reward: -32.925 [-32.925, -32.925], mean action: 1.000 [1.000, 1.000], mean observation: 0.323 [0.000, 3.000], loss: 1178.892456, mean_absolute_error: 17.132521, mean_q: 16.546232
step 2582; T: 1395900; average reward: 12.36 - action: w; reward: 11.92
 1567/2000: episode: 904, duration: 7.941s, episode steps: 3, steps per second: 0, episode reward: -8.083, mean reward: -2.694 [-10.000, 11.917], mean action: 6.000 [6.000, 6.000], mean observation: 0.285 [0.000, 3.000], loss: 1349.170044, mean_absolute_error: 15.072056, mean_q: 14.671247
step 2583; T: 1396650; average reward: 12.47 - action: w; reward: 112.13
 1568/2000: episode: 905, duration: 1.550s, episode steps: 1, steps per second: 1, episode reward: 112.128, mean reward: 112.128 [112.128, 112.128], mean action: 6.000 [6.000, 6.000], mean observation: 0.432 [0.000, 3.000], loss: 484.792053, mean_absolute_error: 14.511431, mean_q: 13.747107
step 2584; T: 1397100; average reward: 12.43 - action: nw; reward: -19.43
 1569/2000: episode: 906, duration: 1.695s, episode steps: 1, steps per second: 1, episode reward: -19.427, mean reward: -19.427 [-19.427, -19.427], mean action: 7.000 [7.000, 7.000], mean observation: 0.275 [0.000, 2.000], loss: 522.759705, mean_absolute_error: 13.395016, mean_q: 16.231216
step 2587; T: 1397700; average reward: 12.41 - action: w; reward: -11.22
 1571/2000: episode: 907, duration: 5.402s, episode steps: 2, steps per second: 0, episode reward: -21.215, mean reward: -10.608 [-11.215, -10.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.280 [0.000, 3.000], loss: 863.104919, mean_absolute_error: 14.671486, mean_q: 14.683965
step 2589; T: 1399350; average reward: 12.39 - action: n; reward: 0.59
 1572/2000: episode: 908, duration: 2.934s, episode steps: 1, steps per second: 0, episode reward: -4.408, mean reward: -4.408 [-4.408, -4.408], mean action: 8.000 [8.000, 8.000], mean observation: 0.267 [0.000, 3.000], loss: 536.322266, mean_absolute_error: 14.537855, mean_q: 17.430149
step 2592; T: 1401750; average reward: 12.43 - action: nw; reward: 47.85
 1574/2000: episode: 909, duration: 4.239s, episode steps: 2, steps per second: 0, episode reward: 47.845, mean reward: 23.923 [0.000, 47.845], mean action: 3.500 [0.000, 7.000], mean observation: 0.329 [0.000, 3.000], loss: 702.719055, mean_absolute_error: 14.066462, mean_q: 16.817978
step 2593; T: 1402500; average reward: 12.37 - action: w; reward: -43.32
 1575/2000: episode: 910, duration: 1.333s, episode steps: 1, steps per second: 1, episode reward: -43.320, mean reward: -43.320 [-43.320, -43.320], mean action: 6.000 [6.000, 6.000], mean observation: 0.272 [0.000, 3.000], loss: 697.793091, mean_absolute_error: 14.415917, mean_q: 12.539923
step 2598; T: 1403700; average reward: 12.47 - action: w; reward: 99.02
 1578/2000: episode: 911, duration: 7.125s, episode steps: 3, steps per second: 0, episode reward: 79.017, mean reward: 26.339 [-10.000, 99.017], mean action: 6.333 [6.000, 7.000], mean observation: 0.397 [0.000, 5.000], loss: 457.548096, mean_absolute_error: 12.802936, mean_q: 13.712420
step 2603; T: 1404750; average reward: 12.35 - action: n; reward: -95.23
 1581/2000: episode: 912, duration: 7.139s, episode steps: 3, steps per second: 0, episode reward: -115.234, mean reward: -38.411 [-95.234, -10.000], mean action: 3.667 [1.000, 8.000], mean observation: 0.380 [0.000, 3.000], loss: 819.540955, mean_absolute_error: 14.251142, mean_q: 16.890886
step 2604; T: 1405200; average reward: 12.47 - action: n; reward: 124.76
 1582/2000: episode: 913, duration: 1.430s, episode steps: 1, steps per second: 1, episode reward: 124.756, mean reward: 124.756 [124.756, 124.756], mean action: 8.000 [8.000, 8.000], mean observation: 0.331 [0.000, 2.000], loss: 305.976807, mean_absolute_error: 14.175544, mean_q: 16.276310
step 2607; T: 1406100; average reward: 12.36 - action: w; reward: -94.06
 1584/2000: episode: 914, duration: 4.537s, episode steps: 2, steps per second: 0, episode reward: -104.056, mean reward: -52.028 [-94.056, -10.000], mean action: 7.000 [6.000, 8.000], mean observation: 0.288 [0.000, 4.000], loss: 1144.872437, mean_absolute_error: 14.250505, mean_q: 11.103833
step 2608; T: 1406400; average reward: 12.38 - action: e; reward: 31.90
 1585/2000: episode: 915, duration: 1.855s, episode steps: 1, steps per second: 1, episode reward: 31.896, mean reward: 31.896 [31.896, 31.896], mean action: 2.000 [2.000, 2.000], mean observation: 0.184 [0.000, 3.000], loss: 1527.743286, mean_absolute_error: 15.445499, mean_q: 17.359785
step 2609; T: 1406850; average reward: 12.47 - action: nw; reward: 96.76
 1586/2000: episode: 916, duration: 1.526s, episode steps: 1, steps per second: 1, episode reward: 96.762, mean reward: 96.762 [96.762, 96.762], mean action: 7.000 [7.000, 7.000], mean observation: 0.192 [0.000, 2.000], loss: 508.150269, mean_absolute_error: 13.520517, mean_q: 18.749643
step 2613; T: 1409850; average reward: 12.55 - action: w; reward: 82.94
 1588/2000: episode: 917, duration: 6.466s, episode steps: 2, steps per second: 0, episode reward: 67.941, mean reward: 33.971 [-10.000, 77.941], mean action: 3.500 [1.000, 6.000], mean observation: 0.307 [0.000, 3.000], loss: 772.531372, mean_absolute_error: 13.347250, mean_q: 14.054223
step 2615; T: 1411500; average reward: 12.61 - action: n; reward: 67.00
 1589/2000: episode: 918, duration: 3.335s, episode steps: 1, steps per second: 0, episode reward: 62.002, mean reward: 62.002 [62.002, 62.002], mean action: 8.000 [8.000, 8.000], mean observation: 0.213 [0.000, 3.000], loss: 833.121643, mean_absolute_error: 14.948174, mean_q: 15.392607
step 2617; T: 1414350; average reward: 12.62 - action: w; reward: 28.46
 1590/2000: episode: 919, duration: 3.419s, episode steps: 1, steps per second: 0, episode reward: 23.461, mean reward: 23.461 [23.461, 23.461], mean action: 6.000 [6.000, 6.000], mean observation: 0.304 [0.000, 3.000], loss: 1041.942993, mean_absolute_error: 14.376655, mean_q: 11.179923
step 2618; T: 1415250; average reward: 12.51 - action: w; reward: -90.26
 1591/2000: episode: 920, duration: 1.590s, episode steps: 1, steps per second: 1, episode reward: -90.258, mean reward: -90.258 [-90.258, -90.258], mean action: 6.000 [6.000, 6.000], mean observation: 0.253 [0.000, 4.000], loss: 959.843811, mean_absolute_error: 14.922601, mean_q: 16.319969
step 2620; T: 1416150; average reward: 12.48 - action: e; reward: -17.25
 1592/2000: episode: 921, duration: 3.809s, episode steps: 1, steps per second: 0, episode reward: -22.253, mean reward: -22.253 [-22.253, -22.253], mean action: 2.000 [2.000, 2.000], mean observation: 0.323 [0.000, 3.000], loss: 935.896362, mean_absolute_error: 13.367307, mean_q: 12.467524
step 2622; T: 1417500; average reward: 12.51 - action: w; reward: 44.71
 1593/2000: episode: 922, duration: 2.937s, episode steps: 1, steps per second: 0, episode reward: 39.712, mean reward: 39.712 [39.712, 39.712], mean action: 6.000 [6.000, 6.000], mean observation: 0.371 [0.000, 2.000], loss: 1326.483643, mean_absolute_error: 15.907940, mean_q: 13.939903
step 2624; T: 1418400; average reward: 12.48 - action: nw; reward: -16.55
 1594/2000: episode: 923, duration: 3.121s, episode steps: 1, steps per second: 0, episode reward: -21.551, mean reward: -21.551 [-21.551, -21.551], mean action: 7.000 [7.000, 7.000], mean observation: 0.395 [0.000, 3.000], loss: 1425.040405, mean_absolute_error: 16.568542, mean_q: 17.455791
step 2627; T: 1419600; average reward: 12.49 - action: nw; reward: 21.77
 1596/2000: episode: 924, duration: 4.605s, episode steps: 2, steps per second: 0, episode reward: 11.772, mean reward: 5.886 [-10.000, 21.772], mean action: 6.500 [6.000, 7.000], mean observation: 0.405 [0.000, 3.000], loss: 862.851501, mean_absolute_error: 15.240826, mean_q: 14.339639
step 2629; T: 1421550; average reward: 12.50 - action: nw; reward: 22.57
 1597/2000: episode: 925, duration: 3.269s, episode steps: 1, steps per second: 0, episode reward: 17.572, mean reward: 17.572 [17.572, 17.572], mean action: 7.000 [7.000, 7.000], mean observation: 0.381 [0.000, 3.000], loss: 296.892944, mean_absolute_error: 14.414381, mean_q: 11.685586
step 2630; T: 1422300; average reward: 12.53 - action: w; reward: 37.15
 1598/2000: episode: 926, duration: 1.580s, episode steps: 1, steps per second: 1, episode reward: 37.152, mean reward: 37.152 [37.152, 37.152], mean action: 6.000 [6.000, 6.000], mean observation: 0.389 [0.000, 3.000], loss: 315.425842, mean_absolute_error: 14.518753, mean_q: 13.756584
step 2634; T: 1424250; average reward: 12.68 - action: w; reward: 148.79
 1600/2000: episode: 927, duration: 7.284s, episode steps: 2, steps per second: 0, episode reward: 133.786, mean reward: 66.893 [-10.000, 143.786], mean action: 6.500 [6.000, 7.000], mean observation: 0.450 [0.000, 3.000], loss: 1439.714844, mean_absolute_error: 16.242176, mean_q: 16.114161
step 2635; T: 1424700; average reward: 12.61 - action: w; reward: -45.64
 1601/2000: episode: 928, duration: 1.558s, episode steps: 1, steps per second: 1, episode reward: -45.638, mean reward: -45.638 [-45.638, -45.638], mean action: 6.000 [6.000, 6.000], mean observation: 0.432 [0.000, 5.000], loss: 428.415833, mean_absolute_error: 13.729742, mean_q: 11.517514
step 2638; T: 1426050; average reward: 12.65 - action: nw; reward: 46.20
 1603/2000: episode: 929, duration: 5.534s, episode steps: 2, steps per second: 0, episode reward: 36.203, mean reward: 18.101 [-10.000, 46.203], mean action: 7.000 [7.000, 7.000], mean observation: 0.353 [0.000, 5.000], loss: 595.235718, mean_absolute_error: 13.981281, mean_q: 11.811800
step 2641; T: 1430100; average reward: 12.64 - action: noop; reward: 0.00
 1605/2000: episode: 930, duration: 4.778s, episode steps: 2, steps per second: 0, episode reward: -10.000, mean reward: -5.000 [-10.000, 0.000], mean action: 3.500 [0.000, 7.000], mean observation: 0.437 [0.000, 4.000], loss: 792.546875, mean_absolute_error: 13.981760, mean_q: 12.514512
step 2643; T: 1432200; average reward: 12.74 - action: w; reward: 111.32
 1606/2000: episode: 931, duration: 3.666s, episode steps: 1, steps per second: 0, episode reward: 106.319, mean reward: 106.319 [106.319, 106.319], mean action: 6.000 [6.000, 6.000], mean observation: 0.264 [0.000, 2.000], loss: 696.413940, mean_absolute_error: 13.827766, mean_q: 12.750869
step 2647; T: 1435650; average reward: 12.69 - action: ne; reward: -39.30
 1608/2000: episode: 932, duration: 6.670s, episode steps: 2, steps per second: 0, episode reward: -54.304, mean reward: -27.152 [-44.304, -10.000], mean action: 4.500 [1.000, 8.000], mean observation: 0.365 [0.000, 3.000], loss: 1692.511963, mean_absolute_error: 16.274464, mean_q: 13.892736
step 2652; T: 1436850; average reward: 12.84 - action: e; reward: 155.37
 1611/2000: episode: 933, duration: 7.606s, episode steps: 3, steps per second: 0, episode reward: 145.365, mean reward: 48.455 [-10.000, 155.365], mean action: 3.000 [0.000, 7.000], mean observation: 0.253 [0.000, 4.000], loss: 678.094543, mean_absolute_error: 14.438741, mean_q: 15.709065
step 2658; T: 1438500; average reward: 12.82 - action: s; reward: -5.00
 1614/2000: episode: 934, duration: 9.675s, episode steps: 3, steps per second: 0, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 3.333 [2.000, 4.000], mean observation: 0.241 [0.000, 3.000], loss: 750.138489, mean_absolute_error: 14.646211, mean_q: 12.457784
step 2660; T: 1439250; average reward: 12.85 - action: ne; reward: 38.99
 1615/2000: episode: 935, duration: 3.015s, episode steps: 1, steps per second: 0, episode reward: 33.992, mean reward: 33.992 [33.992, 33.992], mean action: 1.000 [1.000, 1.000], mean observation: 0.456 [0.000, 3.000], loss: 253.338013, mean_absolute_error: 13.560207, mean_q: 11.606218
step 2661; T: 1440000; average reward: 12.84 - action: nw; reward: 4.94
 1616/2000: episode: 936, duration: 1.530s, episode steps: 1, steps per second: 1, episode reward: 4.938, mean reward: 4.938 [4.938, 4.938], mean action: 7.000 [7.000, 7.000], mean observation: 0.344 [0.000, 3.000], loss: 652.393860, mean_absolute_error: 14.875092, mean_q: 11.210581
step 2663; T: 1441050; average reward: 12.83 - action: nw; reward: 3.79
 1617/2000: episode: 937, duration: 4.209s, episode steps: 1, steps per second: 0, episode reward: -1.210, mean reward: -1.210 [-1.210, -1.210], mean action: 7.000 [7.000, 7.000], mean observation: 0.235 [0.000, 2.000], loss: 914.827148, mean_absolute_error: 15.287170, mean_q: 14.605112
step 2664; T: 1441350; average reward: 12.81 - action: noop; reward: -5.05
 1618/2000: episode: 938, duration: 1.667s, episode steps: 1, steps per second: 1, episode reward: -5.053, mean reward: -5.053 [-5.053, -5.053], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [0.000, 3.000], loss: 2221.186768, mean_absolute_error: 15.687510, mean_q: 14.913672
step 2665; T: 1442250; average reward: 12.75 - action: n; reward: -45.74
 1619/2000: episode: 939, duration: 2.428s, episode steps: 1, steps per second: 0, episode reward: -45.741, mean reward: -45.741 [-45.741, -45.741], mean action: 8.000 [8.000, 8.000], mean observation: 0.427 [0.000, 3.000], loss: 308.951355, mean_absolute_error: 14.133635, mean_q: 14.956360
step 2666; T: 1443450; average reward: 12.74 - action: e; reward: 7.77
 1620/2000: episode: 940, duration: 1.959s, episode steps: 1, steps per second: 1, episode reward: 7.766, mean reward: 7.766 [7.766, 7.766], mean action: 2.000 [2.000, 2.000], mean observation: 0.309 [0.000, 3.000], loss: 487.325653, mean_absolute_error: 14.404448, mean_q: 13.337603
step 2667; T: 1444350; average reward: 12.74 - action: nw; reward: 7.55
 1621/2000: episode: 941, duration: 1.416s, episode steps: 1, steps per second: 1, episode reward: 7.554, mean reward: 7.554 [7.554, 7.554], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 3.000], loss: 851.213867, mean_absolute_error: 16.392681, mean_q: 13.305429
step 2672; T: 1446150; average reward: 12.78 - action: e; reward: 57.03
 1624/2000: episode: 942, duration: 8.202s, episode steps: 3, steps per second: 0, episode reward: 47.033, mean reward: 15.678 [-10.000, 57.033], mean action: 3.333 [0.000, 8.000], mean observation: 0.253 [0.000, 4.000], loss: 1033.609375, mean_absolute_error: 14.627593, mean_q: 12.087379
step 2678; T: 1447950; average reward: 12.77 - action: nw; reward: -5.00
 1627/2000: episode: 943, duration: 11.602s, episode steps: 3, steps per second: 0, episode reward: -30.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.000 [2.000, 7.000], mean observation: 0.207 [0.000, 3.000], loss: 1229.020264, mean_absolute_error: 15.095799, mean_q: 12.367985
step 2679; T: 1448850; average reward: 12.75 - action: nw; reward: -5.00
 1628/2000: episode: 944, duration: 1.604s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.240 [0.000, 3.000], loss: 360.377075, mean_absolute_error: 15.178921, mean_q: 13.755705
step 2680; T: 1450050; average reward: 12.88 - action: ne; reward: 136.15
 1629/2000: episode: 945, duration: 2.631s, episode steps: 1, steps per second: 0, episode reward: 136.148, mean reward: 136.148 [136.148, 136.148], mean action: 1.000 [1.000, 1.000], mean observation: 0.283 [0.000, 3.000], loss: 850.534241, mean_absolute_error: 14.130114, mean_q: 11.936453
step 2681; T: 1450800; average reward: 12.95 - action: se; reward: 77.21
 1630/2000: episode: 946, duration: 1.703s, episode steps: 1, steps per second: 1, episode reward: 77.205, mean reward: 77.205 [77.205, 77.205], mean action: 3.000 [3.000, 3.000], mean observation: 0.240 [0.000, 4.000], loss: 626.075500, mean_absolute_error: 15.601795, mean_q: 10.797914
step 2686; T: 1452000; average reward: 12.93 - action: noop; reward: 0.00
 1633/2000: episode: 947, duration: 8.562s, episode steps: 3, steps per second: 0, episode reward: -20.000, mean reward: -6.667 [-10.000, 0.000], mean action: 3.000 [0.000, 7.000], mean observation: 0.281 [0.000, 4.000], loss: 657.410461, mean_absolute_error: 14.873974, mean_q: 12.320717
step 2687; T: 1453050; average reward: 13.05 - action: n; reward: 127.74
 1634/2000: episode: 948, duration: 2.471s, episode steps: 1, steps per second: 0, episode reward: 127.741, mean reward: 127.741 [127.741, 127.741], mean action: 8.000 [8.000, 8.000], mean observation: 0.237 [0.000, 3.000], loss: 1611.227783, mean_absolute_error: 16.188025, mean_q: 15.758507
step 2695; T: 1454700; average reward: 13.04 - action: noop; reward: 0.00
 1638/2000: episode: 949, duration: 14.025s, episode steps: 4, steps per second: 0, episode reward: -30.000, mean reward: -7.500 [-10.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.310 [0.000, 3.000], loss: 590.843628, mean_absolute_error: 14.162409, mean_q: 13.022229
step 2696; T: 1455300; average reward: 13.16 - action: ne; reward: 128.49
 1639/2000: episode: 950, duration: 2.039s, episode steps: 1, steps per second: 0, episode reward: 128.490, mean reward: 128.490 [128.490, 128.490], mean action: 1.000 [1.000, 1.000], mean observation: 0.437 [0.000, 3.000], loss: 382.378479, mean_absolute_error: 13.715530, mean_q: 14.525359
step 2697; T: 1458150; average reward: 13.21 - action: w; reward: 57.77
 1640/2000: episode: 951, duration: 1.771s, episode steps: 1, steps per second: 1, episode reward: 57.768, mean reward: 57.768 [57.768, 57.768], mean action: 6.000 [6.000, 6.000], mean observation: 0.240 [0.000, 3.000], loss: 1671.574585, mean_absolute_error: 15.133872, mean_q: 13.113636
step 2701; T: 1459050; average reward: 13.23 - action: n; reward: 32.43
 1642/2000: episode: 952, duration: 7.558s, episode steps: 2, steps per second: 0, episode reward: 17.433, mean reward: 8.716 [-10.000, 27.433], mean action: 6.500 [5.000, 8.000], mean observation: 0.292 [0.000, 3.000], loss: 933.545410, mean_absolute_error: 16.067057, mean_q: 13.386614
step 2702; T: 1461750; average reward: 13.26 - action: n; reward: 43.92
 1643/2000: episode: 953, duration: 1.765s, episode steps: 1, steps per second: 1, episode reward: 43.922, mean reward: 43.922 [43.922, 43.922], mean action: 8.000 [8.000, 8.000], mean observation: 0.461 [0.000, 3.000], loss: 590.035645, mean_absolute_error: 14.841169, mean_q: 13.033406
step 2705; T: 1462500; average reward: 13.34 - action: w; reward: 84.70
 1645/2000: episode: 954, duration: 6.030s, episode steps: 2, steps per second: 0, episode reward: 74.699, mean reward: 37.349 [-10.000, 84.699], mean action: 6.000 [6.000, 6.000], mean observation: 0.357 [0.000, 3.000], loss: 687.348022, mean_absolute_error: 15.571274, mean_q: 13.639973
step 2706; T: 1463100; average reward: 13.42 - action: n; reward: 91.68
 1646/2000: episode: 955, duration: 2.748s, episode steps: 1, steps per second: 0, episode reward: 91.684, mean reward: 91.684 [91.684, 91.684], mean action: 8.000 [8.000, 8.000], mean observation: 0.349 [0.000, 3.000], loss: 1199.760132, mean_absolute_error: 14.418112, mean_q: 10.309589
step 2708; T: 1465050; average reward: 13.44 - action: nw; reward: 38.92
 1647/2000: episode: 956, duration: 3.337s, episode steps: 1, steps per second: 0, episode reward: 33.921, mean reward: 33.921 [33.921, 33.921], mean action: 7.000 [7.000, 7.000], mean observation: 0.357 [0.000, 2.000], loss: 908.531494, mean_absolute_error: 16.171890, mean_q: 13.587485
step 2710; T: 1465500; average reward: 13.45 - action: n; reward: 17.97
 1648/2000: episode: 957, duration: 3.642s, episode steps: 1, steps per second: 0, episode reward: 12.967, mean reward: 12.967 [12.967, 12.967], mean action: 8.000 [8.000, 8.000], mean observation: 0.360 [0.000, 3.000], loss: 778.456177, mean_absolute_error: 14.575120, mean_q: 16.131027
step 2711; T: 1467150; average reward: 13.44 - action: nw; reward: 2.39
 1649/2000: episode: 958, duration: 2.226s, episode steps: 1, steps per second: 0, episode reward: 2.387, mean reward: 2.387 [2.387, 2.387], mean action: 7.000 [7.000, 7.000], mean observation: 0.160 [0.000, 4.000], loss: 512.684326, mean_absolute_error: 14.737891, mean_q: 14.304173
step 2713; T: 1468500; average reward: 13.42 - action: w; reward: -5.00
 1650/2000: episode: 959, duration: 3.438s, episode steps: 1, steps per second: 0, episode reward: -10.000, mean reward: -10.000 [-10.000, -10.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.285 [0.000, 4.000], loss: 917.790466, mean_absolute_error: 14.420842, mean_q: 12.961362
step 2718; T: 1469550; average reward: 13.50 - action: e; reward: 93.73
 1653/2000: episode: 960, duration: 8.501s, episode steps: 3, steps per second: 0, episode reward: 73.729, mean reward: 24.576 [-10.000, 93.729], mean action: 3.333 [2.000, 6.000], mean observation: 0.364 [0.000, 3.000], loss: 618.399170, mean_absolute_error: 14.499046, mean_q: 13.475040
step 2719; T: 1470300; average reward: 13.52 - action: n; reward: 35.46
 1654/2000: episode: 961, duration: 1.530s, episode steps: 1, steps per second: 1, episode reward: 35.460, mean reward: 35.460 [35.460, 35.460], mean action: 8.000 [8.000, 8.000], mean observation: 0.387 [0.000, 2.000], loss: 1459.607056, mean_absolute_error: 16.167156, mean_q: 17.463858
step 2720; T: 1470900; average reward: 13.47 - action: nw; reward: -34.98
 1655/2000: episode: 962, duration: 1.669s, episode steps: 1, steps per second: 1, episode reward: -34.984, mean reward: -34.984 [-34.984, -34.984], mean action: 7.000 [7.000, 7.000], mean observation: 0.440 [0.000, 3.000], loss: 768.931335, mean_absolute_error: 15.622886, mean_q: 14.133171
step 2725; T: 1476750; average reward: 13.43 - action: w; reward: -32.23
 1658/2000: episode: 963, duration: 8.910s, episode steps: 3, steps per second: 0, episode reward: -52.231, mean reward: -17.410 [-32.231, -10.000], mean action: 5.000 [2.000, 7.000], mean observation: 0.314 [0.000, 3.000], loss: 1083.473999, mean_absolute_error: 15.958261, mean_q: 13.973649
step 2726; T: 1477350; average reward: 13.43 - action: ne; reward: 12.11
 1659/2000: episode: 964, duration: 2.830s, episode steps: 1, steps per second: 0, episode reward: 12.111, mean reward: 12.111 [12.111, 12.111], mean action: 1.000 [1.000, 1.000], mean observation: 0.331 [0.000, 3.000], loss: 570.068115, mean_absolute_error: 16.103558, mean_q: 13.846934
step 2728; T: 1479300; average reward: 13.40 - action: nw; reward: -8.23
 1660/2000: episode: 965, duration: 4.307s, episode steps: 1, steps per second: 0, episode reward: -13.228, mean reward: -13.228 [-13.228, -13.228], mean action: 7.000 [7.000, 7.000], mean observation: 0.283 [0.000, 3.000], loss: 920.175171, mean_absolute_error: 16.473944, mean_q: 18.500368
step 2731; T: 1482900; average reward: 13.38 - action: w; reward: -4.70
 1662/2000: episode: 966, duration: 4.923s, episode steps: 2, steps per second: 0, episode reward: -14.696, mean reward: -7.348 [-10.000, -4.696], mean action: 7.000 [6.000, 8.000], mean observation: 0.351 [0.000, 4.000], loss: 381.960754, mean_absolute_error: 14.474983, mean_q: 13.519738
step 2732; T: 1483650; average reward: 13.41 - action: w; reward: 36.74
 1663/2000: episode: 967, duration: 1.685s, episode steps: 1, steps per second: 1, episode reward: 36.735, mean reward: 36.735 [36.735, 36.735], mean action: 6.000 [6.000, 6.000], mean observation: 0.256 [0.000, 2.000], loss: 873.061646, mean_absolute_error: 15.363361, mean_q: 15.173662
step 2735; T: 1484550; average reward: 13.39 - action: e; reward: -5.00
 1665/2000: episode: 968, duration: 4.835s, episode steps: 2, steps per second: 0, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 4.000 [2.000, 6.000], mean observation: 0.344 [0.000, 3.000], loss: 1031.105347, mean_absolute_error: 15.930887, mean_q: 14.972118
step 2738; T: 1485600; average reward: 13.41 - action: w; reward: 30.66
 1667/2000: episode: 969, duration: 4.556s, episode steps: 2, steps per second: 0, episode reward: 20.656, mean reward: 10.328 [-10.000, 30.656], mean action: 7.000 [6.000, 8.000], mean observation: 0.388 [0.000, 4.000], loss: 569.770813, mean_absolute_error: 14.050269, mean_q: 13.345045
step 2739; T: 1486500; average reward: 13.25 - action: w; reward: -139.99
 1668/2000: episode: 970, duration: 1.564s, episode steps: 1, steps per second: 1, episode reward: -139.991, mean reward: -139.991 [-139.991, -139.991], mean action: 6.000 [6.000, 6.000], mean observation: 0.312 [0.000, 2.000], loss: 1130.014648, mean_absolute_error: 15.769650, mean_q: 13.493523
step 2740; T: 1487400; average reward: 13.27 - action: w; reward: 29.92
 1669/2000: episode: 971, duration: 1.563s, episode steps: 1, steps per second: 1, episode reward: 29.924, mean reward: 29.924 [29.924, 29.924], mean action: 6.000 [6.000, 6.000], mean observation: 0.181 [0.000, 2.000], loss: 1115.767578, mean_absolute_error: 15.936970, mean_q: 15.349495
step 2748; T: 1489500; average reward: 13.25 - action: e; reward: -5.00
 1673/2000: episode: 972, duration: 13.014s, episode steps: 4, steps per second: 0, episode reward: -30.000, mean reward: -7.500 [-10.000, 0.000], mean action: 3.000 [0.000, 6.000], mean observation: 0.214 [0.000, 3.000], loss: 794.976807, mean_absolute_error: 15.515021, mean_q: 14.314211
step 2750; T: 1490850; average reward: 13.33 - action: nw; reward: 98.70
 1674/2000: episode: 973, duration: 3.116s, episode steps: 1, steps per second: 0, episode reward: 93.697, mean reward: 93.697 [93.697, 93.697], mean action: 7.000 [7.000, 7.000], mean observation: 0.219 [0.000, 3.000], loss: 782.612549, mean_absolute_error: 16.699011, mean_q: 12.980465
step 2752; T: 1491750; average reward: 13.35 - action: e; reward: 25.32
 1675/2000: episode: 974, duration: 4.185s, episode steps: 1, steps per second: 0, episode reward: 20.324, mean reward: 20.324 [20.324, 20.324], mean action: 2.000 [2.000, 2.000], mean observation: 0.400 [0.000, 3.000], loss: 348.550232, mean_absolute_error: 14.114104, mean_q: 12.006052
step 2763; T: 1494450; average reward: 13.33 - action: w; reward: -5.00
 1681/2000: episode: 975, duration: 18.389s, episode steps: 6, steps per second: 0, episode reward: -45.000, mean reward: -7.500 [-10.000, 0.000], mean action: 5.167 [0.000, 7.000], mean observation: 0.328 [0.000, 3.000], loss: 855.174988, mean_absolute_error: 15.338231, mean_q: 13.069482
step 2764; T: 1495050; average reward: 13.33 - action: noop; reward: 14.00
 1682/2000: episode: 976, duration: 1.762s, episode steps: 1, steps per second: 1, episode reward: 14.002, mean reward: 14.002 [14.002, 14.002], mean action: 0.000 [0.000, 0.000], mean observation: 0.195 [0.000, 2.000], loss: 916.627258, mean_absolute_error: 16.088776, mean_q: 12.584820
step 2774; T: 1500900; average reward: 13.31 - action: nw; reward: -5.00
 1687/2000: episode: 977, duration: 17.188s, episode steps: 5, steps per second: 0, episode reward: -30.000, mean reward: -6.000 [-10.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.236 [0.000, 3.000], loss: 862.359985, mean_absolute_error: 15.668802, mean_q: 14.017970
step 2775; T: 1501800; average reward: 13.34 - action: ne; reward: 40.92
 1688/2000: episode: 978, duration: 2.265s, episode steps: 1, steps per second: 0, episode reward: 40.922, mean reward: 40.922 [40.922, 40.922], mean action: 1.000 [1.000, 1.000], mean observation: 0.355 [0.000, 5.000], loss: 690.544434, mean_absolute_error: 14.079539, mean_q: 13.769007
step 2783; T: 1503900; average reward: 13.46 - action: n; reward: 128.94
 1692/2000: episode: 979, duration: 13.849s, episode steps: 4, steps per second: 0, episode reward: 93.938, mean reward: 23.484 [-10.000, 123.938], mean action: 6.250 [5.000, 8.000], mean observation: 0.343 [0.000, 4.000], loss: 938.980713, mean_absolute_error: 14.556076, mean_q: 13.353308
step 2784; T: 1504950; average reward: 13.35 - action: w; reward: -92.25
 1693/2000: episode: 980, duration: 1.706s, episode steps: 1, steps per second: 1, episode reward: -92.248, mean reward: -92.248 [-92.248, -92.248], mean action: 6.000 [6.000, 6.000], mean observation: 0.357 [0.000, 3.000], loss: 412.603485, mean_absolute_error: 15.123724, mean_q: 14.330597
step 2786; T: 1505550; average reward: 13.48 - action: nw; reward: 145.14
 1694/2000: episode: 981, duration: 2.898s, episode steps: 1, steps per second: 0, episode reward: 140.135, mean reward: 140.135 [140.135, 140.135], mean action: 7.000 [7.000, 7.000], mean observation: 0.408 [0.000, 3.000], loss: 1148.527344, mean_absolute_error: 15.844412, mean_q: 13.708172
step 2790; T: 1506150; average reward: 13.49 - action: n; reward: 22.66
 1696/2000: episode: 982, duration: 7.373s, episode steps: 2, steps per second: 0, episode reward: 7.658, mean reward: 3.829 [-10.000, 17.658], mean action: 7.500 [7.000, 8.000], mean observation: 0.427 [0.000, 3.000], loss: 1096.972412, mean_absolute_error: 14.889402, mean_q: 12.835781
step 2792; T: 1506750; average reward: 13.56 - action: nw; reward: 82.58
 1697/2000: episode: 983, duration: 3.242s, episode steps: 1, steps per second: 0, episode reward: 77.580, mean reward: 77.580 [77.580, 77.580], mean action: 7.000 [7.000, 7.000], mean observation: 0.237 [0.000, 4.000], loss: 535.937500, mean_absolute_error: 14.910892, mean_q: 14.023747
step 2793; T: 1507800; average reward: 13.48 - action: w; reward: -66.23
 1698/2000: episode: 984, duration: 1.717s, episode steps: 1, steps per second: 1, episode reward: -66.225, mean reward: -66.225 [-66.225, -66.225], mean action: 6.000 [6.000, 6.000], mean observation: 0.205 [0.000, 2.000], loss: 1111.628662, mean_absolute_error: 16.326286, mean_q: 13.176356
step 2795; T: 1510650; average reward: 13.55 - action: n; reward: 84.91
 1699/2000: episode: 985, duration: 3.415s, episode steps: 1, steps per second: 0, episode reward: 79.912, mean reward: 79.912 [79.912, 79.912], mean action: 8.000 [8.000, 8.000], mean observation: 0.261 [0.000, 4.000], loss: 817.322021, mean_absolute_error: 16.264566, mean_q: 13.433863
step 2799; T: 1511400; average reward: 13.49 - action: w; reward: -50.54
 1701/2000: episode: 986, duration: 8.100s, episode steps: 2, steps per second: 0, episode reward: -65.539, mean reward: -32.770 [-55.539, -10.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.271 [0.000, 3.000], loss: 927.494446, mean_absolute_error: 15.689299, mean_q: 15.536799
step 2801; T: 1512450; average reward: 13.49 - action: se; reward: 17.94
 1702/2000: episode: 987, duration: 3.368s, episode steps: 1, steps per second: 0, episode reward: 12.938, mean reward: 12.938 [12.938, 12.938], mean action: 3.000 [3.000, 3.000], mean observation: 0.440 [0.000, 3.000], loss: 820.543274, mean_absolute_error: 14.676941, mean_q: 13.686533
step 2804; T: 1514400; average reward: 13.55 - action: w; reward: 68.37
 1704/2000: episode: 988, duration: 5.541s, episode steps: 2, steps per second: 0, episode reward: 68.367, mean reward: 34.183 [0.000, 68.367], mean action: 3.000 [0.000, 6.000], mean observation: 0.299 [0.000, 4.000], loss: 929.267395, mean_absolute_error: 14.475916, mean_q: 12.453610
step 2810; T: 1516050; average reward: 13.53 - action: e; reward: -5.00
 1707/2000: episode: 989, duration: 11.246s, episode steps: 3, steps per second: 0, episode reward: -20.000, mean reward: -6.667 [-10.000, 0.000], mean action: 2.667 [0.000, 6.000], mean observation: 0.260 [0.000, 4.000], loss: 448.148895, mean_absolute_error: 13.811677, mean_q: 14.699862
step 2811; T: 1516800; average reward: 13.51 - action: ne; reward: -5.00
 1708/2000: episode: 990, duration: 1.748s, episode steps: 1, steps per second: 1, episode reward: -5.000, mean reward: -5.000 [-5.000, -5.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.307 [0.000, 3.000], loss: 1404.259521, mean_absolute_error: 17.509687, mean_q: 16.909912
step 2813; T: 1518900; average reward: 13.50 - action: noop; reward: 0.00
 1709/2000: episode: 991, duration: 4.553s, episode steps: 1, steps per second: 0, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [0.000, 2.000], loss: 515.433838, mean_absolute_error: 12.430829, mean_q: 15.303061
step 2814; T: 1521750; average reward: 13.55 - action: w; reward: 63.66
 1710/2000: episode: 992, duration: 1.970s, episode steps: 1, steps per second: 1, episode reward: 63.657, mean reward: 63.657 [63.657, 63.657], mean action: 6.000 [6.000, 6.000], mean observation: 0.168 [0.000, 3.000], loss: 1242.760742, mean_absolute_error: 16.914597, mean_q: 15.017979
step 2815; T: 1524000; average reward: 13.71 - action: n; reward: 171.07
 1711/2000: episode: 993, duration: 1.965s, episode steps: 1, steps per second: 1, episode reward: 171.072, mean reward: 171.072 [171.072, 171.072], mean action: 8.000 [8.000, 8.000], mean observation: 0.397 [0.000, 2.000], loss: 568.665161, mean_absolute_error: 13.181383, mean_q: 15.089172
step 2817; T: 1525050; average reward: 13.74 - action: n; reward: 48.68
 1712/2000: episode: 994, duration: 3.198s, episode steps: 1, steps per second: 0, episode reward: 43.678, mean reward: 43.678 [43.678, 43.678], mean action: 8.000 [8.000, 8.000], mean observation: 0.253 [0.000, 2.000], loss: 1495.703491, mean_absolute_error: 15.719772, mean_q: 17.552517
step 2820; T: 1526100; average reward: 13.72 - action: e; reward: -5.00
 1714/2000: episode: 995, duration: 5.252s, episode steps: 2, steps per second: 0, episode reward: -15.000, mean reward: -7.500 [-10.000, -5.000], mean action: 4.000 [2.000, 6.000], mean observation: 0.349 [0.000, 3.000], loss: 741.992676, mean_absolute_error: 14.807165, mean_q: 17.001675
step 2821; T: 1526700; average reward: 13.83 - action: nw; reward: 118.18
 1715/2000: episode: 996, duration: 2.005s, episode steps: 1, steps per second: 0, episode reward: 118.183, mean reward: 118.183 [118.183, 118.183], mean action: 7.000 [7.000, 7.000], mean observation: 0.288 [0.000, 2.000], loss: 497.711426, mean_absolute_error: 15.115980, mean_q: 16.697834
step 2822; T: 1528200; average reward: 13.88 - action: w; reward: 61.60
 1716/2000: episode: 997, duration: 1.812s, episode steps: 1, steps per second: 1, episode reward: 61.603, mean reward: 61.603 [61.603, 61.603], mean action: 6.000 [6.000, 6.000], mean observation: 0.227 [0.000, 2.000], loss: 1221.255615, mean_absolute_error: 15.713522, mean_q: 18.693974
step 2823; T: 1529100; average reward: 13.87 - action: n; reward: 10.74
 1717/2000: episode: 998, duration: 1.707s, episode steps: 1, steps per second: 1, episode reward: 10.739, mean reward: 10.739 [10.739, 10.739], mean action: 8.000 [8.000, 8.000], mean observation: 0.328 [0.000, 3.000], loss: 376.925842, mean_absolute_error: 13.808950, mean_q: 20.757917
step 2825; T: 1533000; average reward: 13.87 - action: nw; reward: 8.62
 1718/2000: episode: 999, duration: 3.313s, episode steps: 1, steps per second: 0, episode reward: 3.616, mean reward: 3.616 [3.616, 3.616], mean action: 7.000 [7.000, 7.000], mean observation: 0.379 [0.000, 2.000], loss: 993.653992, mean_absolute_error: 14.908975, mean_q: 18.653465
step 2827; T: 1534050; average reward: 14.00 - action: ne; reward: 143.67
 1719/2000: episode: 1000, duration: 3.226s, episode steps: 1, steps per second: 0, episode reward: 138.670, mean reward: 138.670 [138.670, 138.670], mean action: 1.000 [1.000, 1.000], mean observation: 0.445 [0.000, 3.000], loss: 616.279419, mean_absolute_error: 14.617997, mean_q: 14.777609
step 2828; T: 1534650; average reward: 13.88 - action: ne; reward: -103.77
 1720/2000: episode: 1001, duration: 1.563s, episode steps: 1, steps per second: 1, episode reward: -103.774, mean reward: -103.774 [-103.774, -103.774], mean action: 1.000 [1.000, 1.000], mean observation: 0.395 [0.000, 3.000], loss: 1473.663940, mean_absolute_error: 16.132017, mean_q: 16.655537